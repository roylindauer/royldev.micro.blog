{
	"version": "https://jsonfeed.org/version/1",
	"title": "ROYL DEV BLOG",
	"icon": "https://micro.blog/royl/avatar.jpg",
	"home_page_url": "https://roylindauer.dev/",
	"feed_url": "https://roylindauer.dev/feed.json",
	"items": [
		
			{
				"id": "http://royldev.micro.blog/2024/11/12/running-a-docker.html",
				"title": "Running a docker registry in my homelab",
				"content_html": "<p>Home labs are a great place to learn and tinker with systems. I love it because I get to wear my systems administrator hat. I&rsquo;ve been doing a lot of application development lately as well as tinkering with various build &amp; deployment tools for those applications. The best way, in my opinion, is <a href=\"https://docs.docker.com/\">docker</a>. It&rsquo;s just so good, you can package up all of the tools and configurations into a distributable unit, using an open standard.</p>\n<p>Those packaged units are <a href=\"https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-an-image/\">called images in docker</a>. And in order to distribute them, you need a place to store them. Enter the registry.</p>\n<blockquote>\n<p>The Registry is a stateless, highly scalable server side application that stores and lets you distribute container images and other content. Learn more about it here -&gt; <a href=\"https://distribution.github.io/distribution/\">https://distribution.github.io/distribution/</a></p>\n</blockquote>\n<p>When you pull an image from the docker hub, you are pulling from an image registry. You can run this registry yourself, which is exactly what I want to do.</p>\n<p>There are hosted registries you can use such as hub.docker.com, or github packages. There are also paid versions like with AWS ECR, or DigitalOcean.</p>\n<p>I want my own because I don&rsquo;t want hobby apps and experiments on a public registry. I also do not want to pay for private registry when I am perfectly capable of running my own. I do not require authentication since it is my home network.</p>\n<hr>\n<p>I do not want this to be a tutorial on how to setup and run the image registry. There are a lot of good resources for that. Instead I want to chat about 3 different ways to run the registry, and the headaches that come with each.</p>\n<p>For a few months I have been running the registry on my home network and deploying some apps from it, but it has felt fragile. Tweaking and turning far more knobs and dials than I had expected. A lot of effort to run the registry where communication between the docker cli and engine was over TLS.</p>\n<p>There are 3 ways to run the registry. Insecurely, Securely with Self-Signed Certificates, and Securely with &ldquo;proper&rdquo; Certificates.</p>\n<h2 id=\"insecurely\">Insecurely</h2>\n<p>This is easy enough. You run the registry, do not create any certs, and then configure each docker installation in your network to use insecure registries. It also mostly just works. What I was running into was that some applications that build docker images that also use buildkit would fail to push the image to the registry. To get passed that required customizing buildkit configuration, but I simply could not get this working correctly (especially with Kamal). Going this route required me to fiddle with may too many knobs that were not the registry itself.</p>\n<p>If I was running the registry on my workstation and only building on my workstation it would be fine.</p>\n<p>Customizing docker run time settings you create a file <code>/etc/docker/daemon.json</code> if it does not already exist. Then you add the following, then restart docker.</p>\n<pre tabindex=\"0\"><code>{\n    &quot;insecure-registries&quot; : [ &quot;registry.lindauerlab.net:5000&quot; ]\n}\n</code></pre><h2 id=\"securely-with-a-self-signed-certificate\">Securely with a self-signed certificate</h2>\n<p>Doable, but annoying to be real blunt. Again, so many knobs and dials to fiddle with that are not the registry or immediately related to me getting work done. Now I am managing the certificates, <em>and</em> distributing to the different machines that need them. I was having some trouble getting this working entirely. Sometimes <code>docker login</code> would fail due to SSL errors. Most of the time buildkit failed to push an image. I realized quickly that I would have to spend more time janitoring certificates than I wanted to, which is to say more than zero percent.</p>\n<h2 id=\"securely-with-certificates-from-letsencrypt\">Securely with certificates from letsencrypt</h2>\n<p>The option I ultimately ended up going with.</p>\n<p>This path has the least amount of friction. None of my VMs are publicly accessible frm the Internet so I went with DNS validation for generating the certs. I did loosen up the permissions for the cert directories so I could mount them in the registry container with less hassle. I probably could run an NGinx proxy on the server and route requests to the registry container and avoid the permissions issue. The trade off I guess is finding which knobs and dials you care to tweak.</p>\n<p>This has far less dials. Far less fiddling. Certbot makes it super easy to get certs. <strong>Most of the fiddling work is done on a single machine now</strong>, instead of across many machines. Overall, just less work. And something I can more easily encapsulate into an ansible playbook.</p>\n<h2 id=\"my-working-registry-project\">My working registry project</h2>\n<p>I am containing this in a project called <code>~/docker-registry</code></p>\n<p>It consists of a <code>docker-compose.yml</code>, a symlink to the letsencrypt certs, and a directory to store the data.</p>\n<p><strong>Generate certificates:</strong></p>\n<pre tabindex=\"0\"><code>sudo certbot certonly --manual --preferred-challenges dns-01 -d registry.lindauerlab.net \n</code></pre><p><strong>Symlink certs directory:</strong></p>\n<pre tabindex=\"0\"><code>sudo ln -s /etc/letsencrypt/live/registry.lindauerlab.net certs\n</code></pre><p><strong>docker-compose.yml</strong></p>\n<pre tabindex=\"0\"><code>services:\n  registry:\n    image: registry:2\n    ports:\n    - &quot;5000:5000&quot;\n    environment:\n      REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data\n      REGISTRY_HTTP_TLS_CERTIFICATE: /certs/fullchain1.pem\n      REGISTRY_HTTP_TLS_KEY: /certs/privkey1.pem\n    volumes:\n      - ./data:/data\n      - ./certs/:/certs\n</code></pre><p>It&rsquo;s important that you use the fullchain for the TLS cert or you will get certificate authority errors.</p>\n<p>Now it&rsquo;s as simple as <code>docker compose up -d</code> and I have a working registry.</p>\n<p>Test with curl -</p>\n<pre tabindex=\"0\"><code>curl https://registry.lindauerlab.net:5000/v2/_catalog\n\n{&quot;repositories&quot;:[&quot;some-test-project-i-already-pushed-up&quot;]}\n</code></pre>",
				"content_text": "Home labs are a great place to learn and tinker with systems. I love it because I get to wear my systems administrator hat. I've been doing a lot of application development lately as well as tinkering with various build & deployment tools for those applications. The best way, in my opinion, is [docker](https://docs.docker.com/). It's just so good, you can package up all of the tools and configurations into a distributable unit, using an open standard. \n\nThose packaged units are [called images in docker](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-an-image/). And in order to distribute them, you need a place to store them. Enter the registry.\n\n> The Registry is a stateless, highly scalable server side application that stores and lets you distribute container images and other content. Learn more about it here -> https://distribution.github.io/distribution/ \n\nWhen you pull an image from the docker hub, you are pulling from an image registry. You can run this registry yourself, which is exactly what I want to do. \n\nThere are hosted registries you can use such as hub.docker.com, or github packages. There are also paid versions like with AWS ECR, or DigitalOcean. \n\nI want my own because I don't want hobby apps and experiments on a public registry. I also do not want to pay for private registry when I am perfectly capable of running my own. I do not require authentication since it is my home network. \n\n---\n\nI do not want this to be a tutorial on how to setup and run the image registry. There are a lot of good resources for that. Instead I want to chat about 3 different ways to run the registry, and the headaches that come with each. \n\nFor a few months I have been running the registry on my home network and deploying some apps from it, but it has felt fragile. Tweaking and turning far more knobs and dials than I had expected. A lot of effort to run the registry where communication between the docker cli and engine was over TLS. \n\nThere are 3 ways to run the registry. Insecurely, Securely with Self-Signed Certificates, and Securely with \"proper\" Certificates. \n\n## Insecurely\n\nThis is easy enough. You run the registry, do not create any certs, and then configure each docker installation in your network to use insecure registries. It also mostly just works. What I was running into was that some applications that build docker images that also use buildkit would fail to push the image to the registry. To get passed that required customizing buildkit configuration, but I simply could not get this working correctly (especially with Kamal). Going this route required me to fiddle with may too many knobs that were not the registry itself. \n\nIf I was running the registry on my workstation and only building on my workstation it would be fine. \n\nCustomizing docker run time settings you create a file `/etc/docker/daemon.json` if it does not already exist. Then you add the following, then restart docker.\n\n```\n{\n    \"insecure-registries\" : [ \"registry.lindauerlab.net:5000\" ]\n}\n```\n\n## Securely with a self-signed certificate\n\nDoable, but annoying to be real blunt. Again, so many knobs and dials to fiddle with that are not the registry or immediately related to me getting work done. Now I am managing the certificates, _and_ distributing to the different machines that need them. I was having some trouble getting this working entirely. Sometimes `docker login` would fail due to SSL errors. Most of the time buildkit failed to push an image. I realized quickly that I would have to spend more time janitoring certificates than I wanted to, which is to say more than zero percent. \n\n## Securely with certificates from letsencrypt\n\nThe option I ultimately ended up going with. \n\nThis path has the least amount of friction. None of my VMs are publicly accessible frm the Internet so I went with DNS validation for generating the certs. I did loosen up the permissions for the cert directories so I could mount them in the registry container with less hassle. I probably could run an NGinx proxy on the server and route requests to the registry container and avoid the permissions issue. The trade off I guess is finding which knobs and dials you care to tweak. \n\nThis has far less dials. Far less fiddling. Certbot makes it super easy to get certs. **Most of the fiddling work is done on a single machine now**, instead of across many machines. Overall, just less work. And something I can more easily encapsulate into an ansible playbook. \n\n\n\n## My working registry project\n\nI am containing this in a project called `~/docker-registry`\n\nIt consists of a `docker-compose.yml`, a symlink to the letsencrypt certs, and a directory to store the data.\n\n**Generate certificates:**\n\n```\nsudo certbot certonly --manual --preferred-challenges dns-01 -d registry.lindauerlab.net \n```\n**Symlink certs directory:**\n\n```\nsudo ln -s /etc/letsencrypt/live/registry.lindauerlab.net certs\n```\n**docker-compose.yml**\n\n```\nservices:\n  registry:\n    image: registry:2\n    ports:\n    - \"5000:5000\"\n    environment:\n      REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data\n      REGISTRY_HTTP_TLS_CERTIFICATE: /certs/fullchain1.pem\n      REGISTRY_HTTP_TLS_KEY: /certs/privkey1.pem\n    volumes:\n      - ./data:/data\n      - ./certs/:/certs\n```\nIt's important that you use the fullchain for the TLS cert or you will get certificate authority errors. \n\nNow it's as simple as `docker compose up -d` and I have a working registry. \n\nTest with curl - \n\n```\ncurl https://registry.lindauerlab.net:5000/v2/_catalog\n\n{\"repositories\":[\"some-test-project-i-already-pushed-up\"]}\n```\n",
				"date_published": "2024-11-12T12:17:35-08:00",
				"url": "https://roylindauer.dev/2024/11/12/running-a-docker.html",
				"tags": ["devops","sysadmin","docker"]
			},
			{
				"id": "http://royldev.micro.blog/2024/06/04/form-objects-in.html",
				"title": "Form Objects in Ruby on Rails",
				"content_html": "<!-- raw HTML omitted -->\n<p>In Ruby on Rails, a Form Object is a design pattern used to handle the complexity of forms that don&rsquo;t map directly to a single model or have unique/complex validation and business logic. It encapsulates the logic related to form processing, validation, and data persistence, often combining attributes from multiple models or handling data that doesn&rsquo;t fit neatly into the traditional ActiveRecord model structure.</p>\n<p>Consider a UsersController that lets you create or edit a user. Typical CRUD. The form to create and edit a User may require a username or email, password, a full name. But then now consider an InvitationsController, which will also create a User, but this form may only require an email address. Our model validations for User will become a little gross with conditional validations, potentially virtual attributes to act as flags for model callbacks. Enter the Form Object.</p>\n<p>A Form Object is a new class that sits between the Controller and the Model.</p>\n<h2 id=\"key-characteristics-of-a-form-object-in-rails\">Key Characteristics of a Form Object in Rails:</h2>\n<ul>\n<li><!-- raw HTML omitted -->Encapsulation of Form Logic:<!-- raw HTML omitted --> Form objects centralize the form-related logic, keeping controllers and models cleaner by moving validation and other form-specific methods into a dedicated class.</li>\n<li><!-- raw HTML omitted -->Composite Forms:<!-- raw HTML omitted --> They are useful when a form interacts with multiple models. For example, a user registration form might need to create both a User and a Profile model.</li>\n<li><!-- raw HTML omitted -->Custom Validations:<!-- raw HTML omitted --> Form objects can have their own validations independent of the underlying models. This allows for more granular control over the validation process.</li>\n<li><!-- raw HTML omitted -->Data Persistence:<!-- raw HTML omitted --> While form objects can validate and manipulate data, they usually delegate the actual persistence of data to the relevant models.</li>\n<li><!-- raw HTML omitted -->Reusable Logic:<!-- raw HTML omitted --> By centralizing form logic, form objects can be reused across different parts of the application, making the codebase more maintainable and DRY (Don&rsquo;t Repeat Yourself).</li>\n</ul>\n<p>Active Model provides features to implement input filtering and validation. We can lean on Active Model to craft our Form Object, called <code>ApplicationForm</code>.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">ApplicationForm</span>\n  <span style=\"color:#66d9ef\">include</span> <span style=\"color:#66d9ef\">ActiveModel</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">API</span>\n  <span style=\"color:#66d9ef\">include</span> <span style=\"color:#66d9ef\">ActiveModel</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Attributes</span>\n\n  <span style=\"color:#66d9ef\">class</span> <span style=\"color:#f92672\">&lt;&lt;</span> self\n\n    <span style=\"color:#75715e\"># This method is used in the controller to create a new instance of the form</span>\n    <span style=\"color:#75715e\"># and populate it with the params from the request.</span>\n    <span style=\"color:#75715e\"># Usage: Lead::LeadGenForm.from(params.require(:lead_lead_gen_form))</span>\n    <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">from</span>(params)\n      <span style=\"color:#66d9ef\">new</span>(params<span style=\"color:#f92672\">.</span>permit(attribute_names<span style=\"color:#f92672\">.</span>map(<span style=\"color:#f92672\">&amp;</span><span style=\"color:#e6db74\">:to_sym</span>)))\n    <span style=\"color:#66d9ef\">end</span>\n  <span style=\"color:#66d9ef\">end</span>\n\n  <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">save</span>\n    <span style=\"color:#66d9ef\">return</span> <span style=\"color:#66d9ef\">false</span> <span style=\"color:#66d9ef\">unless</span> valid?\n\n    with_transaction <span style=\"color:#66d9ef\">do</span>\n      submit!\n    <span style=\"color:#66d9ef\">end</span>\n  <span style=\"color:#66d9ef\">end</span>\n\n  <span style=\"color:#66d9ef\">private</span>\n\n  <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">with_transaction</span>(<span style=\"color:#f92672\">&amp;</span>)\n    <span style=\"color:#66d9ef\">ApplicationRecord</span><span style=\"color:#f92672\">.</span>transaction(<span style=\"color:#f92672\">&amp;</span>)\n  <span style=\"color:#66d9ef\">end</span>\n\n  <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">submit!</span>\n    <span style=\"color:#66d9ef\">raise</span> <span style=\"color:#66d9ef\">NotImplementedError</span>\n  <span style=\"color:#66d9ef\">end</span>\n<span style=\"color:#66d9ef\">end</span>\n\n</code></pre></div><p>We include a couple of ActiveModel concerns.</p>\n<p><!-- raw HTML omitted -->ActiveModel::API<!-- raw HTML omitted --> automatically gives us validation support! From the docs:</p>\n<blockquote>\n<p>Includes the required interface for an object to interact with Action Pack and Action View, using different Active Model modules. It includes model name introspections, conversions, translations, and validations. Besides that, it allows you to initialize the object with a hash of attributes, pretty much like Active Record does.</p>\n</blockquote>\n<p>Super useful for us.</p>\n<p>We also include <!-- raw HTML omitted -->ActiveModel::Attributes.<!-- raw HTML omitted --> Basically, this provides a DSL for defining a form object schema and parameter types.</p>\n<blockquote>\n<p>The Attributes module allows models to define attributes beyond simple Ruby readers and writers. Similar to Active Record attributes, which are typically inferred from the database schema, Active Model Attributes are aware of data types, can have default values, and can handle casting and serialization.<!-- raw HTML omitted --><!-- raw HTML omitted -->To use Attributes, include the module in your model class and define your attributes using the attribute macro. It accepts a name, a type, a default value, and any other options supported by the attribute type.</p>\n</blockquote>\n<p>The attribute method provided by ActiveModel::Attributes is used to define our form inputs.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">Lead</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">LeadGenForm</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#66d9ef\">ApplicationForm</span>\n  attribute <span style=\"color:#e6db74\">:name</span>, <span style=\"color:#e6db74\">:string</span>\n  attribute <span style=\"color:#e6db74\">:email</span>, <span style=\"color:#e6db74\">:string</span>\n  attribute <span style=\"color:#e6db74\">:message</span>, <span style=\"color:#e6db74\">:string</span>\n\n  validates <span style=\"color:#e6db74\">:name</span>, <span style=\"color:#e6db74\">:email</span>, <span style=\"color:#e6db74\">:message</span>, <span style=\"color:#e6db74\">presence</span>: <span style=\"color:#66d9ef\">true</span>\n  validates <span style=\"color:#e6db74\">:email</span>, format: {<span style=\"color:#e6db74\">with</span>: <span style=\"color:#66d9ef\">URI</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">MailTo</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">EMAIL_REGEXP</span>}\n\n  <span style=\"color:#66d9ef\">attr_accessor</span> <span style=\"color:#e6db74\">:lead</span>\n\n  <span style=\"color:#66d9ef\">private</span>\n\n  <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">submit!</span>\n    @lead <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">Lead</span><span style=\"color:#f92672\">.</span>new({name: name, <span style=\"color:#e6db74\">email</span>: email, <span style=\"color:#e6db74\">message</span>: message})\n    @lead<span style=\"color:#f92672\">.</span>save!\n    <span style=\"color:#66d9ef\">LeadsMailer</span><span style=\"color:#f92672\">.</span>contact_form_auto_responder(<span style=\"color:#e6db74\">lead</span>: lead)<span style=\"color:#f92672\">.</span>deliver_later\n    <span style=\"color:#66d9ef\">LeadsMailer</span><span style=\"color:#f92672\">.</span>contact_form_notification(<span style=\"color:#e6db74\">lead</span>: lead)<span style=\"color:#f92672\">.</span>deliver_later\n  <span style=\"color:#66d9ef\">end</span>\n<span style=\"color:#66d9ef\">end</span>\n</code></pre></div><p>Now in our Controller we can instantiate a new Form Object and use it in our view! Since the Form Object belongs to the presentation layer they can be used in view templates.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">create</span>\n    @lead <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">Lead</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">LeadGenForm</span><span style=\"color:#f92672\">.</span>from(params<span style=\"color:#f92672\">.</span>require(<span style=\"color:#e6db74\">:lead_lead_gen_form</span>))\n\n    <span style=\"color:#66d9ef\">if</span> @lead<span style=\"color:#f92672\">.</span>save\n      redirect_to <span style=\"color:#e6db74\">:thank_you</span>\n    <span style=\"color:#66d9ef\">else</span>\n      render <span style=\"color:#e6db74\">:new</span>, <span style=\"color:#e6db74\">status</span>: <span style=\"color:#e6db74\">:unprocessable_entity</span>\n    <span style=\"color:#66d9ef\">end</span>\n  <span style=\"color:#66d9ef\">end</span>\n</code></pre></div><p>We can use the Form Object just as we would use a model in our view:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"color:#f92672\">&lt;%=</span> form_with <span style=\"color:#e6db74\">model</span>: @lead, <span style=\"color:#e6db74\">url</span>: lead_gen_path <span style=\"color:#66d9ef\">do</span> <span style=\"color:#f92672\">|</span>form<span style=\"color:#f92672\">|</span> <span style=\"color:#f92672\">%&gt;</span>\n</code></pre></div><p>One thing I particularly like about this pattern is that I can easily write tests for my Form Objects and do not have to juggle as many model callbacks (and deal with that complexity in tests).</p>\n<p>Using Form Objects means I can achieve cleaner, more maintainable code, especially in complex form scenarios.</p>\n",
				"content_text": "<img src=\"uploads/2024/image.png\" alt=\"\" width=\"100%\">\n\nIn Ruby on Rails, a Form Object is a design pattern used to handle the complexity of forms that don't map directly to a single model or have unique/complex validation and business logic. It encapsulates the logic related to form processing, validation, and data persistence, often combining attributes from multiple models or handling data that doesn't fit neatly into the traditional ActiveRecord model structure.\n\nConsider a UsersController that lets you create or edit a user. Typical CRUD. The form to create and edit a User may require a username or email, password, a full name. But then now consider an InvitationsController, which will also create a User, but this form may only require an email address. Our model validations for User will become a little gross with conditional validations, potentially virtual attributes to act as flags for model callbacks. Enter the Form Object.\n\nA Form Object is a new class that sits between the Controller and the Model.\n\n## Key Characteristics of a Form Object in Rails:\n\n* <strong>Encapsulation of Form Logic:</strong> Form objects centralize the form-related logic, keeping controllers and models cleaner by moving validation and other form-specific methods into a dedicated class.\n* <strong>Composite Forms:</strong> They are useful when a form interacts with multiple models. For example, a user registration form might need to create both a User and a Profile model.\n* <strong>Custom Validations:</strong> Form objects can have their own validations independent of the underlying models. This allows for more granular control over the validation process.\n* <strong>Data Persistence:</strong> While form objects can validate and manipulate data, they usually delegate the actual persistence of data to the relevant models.\n* <strong>Reusable Logic:</strong> By centralizing form logic, form objects can be reused across different parts of the application, making the codebase more maintainable and DRY (Don't Repeat Yourself).\n\nActive Model provides features to implement input filtering and validation. We can lean on Active Model to craft our Form Object, called `ApplicationForm`.\n\n```ruby\nclass ApplicationForm\n  include ActiveModel::API\n  include ActiveModel::Attributes\n\n  class << self\n\n    # This method is used in the controller to create a new instance of the form\n    # and populate it with the params from the request.\n    # Usage: Lead::LeadGenForm.from(params.require(:lead_lead_gen_form))\n    def from(params)\n      new(params.permit(attribute_names.map(&:to_sym)))\n    end\n  end\n\n  def save\n    return false unless valid?\n\n    with_transaction do\n      submit!\n    end\n  end\n\n  private\n\n  def with_transaction(&)\n    ApplicationRecord.transaction(&)\n  end\n\n  def submit!\n    raise NotImplementedError\n  end\nend\n\n```\nWe include a couple of ActiveModel concerns.\n\n<a href=\"https://api.rubyonrails.org/classes/ActiveModel/API.html\" target=\"_blank\" rel=\"noopener\">ActiveModel::API</a> automatically gives us validation support! From the docs:\n\n> Includes the required interface for an object to interact with Action Pack and Action View, using different Active Model modules. It includes model name introspections, conversions, translations, and validations. Besides that, it allows you to initialize the object with a hash of attributes, pretty much like Active Record does.\n\nSuper useful for us.\n\nWe also include <a href=\"https://api.rubyonrails.org/classes/ActiveModel/Attributes.html\" target=\"_blank\" rel=\"noopener\">ActiveModel::Attributes.</a> Basically, this provides a DSL for defining a form object schema and parameter types.\n\n> The Attributes module allows models to define attributes beyond simple Ruby readers and writers. Similar to Active Record attributes, which are typically inferred from the database schema, Active Model Attributes are aware of data types, can have default values, and can handle casting and serialization.<br><br>To use Attributes, include the module in your model class and define your attributes using the attribute macro. It accepts a name, a type, a default value, and any other options supported by the attribute type.\n\nThe attribute method provided by ActiveModel::Attributes is used to define our form inputs.\n\n```ruby\nclass Lead::LeadGenForm < ApplicationForm\n  attribute :name, :string\n  attribute :email, :string\n  attribute :message, :string\n\n  validates :name, :email, :message, presence: true\n  validates :email, format: {with: URI::MailTo::EMAIL_REGEXP}\n\n  attr_accessor :lead\n\n  private\n\n  def submit!\n    @lead = Lead.new({name: name, email: email, message: message})\n    @lead.save!\n    LeadsMailer.contact_form_auto_responder(lead: lead).deliver_later\n    LeadsMailer.contact_form_notification(lead: lead).deliver_later\n  end\nend\n```\nNow in our Controller we can instantiate a new Form Object and use it in our view! Since the Form Object belongs to the presentation layer they can be used in view templates.\n\n```ruby\ndef create\n    @lead = Lead::LeadGenForm.from(params.require(:lead_lead_gen_form))\n\n    if @lead.save\n      redirect_to :thank_you\n    else\n      render :new, status: :unprocessable_entity\n    end\n  end\n```\nWe can use the Form Object just as we would use a model in our view:\n\n```ruby\n<%= form_with model: @lead, url: lead_gen_path do |form| %>\n```\nOne thing I particularly like about this pattern is that I can easily write tests for my Form Objects and do not have to juggle as many model callbacks (and deal with that complexity in tests).\n\nUsing Form Objects means I can achieve cleaner, more maintainable code, especially in complex form scenarios.\n",
				"date_published": "2024-06-04T10:32:28-08:00",
				"url": "https://roylindauer.dev/2024/06/04/form-objects-in.html"
			},
			{
				"id": "http://royldev.micro.blog/2024/06/02/variable-swapping-in.html",
				"title": "Variable swapping in Ruby",
				"content_html": "<p>Swapping variables is a fundamental concept in programming, often encountered in algorithm development and problem-solving scenarios. While the process is straightforward in many languages, Ruby offers its own particularly elegant and concise syntax for this task. Let&rsquo;s first explore the more traditional approaches.</p>\n<p>First, you might use a temporary variable.</p>\n<pre tabindex=\"0\"><code>a = 10\nb = 12\nputs &quot;a: #{a}, b: #{b}&quot;\n\ntmp = a \na = b\nb = tmp\n\nputs &quot;a: #{a}, b: #{b}&quot;\n</code></pre><p>Outputs:</p>\n<pre tabindex=\"0\"><code>a: 10, b: 12\na: 12, b: 10\n</code></pre><p>A more clever way to do this is with the Bitwise XOR <code>^</code> operator.</p>\n<pre tabindex=\"0\"><code>a = 10\nb = 12\nputs &quot;a: #{a}, b: #{b}&quot;\n\na = a ^ b\nb = a ^ b\na = a ^ b\n\nputs &quot;a: #{a}, b: #{b}&quot;\n</code></pre><p>outputs:</p>\n<pre tabindex=\"0\"><code>a: 10, b: 12\na: 12, b: 10\n</code></pre><p>Weird, and looks like magic. Why would calling the same operation 3 times work? The <!-- raw HTML omitted -->^<!-- raw HTML omitted --> operator is a method on <!-- raw HTML omitted -->Integer<!-- raw HTML omitted -->, <!-- raw HTML omitted -->TrueClass<!-- raw HTML omitted -->, <!-- raw HTML omitted -->FalseClass<!-- raw HTML omitted -->, and <!-- raw HTML omitted -->NilClass<!-- raw HTML omitted --> that performs XOR.</p>\n<p>The trick here is that XOR <em>does not lose information</em>.</p>\n<p>The way I understand it is it basically does something like:</p>\n<pre tabindex=\"0\"><code>a = 1\nb = 2\nputs &quot;a: #{a}, b: #{b}&quot;\n\na = a + b # 1 + 2 = 3, a = 3\nb = a - b # 3 - 2 = 1, b = 1\na = a - b # 3 - 1 = 2, a = 2\n\nputs &quot;a: #{a}, b: #{b}&quot;\n</code></pre><h2 id=\"the-ruby-way\">The Ruby Way</h2>\n<p>The Ruby way to do this is actually super elegant! Using <!-- raw HTML omitted -->multiple assignment<!-- raw HTML omitted --> we can just swap the variables.</p>\n<pre tabindex=\"0\"><code>a = 1\nb = 2\nputs &quot;a: #{a}, b: #{b}&quot; # a: 1, b: 2\n\n# Swapping variables using multiple assignment\na, b = b, a\nputs &quot;a: #{a}, b: #{b}&quot; # a: 2, b: 1\n</code></pre><h2 id=\"strings\">Strings</h2>\n<p>What about strings? You cannot subtract a string in Ruby.</p>\n<pre tabindex=\"0\"><code>&quot;string&quot; ^ &quot;another string&quot;\nundefined method `^' for &quot;string&quot;:String (NoMethodError)\n</code></pre><p>Could do something like concatenate and substring but that just seems like it&rsquo;s missing the point. Maybe use a temporary variable for strings.</p>\n<pre tabindex=\"0\"><code>a = &quot;first&quot;\nb = &quot;second&quot;\nputs &quot;a: #{a}, b: #{b}&quot; # a: first, b: second\n\na = a + b # firstsecond\nb = a[0..((a.length - b.length)-1)] # first\na = a[b.length..a.length] # second \n\nputs &quot;a: #{a}, b: #{b}&quot; # a: second, b: first\n</code></pre><p>Ruby being awesome though, really just use multiple assignment :D</p>\n<h2 id=\"update\">Update</h2>\n<p>Okay I am more clear on how XOR actually works.</p>\n<p>&gt; XOR is a binary operation, it stands for &ldquo;exclusive or&rdquo;, that is to say the resulting bit evaluates to one if only exactly one of the bits is set.</p>\n<p>a = a ^ b means, if a is 0 or b is 0, return 0. If a is 1 OR b is 1, return 1, if a is 1 and b is 1, return 0. Using our example values of a = 10 and b = 12, and we convert to binary: a = 1010, and b = 1100.</p>\n<p>The math looks like:</p>\n<pre tabindex=\"0\"><code>a = a ^ b # 1010 ^ 1100 = 0110 (converted to decimal: 10 ^ 12 = 6)\nb = a ^ b # 0110 ^ 1100 = 1010 (6 ^ 12 = 10)\na = a ^ b # 0110 ^ 1010 = 1100 (6 ^ 10 = 12)\n</code></pre><p>Or, a is now 12, and b is now 10.</p>\n",
				"content_text": "Swapping variables is a fundamental concept in programming, often encountered in algorithm development and problem-solving scenarios. While the process is straightforward in many languages, Ruby offers its own particularly elegant and concise syntax for this task. Let's first explore the more traditional approaches.\n\nFirst, you might use a temporary variable.\n\n```\na = 10\nb = 12\nputs \"a: #{a}, b: #{b}\"\n\ntmp = a \na = b\nb = tmp\n\nputs \"a: #{a}, b: #{b}\"\n```\n\nOutputs:\n\n```\na: 10, b: 12\na: 12, b: 10\n```\n\nA more clever way to do this is with the Bitwise XOR `^` operator.\n\n```\na = 10\nb = 12\nputs \"a: #{a}, b: #{b}\"\n\na = a ^ b\nb = a ^ b\na = a ^ b\n\nputs \"a: #{a}, b: #{b}\"\n```\n\noutputs:\n\n```\na: 10, b: 12\na: 12, b: 10\n```\n\nWeird, and looks like magic. Why would calling the same operation 3 times work? The <code>^</code> operator is a method on <code>Integer</code>, <code>TrueClass</code>, <code>FalseClass</code>, and <code>NilClass</code> that performs XOR.\n\nThe trick here is that XOR _does not lose information_.\n\nThe way I understand it is it basically does something like:\n\n```\na = 1\nb = 2\nputs \"a: #{a}, b: #{b}\"\n\na = a + b # 1 + 2 = 3, a = 3\nb = a - b # 3 - 2 = 1, b = 1\na = a - b # 3 - 1 = 2, a = 2\n\nputs \"a: #{a}, b: #{b}\"\n```\n\n## The Ruby Way\n\nThe Ruby way to do this is actually super elegant! Using <a href=\"https://docs.ruby-lang.org/en/3.2/syntax/assignment_rdoc.html#label-Multiple+Assignment\" target=\"_blank\" rel=\"noopener\">multiple assignment</a> we can just swap the variables.\n\n```\na = 1\nb = 2\nputs \"a: #{a}, b: #{b}\" # a: 1, b: 2\n\n# Swapping variables using multiple assignment\na, b = b, a\nputs \"a: #{a}, b: #{b}\" # a: 2, b: 1\n```\n\n## Strings\n\nWhat about strings? You cannot subtract a string in Ruby.\n\n```\n\"string\" ^ \"another string\"\nundefined method `^' for \"string\":String (NoMethodError)\n```\n\nCould do something like concatenate and substring but that just seems like it's missing the point. Maybe use a temporary variable for strings.\n\n```\na = \"first\"\nb = \"second\"\nputs \"a: #{a}, b: #{b}\" # a: first, b: second\n\na = a + b # firstsecond\nb = a[0..((a.length - b.length)-1)] # first\na = a[b.length..a.length] # second \n\nputs \"a: #{a}, b: #{b}\" # a: second, b: first\n```\n\nRuby being awesome though, really just use multiple assignment :D\n\n## Update\n\nOkay I am more clear on how XOR actually works.\n\n&gt; XOR is a binary operation, it stands for \"exclusive or\", that is to say the resulting bit evaluates to one if only exactly one of the bits is set.\n\na = a ^ b means, if a is 0 or b is 0, return 0. If a is 1 OR b is 1, return 1, if a is 1 and b is 1, return 0. Using our example values of a = 10 and b = 12, and we convert to binary: a = 1010, and b = 1100.\n\nThe math looks like:\n\n```\na = a ^ b # 1010 ^ 1100 = 0110 (converted to decimal: 10 ^ 12 = 6)\nb = a ^ b # 0110 ^ 1100 = 1010 (6 ^ 12 = 10)\na = a ^ b # 0110 ^ 1010 = 1100 (6 ^ 10 = 12)\n```\n\nOr, a is now 12, and b is now 10.\n",
				"date_published": "2024-06-02T20:49:06-08:00",
				"url": "https://roylindauer.dev/2024/06/02/variable-swapping-in.html"
			},
			{
				"id": "http://royldev.micro.blog/2024/06/02/quirky-ruby-feature.html",
				"title": "Quirky Ruby Feature - Mixing Code and Data",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<p><strong>END</strong>\nthis a line\nthis another line\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n",
				"content_text": "<img src=\"uploads/2024/20f468108c.png\" alt=\"\" width=\"100%\">\n\n<p>Ruby has some fun quirky features. One of them is that you can mix code and data in a single file using the special keywords <code>__END__</code> and <code>DATA</code>. This is a weird concept, but Ruby allows you to use the script itself as a source of data.</p>\n<p>References:</p>\n<ul>\n<li><a href=\"https://docs.ruby-lang.org/en/3.0/Object.html#DATA\" target=\"_blank\" rel=\"noopener\">https://docs.ruby-lang.org/en/3.0/Object.html#DATA</a></li>\n<li><a href=\"https://ruby-doc.org/docs/keywords/1.9/Object.html#method-i-__END__\" target=\"_blank\" rel=\"noopener\">https://ruby-doc.org/docs/keywords/1.9/Object.html#method-i-__END__</a></li>\n</ul>\n<p>The documentation says about <code>__END__</code>:</p>\n<blockquote>\n<p>Denotes the end of the regular source code section of a program file. Lines below __END__ will not be executed. Those lines will be available via the special filehandle DATA. The following code will print out two stanzas of personal information. Note that __END__ has to be flush left, and has to be the only thing on its line.</p>\n</blockquote>\n<p>The way I use this is for one-off scripts. For example, doing coding challenges or demos. I used this extensively when doing the Advent of Code last year. I had contained the dataset and the logic for each daily exercise in a single script.</p>\n<pre class=\"language-ruby\"><code>DATA.each do | line |\n    p line.chomp\nend\n\n__END__\nthis a line\nthis another line\n</code></pre>\n<p>This will print out each line below <code>__END__.</code> Pretty neat!</p>\n<p>Basically, Ruby ignores everything after <code>__END__</code> when executing the script, BUT all of the data becomes available in the special object <code>DATA</code>, which is actually a <a href=\"https://docs.ruby-lang.org/en/3.0/File.html\" target=\"_blank\" rel=\"noopener\">File object</a>.</p>\n",
				"date_published": "2024-06-02T00:30:05-08:00",
				"url": "https://roylindauer.dev/2024/06/02/quirky-ruby-feature.html"
			},
			{
				"id": "http://royldev.micro.blog/2024/04/15/how-to-change.html",
				"title": "How to change a Users Password via Tinker in Laravel",
				"content_html": "<!-- raw HTML omitted -->\n",
				"content_text": "<p>Laravel has a pretty great console through a REPL called Tinker.</p>\n<p>From Tinker you can interact with your models.</p>\n<pre class=\"language-php\"><code>$user = App\\User::where('email', 'their@email.com')-&gt;first();\n$user-&gt;password = Hash::make('their new password');\n$user-&gt;save();</code></pre>\n<p>Super convenient! You just need SSH access to a server in the environment in which the app is running. I like to setup an \"ops\" box that lives in my production environment that is only accessible through a VPN. This gives me a management console to my app for emergencies or whatever!</p>\n<p>Even better is to launch a docker container on demand that only runs for the session and is killed when you exit Tinker. Anyways, I have become very accustomed to the Ruby on Rails console so having Tinker around is incredibly comforting :D</p>\n",
				"date_published": "2024-04-15T16:27:18-08:00",
				"url": "https://roylindauer.dev/2024/04/15/how-to-change.html"
			},
			{
				"id": "http://royldev.micro.blog/2024/04/14/a-docker-based.html",
				"title": "A docker based setup for testing with Laravel",
				"content_html": "<!-- raw HTML omitted -->\n",
				"content_text": "<p>I experienced some annoying issues while running integration tests in a Laravel app. The <a href=\"https://hub.docker.com/_/mysql\" target=\"_blank\" rel=\"noopener\">official MySQL docker image</a> will create a user and a database for you which is very convenient, but that user does not have permission to create new databases. I configure my applications to use a separate database for testing, usually with a _testing suffix and so just hit a brick wall.</p>\n<p>The solution was to mount an entrypoint script, basically, some SQL statements I want to execute when the container is created. The script will create all of the necessary databases I need.</p>\n<p>The file: <strong>config/create-databases.sql</strong></p>\n<pre class=\"language-shell\"><code>CREATE DATABASE IF NOT EXISTS app DEFAULT CHARACTER SET = utf8;\nCREATE DATABASE IF NOT EXISTS app_testing DEFAULT CHARACTER SET = utf8;\nGRANT ALL PRIVILEGES ON app.* TO 'admin'@'%';\nGRANT ALL PRIVILEGES ON app_testing.* TO 'admin'@'%';\nFLUSH PRIVILEGES;</code></pre>\n<p>My <strong>docker-compose.yml</strong></p>\n<pre class=\"language-shell\"><code>services:\n    db:\n        image: mysql/mysql-server:8.0.28\n        environment:\n            MYSQL_ROOT_PASSWORD: root\n            MYSQL_DATABASE: app\n            MYSQL_USER: admin\n            MYSQL_PASSWORD: password\n        cap_add:\n            - SYS_NICE\n        ports:\n            - \"127.0.0.1:3306:3306\"\n        volumes:\n            - db:/var/lib/mysql\n            - ./config/create-databases.sql:/docker-entrypoint-initdb.d/create-databases.sql\nvolumes:\n    db:\n</code></pre>\n<p>Then into <code>.env.testing</code> I set <code>DB_DATABASE=app_testing</code> and now I can setup the test database and execute tests.</p>\n<pre class=\"language-shell\"><code>APP_ENV=testing php artisan migrate\nphp artisan test\n</code></pre>\n<p>Success!</p>\n",
				"date_published": "2024-04-14T20:15:32-08:00",
				"url": "https://roylindauer.dev/2024/04/14/a-docker-based.html"
			},
			{
				"id": "http://royldev.micro.blog/2024/04/14/add-taggable-support.html",
				"title": "Add taggable support to my personal blog",
				"content_html": "<!-- raw HTML omitted -->\n<pre><code>def process(element)\n  return if element[:tags].nil?\n  return unless element.tag_list.empty?\n\n  element.tag_list = element[:tags]\n  element.save\nend\n</code></pre>\n<p>end\nend\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<pre><code>  let(:blog) { create(:blog) }\n\n  let(:element) {\n    blog[:tags] = %w[tag1 tag2]\n    blog\n  }\n\n  it &quot;updates the tag list&quot; do\n    process\n    expect(element.reload.tag_list).to eq(%w[tag1 tag2])\n  end\n\n  context &quot;when the old tags field is nil&quot; do\n    let(:element) {\n      blog[:tags] = nil\n      blog\n    }\n\n    it &quot;does not update the tags&quot; do\n      process\n      expect(element.reload.tag_list).to eq([])\n    end\n  end\n\n  context &quot;when the blog post already has a tag list&quot; do\n    let(:element) {\n      blog[:tags] = %w[tag1 tag2]\n      create(:blog_with_tags, tag_list: %w[tag3 tag4])\n    }\n\n    it &quot;does not update the record&quot; do\n      process\n      expect(element.reload.updated_at).to eq(element.updated_at)\n      expect(element.reload.tag_list).to eq(%w[tag3 tag4])\n    end\n  end\nend\n</code></pre>\n<p>end\nend\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>Finished in 2.52 seconds (files took 1.48 seconds to load)\n92 examples, 0 failures, 8 pending\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n",
				"content_text": "<p>Today I added taggable support to my blog. I do have blog posts tagged, but it's a simple comma-separated list in the database. I want to have a more robust system that allows me to query and filter by tags.</p>\n<p>I used the <a href=\"https://github.com/mbleigh/acts-as-taggable-on/\" target=\"_blank\" rel=\"noopener\">ruby gem <code>acts-as-taggable-on</code></a> to add taggable support to my blog.</p>\n<p>First thing was to install the gem by adding it to the Gemfile and running <code>bundle</code>.</p>\n<pre class=\"language-ruby\"><code>gem \"acts-as-taggable-on\", \"~&gt; 10.0\"</code></pre>\n<p>Generate and run database migrations</p>\n<pre class=\"language-shell\"><code>$ bundle exec rake acts_as_taggable_on_engine:install:migrations\n$ bundle exec rails db:migrate\n</code></pre>\n<p>Don't forget to fail to read the instructions for when you are using MySQL, get some errors, then roll back and do it correctly.</p>\n<pre class=\"language-shell\"><code>$ bundle exec rails db:rollback\n$ bundle exec rake acts_as_taggable_on_engine:tag_names:collate_bin\n</code></pre>\n<p>There is another issue with MySQL. Seems to be an old issues resurfacing...</p>\n<pre class=\"language-shell\"><code>Mysql2::Error: Cannot drop index 'index_taggings_on_tag_id': needed in a foreign key constraint\n</code></pre>\n<p>The issue here is that you have to create an index for a foreign key constraint. It's required by MYSQL. We try to delete the index without dropping the foreign key first.</p>\n<p>Thanks to <a href=\"https://github.com/ndrix\" target=\"_blank\" rel=\"noopener\">ndrix</a> for <a href=\"https://github.com/mbleigh/acts-as-taggable-on/pull/983\" target=\"_blank\" rel=\"noopener\">the solution:</a></p>\n<pre class=\"language-ruby\"><code># https://github.com/mbleigh/acts-as-taggable-on/issues/978\n# remove_index ActsAsTaggableOn.taggings_table, :tag_id if index_exists?(ActsAsTaggableOn.taggings_table, :tag_id)\nif index_exists?(ActsAsTaggableOn.taggings_table, :tag_id)\n  remove_foreign_key :taggings, :tags\n  remove_index ActsAsTaggableOn.taggings_table, :tag_id\nend\n</code></pre>\n<p>Run the migration again, and success! No issues. Time to hook up tags.</p>\n<p>Add <code>acts_as_taggable_on :tags</code> to the Blog model. Update the admin blog form. And update the admin blog controller to accept <code>tag_list</code> in the params.</p>\n<p>Ok now all the fun stuff. I have existing tags in the database as a comma-separated list. I need to migrate these tags to the new taggable system. I am going to use Maintenance Tasks to do this because it's more fun than just hopping into the console and running some queries.</p>\n<p>Here's a very simple task that will migrate the tags from the old tags column to the new taggable system. I have configured soft deletes so I also include deleted records in the collection.</p>\n<pre class=\"language-ruby\"><code>module Maintenance\n  class MigrateBlogTagsTask &lt; MaintenanceTasks::Task\n    def collection\n      Blog.with_deleted.all\n    end\n\n    def process(element)\n      return if element[:tags].nil?\n      return unless element.tag_list.empty?\n\n      element.tag_list = element[:tags]\n      element.save\n    end\n  end\nend\n</code></pre>\n<p>Don't forget to create some tests for the maintenance task. Here's a spec that covers the happy path and a couple of edge cases:</p>\n<pre class=\"language-ruby\"><code>module Maintenance\n  RSpec.describe MigrateBlogTagsTask do\n    describe \"#process\" do\n      subject(:process) { described_class.process(element) }\n\n      let(:blog) { create(:blog) }\n\n      let(:element) {\n        blog[:tags] = %w[tag1 tag2]\n        blog\n      }\n\n      it \"updates the tag list\" do\n        process\n        expect(element.reload.tag_list).to eq(%w[tag1 tag2])\n      end\n\n      context \"when the old tags field is nil\" do\n        let(:element) {\n          blog[:tags] = nil\n          blog\n        }\n\n        it \"does not update the tags\" do\n          process\n          expect(element.reload.tag_list).to eq([])\n        end\n      end\n\n      context \"when the blog post already has a tag list\" do\n        let(:element) {\n          blog[:tags] = %w[tag1 tag2]\n          create(:blog_with_tags, tag_list: %w[tag3 tag4])\n        }\n\n        it \"does not update the record\" do\n          process\n          expect(element.reload.updated_at).to eq(element.updated_at)\n          expect(element.reload.tag_list).to eq(%w[tag3 tag4])\n        end\n      end\n    end\n  end\nend\n</code></pre>\n<p>Run the maintenance task to migrate the tags.</p>\n<pre class=\"language-ruby\"><code>$ bundle exec maintenance_tasks perform Maintenance::MigrateBlogTagsTask\n</code></pre>\n<p>I do have some existing tests for my Blog model and requests. I need to update these tests to use the new taggable system. It's mostly some minor changes to the tests to use the new tag_list attribute. Pretty easy to update. I made the necessary updates, and my tests passed:</p>\n<pre class=\"language-shell\"><code>$ bundle exec rspec\n\nFinished in 2.52 seconds (files took 1.48 seconds to load)\n92 examples, 0 failures, 8 pending\n</code></pre>\n<p>And I think that's all for this session. I have better tags.</p>\n<p>Next steps would be to:</p>\n<ul>\n<li>Create a Stimulus Controller to create auto suggestion for tags when editing or creating a blog entry</li>\n<li>Roll out to Project model</li>\n<li>Clean up and remove the old fields from my tables and old model Concerns</li>\n</ul>\n",
				"date_published": "2024-04-14T19:59:41-08:00",
				"url": "https://roylindauer.dev/2024/04/14/add-taggable-support.html"
			},
			{
				"id": "http://royldev.micro.blog/2024/04/13/trying-to-dispatch.html",
				"title": "Trying to Dispatch Jobs via Tinker with Laravel and SQLite",
				"content_html": "<!-- raw HTML omitted -->\n<p>$recipe = Recipe::find(5);</p>\n<p>[!] Aliasing &lsquo;Recipe&rsquo; to &lsquo;App\\Models\\Recipe&rsquo; for this Tinker session.</p>\n<p>Illuminate\\Database\\QueryException  SQLSTATE[HY000]: General error: 10 disk I/O error (Connection: sqlite, SQL: select * from &ldquo;recipes&rdquo; where &ldquo;recipes&rdquo;.&ldquo;id&rdquo; = 17 limit 1).<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>Illuminate\\Database\\QueryException  SQLSTATE[HY000]: General error: 10 disk I/O error (Connection: sqlite, SQL: select * from &ldquo;users&rdquo;).</p>\n<p>&gt;<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n",
				"content_text": "<p>I've been learning Laravel queues and jobs. While creating some jobs I wanted a quick way to test by executing the job. I just want to see it run. There wasn't a super clear path for me here so my first thought was let's just quickly create a one off controller action. To replay that job over again I just had to refresh my browser. Not great, super odd, but also not the end of the world. Yes, of course I could write some actual tests (and I will), but I really just want a way to execute services and such in my app ad hoc.</p>\n<pre class=\"language-php\"><code>public function testing_this_job()\n{\n   $recipe = Recipe::find(5); \n   ProcessRecipeImport::dispatch($recipe);\n}</code></pre>\n<p>Yeah it works, but is <em>_super_</em> clunky. Let's try tinker.</p>\n<pre class=\"language-shell\"><code>php artisan tinker\n\n$recipe = Recipe::find(5);\n\n[!] Aliasing 'Recipe' to 'App\\Models\\Recipe' for this Tinker session.\n\n   Illuminate\\Database\\QueryException  SQLSTATE[HY000]: General error: 10 disk I/O error (Connection: sqlite, SQL: select * from \"recipes\" where \"recipes\".\"id\" = 17 limit 1).</code></pre>\n<p>Hmm that didn't work. WHY? The SQLite database does exist and does work as I can view data in my app when I run the server. I can also connect to the db via DataGrip and query data. I've tried closing all connections. Still cannot read from tables. Do I need to somehow load db configuration manually when I start a tinker session? At this point I've searched the internet for answers and tried to ChatGPT my way through this to no avail. Totally stuck.</p>\n<p>For being the default database connection I would expect this to just work out of the box.</p>\n<p>Hours of searching and I have come to realize that SQLite is maybe not great and shouldn't be used even for development? I dunno, it's probably fine, but there is _something_ with tinker and psysh that is preventing me from interacting with the database. It smells like the db connection is already opened and when I try to connect I get the error, because of the existing connection. When I connect to the console, then run from my terminal:</p>\n<pre class=\"language-shell\"><code>$ sudo lsof | grep database.sqlite\nphp     74738   roylindauer     7u  REG     1,18    339968  65235486 /Users/roylindauer/code/laravel-app/database/database-copy.sqlite\n</code></pre>\n<p>It's open. So does that mean when I try to query a record it tries to open another connection which fails? Seems that way. Just not sure. And i'm growing impatient. It also fails on a brand new app:</p>\n<pre class=\"language-shell\"><code>$ composer create-project laravel/laravel:^11.0 laravel-testing-sqlite\n$ cd laravel-testing-sqlite\n$ php artisan tinker\nPsy Shell v0.12.3 (PHP 8.3.4  cli) by Justin Hileman\n&gt; User::all()\n[!] Aliasing 'User' to 'App\\Models\\User' for this Tinker session.\n\n   Illuminate\\Database\\QueryException  SQLSTATE[HY000]: General error: 10 disk I/O error (Connection: sqlite, SQL: select * from \"users\").\n\n&gt;</code></pre>\n<p>This has to be a MacOS thing. Or maybe an Apple Silicon thing? I kinda want to try on Ubuntu next. Other things it could be: the version of SQLite I am running? Something with my homebrew setup? I did install PHP 8.3.4 via ASDF maybe the answer lies there.</p>\n<p>Next I tried PHP 8.2.17. I updated my `.tool-versions` file to use PHP 8.2.17 then executed `asdf install`. I was asked to install command line developer tools, which should have already been installed. That's wild. Doing that I guess, then installing PHP 8.2.17 again. No matter I am still prompted to install command line developer tools, which are already installed. I tried a new terminal session. I tried rebooting. No success.</p>\n<p>At this point I am going to do two things: Abandon SQLite, and use MySQL and Docker. <br><br>I am a little disappointed I couldn't figure the issue out with SQLite. <br><br>Running the database through MySQL and Docker and this all works fine! I can finally query my database! So back to my initial problem. How do I dispatch jobs through tinker?</p>\n<pre class=\"language-php\"><code>$recipe = Recipe::find(5); \nProcessRecipeImport::dispatch($recipe);</code></pre>\n<p>This actually does not work! The <a href=\"https://laravel.com/docs/11.x/artisan#usage\" target=\"_blank\" rel=\"noopener\">dispatch helper function depends on garbage collection</a>.</p>\n<blockquote>\n<p>The dispatch helper function and dispatch method on the Dispatchable class depends on garbage collection to place the job on the queue. Therefore, when using tinker, you should use Bus::dispatch or Queue::push to dispatch jobs.</p>\n</blockquote>\n<p>So the correct way to quickly dispatch my job and let me get back to tinkering is:</p>\n<pre class=\"language-php\"><code>$recipe = Recipe::find(5); \n\\Bus::dispatch(new App\\Jobs\\ProcessRecipeImport($recipe));</code></pre>\n<p>Wonderful! Tinker is awesome!</p>\n",
				"date_published": "2024-04-13T21:01:14-08:00",
				"url": "https://roylindauer.dev/2024/04/13/trying-to-dispatch.html"
			},
			{
				"id": "http://royldev.micro.blog/2024/04/01/use-ansible-to.html",
				"title": "Use Ansible to Configure your Workstation",
				"content_html": "<!-- raw HTML omitted -->\n<p>vars_files:\n- vars/main.yml</p>\n<p>roles:\n- geerlingguy.dotfiles\n- geerlingguy.mac.homebrew</p>\n<p>tasks: []<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>homebrew_cask_accept_external_apps: true\nhomebrew_installed_packages:</p>\n<ul>\n<li>libyaml</li>\n<li>curl</li>\n<li>coreutils</li>\n<li>git</li>\n<li>git-crypt</li>\n<li>terraform</li>\n<li>wget</li>\n<li>zsh</li>\n<li>doctl</li>\n<li>go</li>\n<li>tflint</li>\n<li>asdf</li>\n<li>direnv\nhomebrew_cask_appdir: /Applications\nhomebrew_cask_apps:</li>\n<li>iterm2</li>\n<li>slack</li>\n<li>visual-studio-code</li>\n<li>docker</li>\n<li>postman</li>\n<li>spotify</li>\n<li>notion</li>\n<li>discord<!-- raw HTML omitted --><!-- raw HTML omitted --></li>\n</ul>\n<!-- raw HTML omitted -->\n",
				"content_text": "<p>Let me show you a simple and easy way to manage your MacOS workstation using Ansible. <a href=\"https://www.ansible.com/\" target=\"_blank\" rel=\"noopener\">Ansible is awesome,</a> and we should automate all the things.</p>\n<p>I tinkered with doing this myself from scratch with some success, but I've discovered Jeff Geerling's incredible collection of Ansible roles and collections and didn't look back. They are simple, and ready to go.</p>\n<p>Take a look at my <a href=\"https://github.com/roylindauer/workstation\" target=\"_blank\" rel=\"noopener\">workstation repository</a>, also do check out <a href=\"https://ansible.jeffgeerling.com/\" target=\"_blank\" rel=\"noopener\">Jeff's Ansible work</a>.</p>\n<p>First you need to install the dependencies. I am using a <a href=\"https://galaxy.ansible.com/ui/standalone/roles/geerlingguy/dotfiles/documentation/\" target=\"_blank\" rel=\"noopener\">dotfiles Ansible role</a> to manage my dotfiles, and a collection of mac roles:</p>\n<pre class=\"language-shell\"><code>ansible-galaxy collection install geerlingguy.mac\nansible-galaxy role install geerlingguy.dotfiles</code></pre>\n<p>The mac collection has roles for homebrew, mas, and dock, but I am only concerned with homebrew to install some of my favorite packages. Learn more at <a href=\"https://github.com/geerlingguy/ansible-collection-mac\" target=\"_blank\" rel=\"noopener\">https://github.com/geerlingguy/ansible-collection-mac</a>.</p>\n<p>Create a playbook <em>setup-macos-workstation.yml</em>:</p>\n<pre class=\"language-shell\"><code>---\n- hosts: localhost\n  name: Setup Workstation\n  user: roy\n  connection: local\n\n  vars_files:\n    - vars/main.yml\n\n  roles:\n    - geerlingguy.dotfiles\n    - geerlingguy.mac.homebrew\n\n  tasks: []</code></pre>\n<p>The next thing is to define the dotfiles you want and where to get them, and then also what you want to install via homebrew.</p>\n<p>Create <em>vars/main.yml</em></p>\n<pre class=\"language-xml\"><code>---\ndotfiles_repo: \"https://github.com/roylindauer/dotfiles.git\"\ndotfiles_repo_version: main\ndotfiles_files:\n  - .zshrc\n  - .gitconfig\n  - .railsrc\n  - .aliases\n  - .vimrc\n  - rails-template.rb\n\nhomebrew_cask_accept_external_apps: true\nhomebrew_installed_packages:\n  - libyaml\n  - curl\n  - coreutils\n  - git\n  - git-crypt\n  - terraform\n  - wget\n  - zsh\n  - doctl\n  - go\n  - tflint\n  - asdf\n  - direnv\nhomebrew_cask_appdir: /Applications\nhomebrew_cask_apps:\n  - iterm2\n  - slack\n  - visual-studio-code\n  - docker\n  - postman\n  - spotify\n  - notion\n  - discord</code></pre>\n<p>Run the playbook to download your dotfiles and symlink them to your home directory, and then install all of the packages with brew.</p>\n<pre class=\"language-shell\"><code>ansible-playbook setup-macos-workstation.yml</code></pre>\n<p>This gets me like 80% of the way for setting up my workstation. Some things you have to configure and install manually. But it's nice to have my workstation expressed as code.</p>\n",
				"date_published": "2024-04-01T19:52:22-08:00",
				"url": "https://roylindauer.dev/2024/04/01/use-ansible-to.html"
			},
			{
				"id": "http://royldev.micro.blog/2024/03/10/develop-for-environment.html",
				"title": "Develop for Environment Specific Configuration",
				"content_html": "<p>There is a pattern I see often among junior/associate developers when it comes to handling environment-specific configuration. The observed pattern could be summarized as junior developers creating separate fields for development and production environments in the application, misunderstanding the need for a single environment-aware configuration, resulting in duplicated logic to handle different keys based on the environment.</p>\n<p>An example of this in action is there will be a request to add a field in the admin area of a website to store a key for some purpose. We would typically have a dev key and a production key so depending on the environment we would use the appropriate key. The junior developer might see this and understand the request to be: &ldquo;Create two fields in the admin, one for each environment&rdquo;. After which they also add logic into the code base to check which environment the application is running and read the correct key. I was not clear at first about how to explain this, but I see now it comes down to understanding, or maybe not understanding environment-specific configuration. Understanding that we have multiple versions of the application (development and production), each with its databases, and so we just need to define the configuration once, in each version of the app.</p>\n<p>It&rsquo;s come up enough that I want to call it out, to help people on the path to thinking like a software engineer.</p>\n<p>When thinking about environment-specific configuration there are some good practices to remember:</p>\n<ul>\n<li><strong>Configuration Files:</strong> Use separate configuration files for each environment (dev, prod, etc.), allowing easy management and modification of settings.</li>\n<li><strong>Environment Variables:</strong> Leverage environment variables to store sensitive or environment-specific information, enhancing security and flexibility.</li>\n<li><strong>Single Source of Truth:</strong> Maintain a single configuration source to avoid duplication and inconsistency. Centralize configuration management for better control.</li>\n<li><strong>Parameterization:</strong> Parameterize your application to make it adaptable to different environments. Avoid hardcoding values that might change between setups.</li>\n<li><strong>Automated Deployment:</strong> Implement automated deployment processes to ensure consistency across environments, reducing the chances of configuration errors.</li>\n<li><strong>Version Control:</strong> Store configuration files in version control systems to track changes and easily roll back if needed.</li>\n<li><strong>Documentation:</strong> Document the configuration settings clearly, specifying their purpose and acceptable values. This aids understanding and future maintenance.</li>\n<li><strong>Secure Configuration:</strong> Separate sensitive information (like API keys or database passwords) from the codebase and manage them securely, possibly using vaults or secure storage solutions.</li>\n<li><strong>Testing Environments:</strong> Mimic production environments as closely as possible in testing setups to catch potential issues related to environment-specific configurations early.</li>\n<li><strong>Continuous Integration:</strong> Integrate configuration checks into your continuous integration process to identify any issues before deployment.</li>\n</ul>\n",
				"content_text": "There is a pattern I see often among junior/associate developers when it comes to handling environment-specific configuration. The observed pattern could be summarized as junior developers creating separate fields for development and production environments in the application, misunderstanding the need for a single environment-aware configuration, resulting in duplicated logic to handle different keys based on the environment.\n\nAn example of this in action is there will be a request to add a field in the admin area of a website to store a key for some purpose. We would typically have a dev key and a production key so depending on the environment we would use the appropriate key. The junior developer might see this and understand the request to be: \"Create two fields in the admin, one for each environment\". After which they also add logic into the code base to check which environment the application is running and read the correct key. I was not clear at first about how to explain this, but I see now it comes down to understanding, or maybe not understanding environment-specific configuration. Understanding that we have multiple versions of the application (development and production), each with its databases, and so we just need to define the configuration once, in each version of the app.\n\nIt's come up enough that I want to call it out, to help people on the path to thinking like a software engineer.\n\nWhen thinking about environment-specific configuration there are some good practices to remember:\n\n* **Configuration Files:** Use separate configuration files for each environment (dev, prod, etc.), allowing easy management and modification of settings.\n* **Environment Variables:** Leverage environment variables to store sensitive or environment-specific information, enhancing security and flexibility.\n* **Single Source of Truth:** Maintain a single configuration source to avoid duplication and inconsistency. Centralize configuration management for better control.\n* **Parameterization:** Parameterize your application to make it adaptable to different environments. Avoid hardcoding values that might change between setups.\n* **Automated Deployment:** Implement automated deployment processes to ensure consistency across environments, reducing the chances of configuration errors.\n* **Version Control:** Store configuration files in version control systems to track changes and easily roll back if needed.\n* **Documentation:** Document the configuration settings clearly, specifying their purpose and acceptable values. This aids understanding and future maintenance.\n* **Secure Configuration:** Separate sensitive information (like API keys or database passwords) from the codebase and manage them securely, possibly using vaults or secure storage solutions.\n* **Testing Environments:** Mimic production environments as closely as possible in testing setups to catch potential issues related to environment-specific configurations early.\n* **Continuous Integration:** Integrate configuration checks into your continuous integration process to identify any issues before deployment.\n\n",
				"date_published": "2024-03-10T15:02:00-08:00",
				"url": "https://roylindauer.dev/2024/03/10/develop-for-environment.html",
				"tags": ["devops"]
			},
			{
				"id": "http://royldev.micro.blog/2024/03/09/managing-jumpstart-pro.html",
				"title": "Managing Jumpstart Pro Updates",
				"content_html": "<p><a href=\"https://jumpstartrails.com/\">Jumpstart Pro is a Ruby on Rails SaaS template</a> that lets you quickly deliver and ship business-ready web applications. It takes care of so much boilerplate and common configuration. I&rsquo;ve been using it to build a small app for myself and love how easy it is to build production-ready applications. But it is not without some friction.</p>\n<p>As a subscriber to Jumpstart Rails you are entitled to updates. The way you get those updates is by merging in changes from upstream. As your application grows the chances of merge conflicts increase. Part of the update process is to manage those conflicts, which can be a tedious process.</p>\n<p>The things I do to smooth this process out:</p>\n<ul>\n<li>After merging changes from jumpstart/main, remove from the git index any files I for which know I do not want upstream changes</li>\n<li>Remove from the index and regenerate generated files such as yarn.lock and Gemfile.lock</li>\n<li>And the best course is to avoid making changes to core Jumpstart files where possible</li>\n</ul>\n<p>To remove a file from the git index (and not delete the file) you can run:</p>\n<pre tabindex=\"0\"><code>git rm --cached &lt;file&gt;\n</code></pre><p>A run a script after merging (and resolving conflicts) to regenerate my generated files:</p>\n<pre tabindex=\"0\"><code>#!/bin/bash\n\nif [[ -f yarn.lock ]]; then\n  rm yarn.lock\nfi\n\nif [[ -f Gemfile.lock ]]; then\n  rm Gemfile.lock\nfi\n\nyarn install\nbundle install\n</code></pre><p>All of this makes applying Jumpstart Pro updates easier for me and I hope it helps you as well.</p>\n",
				"content_text": "[Jumpstart Pro is a Ruby on Rails SaaS template](https://jumpstartrails.com/) that lets you quickly deliver and ship business-ready web applications. It takes care of so much boilerplate and common configuration. I've been using it to build a small app for myself and love how easy it is to build production-ready applications. But it is not without some friction.\n\nAs a subscriber to Jumpstart Rails you are entitled to updates. The way you get those updates is by merging in changes from upstream. As your application grows the chances of merge conflicts increase. Part of the update process is to manage those conflicts, which can be a tedious process.\n\nThe things I do to smooth this process out:\n\n* After merging changes from jumpstart/main, remove from the git index any files I for which know I do not want upstream changes\n* Remove from the index and regenerate generated files such as yarn.lock and Gemfile.lock\n* And the best course is to avoid making changes to core Jumpstart files where possible\n\nTo remove a file from the git index (and not delete the file) you can run:\n\n```\ngit rm --cached <file>\n```\nA run a script after merging (and resolving conflicts) to regenerate my generated files:\n\n```\n#!/bin/bash\n\nif [[ -f yarn.lock ]]; then\n  rm yarn.lock\nfi\n\nif [[ -f Gemfile.lock ]]; then\n  rm Gemfile.lock\nfi\n\nyarn install\nbundle install\n```\nAll of this makes applying Jumpstart Pro updates easier for me and I hope it helps you as well.\n\n",
				"date_published": "2024-03-09T15:12:00-08:00",
				"url": "https://roylindauer.dev/2024/03/09/managing-jumpstart-pro.html"
			},
			{
				"id": "http://royldev.micro.blog/2023/11/24/simple-database-seeder.html",
				"title": "Simple Database Seeder for WordPress",
				"content_html": "<p>I&rsquo;ve been doing some WordPress development at my day job and have come to the conclusion that the development life-cycle of WordPress is kinda bad, actually. However, there are systems &amp; processes we can implement to improve the developer and development experience. We <em>can</em> make it more enjoyable to work on WordPress!</p>\n<p>One of those systems that I sorely miss is a database seeder that can populate your site with test and dummy content. The go-to for WordPress seems to be to download a database from a live or test site. I am not a fan of this pattern at all and would rather not touch production data. If the database is large this can be a very time-consuming process. If you need media and attachments this can become a huge burden on your workstation as you download gigabytes of assets.</p>\n<p>The default method for seeding data on WordPress is to first set up a WordPress site <!-- raw HTML omitted -->somewhere<!-- raw HTML omitted -->, then create a bunch of test content, then export the content into an XML file. You can import that XML file into your local install of WordPress. However, a downside of this is that if there are attachments, they need to be publicly available either at the test site or somehow locally available. You need to have already created the content. Making updates is also tedious. I think the workflow is poor and we can do better.</p>\n<p>What I would prefer is to be more intentional about creating that content.</p>\n<p>My end goal is to be able to run a command to seed content from classes I create that very intentionally define the type of content I want available. Also, it would be nice to be able to reset the data and reseed. The command interface I want:</p>\n<pre tabindex=\"0\"><code>$ wp-cli dbseeder seed\n$ wp-cli dbseeder truncate\n$ wp-cli dbseeder replant\n</code></pre><p>I leaned into wp-cli to help me here. I created a custom wp-cli command to wrap the business logic of seeding, truncating, and replanting the content.</p>\n<pre tabindex=\"0\"><code>&lt;?php\nnamespace RoycomCommands;\n\nuse Exception;\nuse HaydenPierce\\ClassFinder\\ClassFinder;\nuse WP_CLI;\nuse WP_CLI_Command;\nuse RoycomTheme\\DbTruncate;\n\nif (class_exists('WP_CLI_Command')) {\n    /**\n     * Class DbSeederCommand\n     */\n    class DbSeederCommand extends WP_CLI_Command\n    {\n        /**\n         * @return void\n         * @throws Exception\n         */\n        public function seed(): void\n        {\n            $classes = $this-&gt;getSeedClasses();\n            foreach ($classes as $class) {\n                WP_CLI::log(sprintf(&quot;Seeding %s...&quot;, $class));\n\n                $seed = new $class;\n                $result = $seed-&gt;run();\n\n                if ($result[&quot;success&quot;] === false) {\n                    WP_CLI::error($result[&quot;message&quot;]);\n                } else {\n                    WP_CLI::success($result[&quot;message&quot;]);\n                }\n            }\n\n            WP_CLI::success(&quot;Command executed&quot;);\n        }\n\n        /**\n         * @return void\n         */\n        public function truncate(): void\n        {\n            WP_CLI::warning(&quot;Truncating database...&quot;);\n            $DbTruncate = new DbTruncate();\n            $DbTruncate-&gt;run();\n        }\n\n        /**\n         * @return void\n         * @throws Exception\n         */\n        public function replant(): void\n        {\n            $this-&gt;truncate();\n            $this-&gt;seed();\n        }\n\n        /**\n         * @return array\n         * @throws Exception\n         */\n        private function getSeedClasses(): array\n        {\n            $class_finder = new ClassFinder();\n            $class_finder::setAppRoot(ABSPATH);\n            return $class_finder::getClassesInNamespace('RoycomTheme\\\\DbSeeds', ClassFinder::RECURSIVE_MODE);\n        }\n    }\n}\n</code></pre><p>I registered the command on my WordPress site.</p>\n<pre tabindex=\"0\"><code>&lt;?php\n\nuse WP_CLI;\n\n# ... the rest of functions.php...\n\nWP_CLI::add_command('dbseeder', 'RoycomCommands\\\\DbSeederCommand');\n</code></pre><p>I created my first seed. The class <code>AbstractSeed</code> is pretty bare bones here, it just defines an interface method called `run1 that we have to implement in our seed class.</p>\n<pre tabindex=\"0\"><code>&lt;?php\n\nnamespace RoycomTheme\\DbSeeds;\n\nuse RoycomTheme\\AbstractDbSeed;\n\nclass ExampleSeed extends AbstractDbSeed\n{\n    public function run(): array\n    {\n        $post_id = wp_insert_post([\n            'post_title' =&gt; 'Example Post',\n            'post_content' =&gt; 'This is an example post.',\n            'post_status' =&gt; 'publish',\n            'post_author' =&gt; 1,\n            'post_type' =&gt; 'post',\n        ]);\n\n        return [\n            &quot;success&quot; =&gt; true,\n            &quot;message&quot; =&gt; sprintf('Created Post with ID #%d', $post_id),\n            &quot;post_id&quot; =&gt; $post_id\n        ];\n    }\n}\n\n</code></pre><p>My composer.json file defines where to find the seeds, and where to find commands. This is a very trimmed down version of composer.json.</p>\n<pre tabindex=\"0\"><code>{\n  &quot;require&quot;: {\n    &quot;haydenpierce/class-finder&quot;: &quot;^0.5.3&quot;\n  },\n  &quot;autoload&quot;: {\n    &quot;psr-4&quot;: {\n      &quot;Roycom\\\\&quot;: &quot;src/&quot;,\n      &quot;RoycomCommands\\\\&quot;: &quot;wp-cli/commands/&quot;,\n      &quot;RoycomTheme\\\\&quot;: &quot;wp-content/themes/roylart/includes/classes&quot;\n    }\n  }\n}\n</code></pre><p>The gist of what is happening is I discover seed files in the directory <code>DbSeeds</code> in my theme directory using <code>ClassFinder</code> and then for each of them I execute a <code>run</code> method. The <code>run</code> method simply runs some WordPress functions to create posts, create taxonomies, add placeholder images. Really, it can do whatever I need it to do.</p>\n<p>For example, I created a <code>PagesSeed</code> class that creates some generic pages, makes one of them the front page and one the blog page, and then also create a menu.</p>\n<p>Another seed I created is called <code>ArtworksSeed</code> for a custom post type called <code>artworks</code> that will create example posts, download images from loremflickr, and also add data to custom MetaBox fields.</p>\n",
				"content_text": "I've been doing some WordPress development at my day job and have come to the conclusion that the development life-cycle of WordPress is kinda bad, actually. However, there are systems & processes we can implement to improve the developer and development experience. We _can_ make it more enjoyable to work on WordPress!\n\nOne of those systems that I sorely miss is a database seeder that can populate your site with test and dummy content. The go-to for WordPress seems to be to download a database from a live or test site. I am not a fan of this pattern at all and would rather not touch production data. If the database is large this can be a very time-consuming process. If you need media and attachments this can become a huge burden on your workstation as you download gigabytes of assets.\n\nThe default method for seeding data on WordPress is to first set up a WordPress site <em>somewhere</em>, then create a bunch of test content, then export the content into an XML file. You can import that XML file into your local install of WordPress. However, a downside of this is that if there are attachments, they need to be publicly available either at the test site or somehow locally available. You need to have already created the content. Making updates is also tedious. I think the workflow is poor and we can do better.\n\nWhat I would prefer is to be more intentional about creating that content.\n\nMy end goal is to be able to run a command to seed content from classes I create that very intentionally define the type of content I want available. Also, it would be nice to be able to reset the data and reseed. The command interface I want:\n\n```\n$ wp-cli dbseeder seed\n$ wp-cli dbseeder truncate\n$ wp-cli dbseeder replant\n```\nI leaned into wp-cli to help me here. I created a custom wp-cli command to wrap the business logic of seeding, truncating, and replanting the content.\n\n```\n<?php\nnamespace RoycomCommands;\n\nuse Exception;\nuse HaydenPierce\\ClassFinder\\ClassFinder;\nuse WP_CLI;\nuse WP_CLI_Command;\nuse RoycomTheme\\DbTruncate;\n\nif (class_exists('WP_CLI_Command')) {\n    /**\n     * Class DbSeederCommand\n     */\n    class DbSeederCommand extends WP_CLI_Command\n    {\n        /**\n         * @return void\n         * @throws Exception\n         */\n        public function seed(): void\n        {\n            $classes = $this->getSeedClasses();\n            foreach ($classes as $class) {\n                WP_CLI::log(sprintf(\"Seeding %s...\", $class));\n\n                $seed = new $class;\n                $result = $seed->run();\n\n                if ($result[\"success\"] === false) {\n                    WP_CLI::error($result[\"message\"]);\n                } else {\n                    WP_CLI::success($result[\"message\"]);\n                }\n            }\n\n            WP_CLI::success(\"Command executed\");\n        }\n\n        /**\n         * @return void\n         */\n        public function truncate(): void\n        {\n            WP_CLI::warning(\"Truncating database...\");\n            $DbTruncate = new DbTruncate();\n            $DbTruncate->run();\n        }\n\n        /**\n         * @return void\n         * @throws Exception\n         */\n        public function replant(): void\n        {\n            $this->truncate();\n            $this->seed();\n        }\n\n        /**\n         * @return array\n         * @throws Exception\n         */\n        private function getSeedClasses(): array\n        {\n            $class_finder = new ClassFinder();\n            $class_finder::setAppRoot(ABSPATH);\n            return $class_finder::getClassesInNamespace('RoycomTheme\\\\DbSeeds', ClassFinder::RECURSIVE_MODE);\n        }\n    }\n}\n```\nI registered the command on my WordPress site.\n\n```\n<?php\n\nuse WP_CLI;\n\n# ... the rest of functions.php...\n\nWP_CLI::add_command('dbseeder', 'RoycomCommands\\\\DbSeederCommand');\n```\nI created my first seed. The class `AbstractSeed` is pretty bare bones here, it just defines an interface method called `run1 that we have to implement in our seed class.\n\n```\n<?php\n\nnamespace RoycomTheme\\DbSeeds;\n\nuse RoycomTheme\\AbstractDbSeed;\n\nclass ExampleSeed extends AbstractDbSeed\n{\n    public function run(): array\n    {\n        $post_id = wp_insert_post([\n            'post_title' => 'Example Post',\n            'post_content' => 'This is an example post.',\n            'post_status' => 'publish',\n            'post_author' => 1,\n            'post_type' => 'post',\n        ]);\n\n        return [\n            \"success\" => true,\n            \"message\" => sprintf('Created Post with ID #%d', $post_id),\n            \"post_id\" => $post_id\n        ];\n    }\n}\n\n```\nMy composer.json file defines where to find the seeds, and where to find commands. This is a very trimmed down version of composer.json.\n\n```\n{\n  \"require\": {\n    \"haydenpierce/class-finder\": \"^0.5.3\"\n  },\n  \"autoload\": {\n    \"psr-4\": {\n      \"Roycom\\\\\": \"src/\",\n      \"RoycomCommands\\\\\": \"wp-cli/commands/\",\n      \"RoycomTheme\\\\\": \"wp-content/themes/roylart/includes/classes\"\n    }\n  }\n}\n```\nThe gist of what is happening is I discover seed files in the directory `DbSeeds` in my theme directory using `ClassFinder` and then for each of them I execute a `run` method. The `run` method simply runs some WordPress functions to create posts, create taxonomies, add placeholder images. Really, it can do whatever I need it to do.\n\nFor example, I created a `PagesSeed` class that creates some generic pages, makes one of them the front page and one the blog page, and then also create a menu.\n\nAnother seed I created is called `ArtworksSeed` for a custom post type called `artworks` that will create example posts, download images from loremflickr, and also add data to custom MetaBox fields.\n\n",
				"date_published": "2023-11-24T15:00:00-08:00",
				"url": "https://roylindauer.dev/2023/11/24/simple-database-seeder.html"
			},
			{
				"id": "http://royldev.micro.blog/2023/09/26/turbo-will-call.html",
				"title": "Turbo Will Call Stimulus `connect()` Twice",
				"content_html": "<p>When you click a link to return to a page, or use your browsers back button to return to a page Turbo will render a cached preview of the page. It will then fetch an updated version of the page.</p>\n<p>If you have a stimulus controller on the page what will happen is the controller is initialized twice. The initial cached version will load the controller and fire off the connect() method, then the page is refreshed via Turbo, the controller is disconnected, then reconnected when the DOM is updated from the Turbo update. This will fire the connect method a second time. Super annoying.</p>\n<p>If you run into this you can detect when a Turbo preview is visible.</p>\n<pre tabindex=\"0\"><code>if (document.documentElement.hasAttribute(&quot;data-turbo-preview&quot;)) {\n  // Turbo Drive is displaying a preview\n}\n</code></pre><p>I feel though, that this is the wrong solution and that my approach to how I am using StimulusJS controllers is inherently wrong.</p>\n",
				"content_text": "When you click a link to return to a page, or use your browsers back button to return to a page Turbo will render a cached preview of the page. It will then fetch an updated version of the page.\n\nIf you have a stimulus controller on the page what will happen is the controller is initialized twice. The initial cached version will load the controller and fire off the connect() method, then the page is refreshed via Turbo, the controller is disconnected, then reconnected when the DOM is updated from the Turbo update. This will fire the connect method a second time. Super annoying.\n\nIf you run into this you can detect when a Turbo preview is visible.\n\n```\nif (document.documentElement.hasAttribute(\"data-turbo-preview\")) {\n  // Turbo Drive is displaying a preview\n}\n```\nI feel though, that this is the wrong solution and that my approach to how I am using StimulusJS controllers is inherently wrong.\n\n",
				"date_published": "2023-09-26T14:54:00-08:00",
				"url": "https://roylindauer.dev/2023/09/26/turbo-will-call.html",
				"tags": ["ruby-on-rails"]
			},
			{
				"id": "http://royldev.micro.blog/2023/04/24/the-three-core.html",
				"title": "The Three Core Principles of DevOps",
				"content_html": "<!-- raw HTML omitted -->\n",
				"content_text": "<p>If you ask a group of people \"what is DevOps?\" you are likely going to get a variety of different answers. Some think it's just the automation of a pipeline. Some people may say it's just developers doing operations work. I subscribe to the idea that DevOps is a mental model for how to think about creating software. From that philosophy some concrete tactics have emerged, such as automating infrastructure and deployments. However those are the end results of thinking like DevOps and applying some principles to the processes of getting work done.</p>\n<p>To realize the potential of DevOps you should become intimately familiar with some core principles, expressed as the \"The Three Ways of DevOps\" from the book, \"The DevOps Handbook\". If you've not read it I do recommend it, it goes into great detail about the principles. I will summarize them in this blog post though.</p>\n<p>I first started learning DevOps by learning about the \"Three Ways of Devops\", which are the <strong>Principle of Flow</strong>, the <strong>Principle of Feedback</strong>, and the <strong>Principle of Continuous Improvement</strong>.</p>\n<h2>The Principle of Flow.</h2>\n<p>The first principle is all about creating smooth flow of work through the different functional areas of an organization, from gathering requirements to releasing features to production. It emphasizes the importance of continuously improving the flow of work through the entire software delivery process, from idea to deployment. The goal is to eliminate bottlenecks and streamline the process of delivering \"value\" or code. Part of this demands collaboration and communication between teams involved in the process, such as development, testing, and operations. You gotta break down communication silos and allow people to work together.</p>\n<p>Functionally, this principle is asking that we do two things in order to improve the flow of work. The first is to reduce the batch size of that work. The second is to eliminate constraints that impede the flow of the work.</p>\n<p><strong>Reducing batch size</strong> is about breaking down large work into smaller, more manageable pieces. Reducing batch size means less time to complete an item and an increase in frequency of feedback (more on that later). Smaller batches allow for faster and more frequent deployments, which can help to identify and fix issues more quickly. In practice, you can achieve this by implementing agile development methodologies, such as Kanban, that prioritize iterative and incremental delivery of work. It may also involve automating some processes, such as testing, to reduce the amount of manual effort required to complete each batch of work. Reducing batch sizes also reduces the amount of work-in-progress (WIP). Too much WIP can lead to delays and bottlenecks, while too little can result in idle resources and slower progress. Managing WIP is important as it can impact the flow of work through the process.</p>\n<p><strong>Eliminating constraints</strong> refers to identifying and removing any barriers or limitations that impede the flow of work. This can include technical constraints such as outdated systems, or organizational constraints such as siloed teams. To eliminate constraints, teams often use techniques such as value stream mapping, which involves mapping out the entire delivery process to identify areas where constraints exist. Once constraints are identified, teams can work together to develop solutions to eliminate them, such as upgrading systems, improving communication and collaboration, or automating manual processes.</p>\n<p>Dr. Goldratt states in his book, Beyond the Goal: Theory of Constraints</p>\n<blockquote>\n<p>In any value stream, there is always a direction of flow, and there is always one and only one constraint; any improvement not made at that constraint is an illusion.</p>\n</blockquote>\n<h2>The Principle of Feedback.</h2>\n<p>The second principle is the principle of feedback. This is all about creating fast feedback loops across the entire process. Feedback is crucial when it comes to improving quality and efficiency of the process. Feedback comes from a wide variety of sources, including customers, end-users, internal stakeholders, and can be both qualitative and quantitative. Teams want to gather and analyze feedback in order to identify areas for improvement, to validate assumptions, and to make informed decisions about future work. A concrete example of this is creating a CI/CD pipeline to automate testing, which allows for quick feedback on changes made to software. Another example is collecting data on user behavior and system performance to inform how to improve the overall user experience.</p>\n<p>The DevOps approach to feedback is to <strong>move it closer to the source</strong>, rather than making it someone else's problem. For example, if a QA team is exclusively in charge of the correctness of work produced by developers the quality of the product is shifted away from the source. Feedback is received when work moves between work centers in the value stream. This hand-off can create delays and could become a bottleneck. It can disrupt the flow of work. Moving quality closer to the source can improve quality and flow and reduce the WIP.</p>\n<p>Overall the principle of feedback is essential in DevOps as it <strong>promotes a culture of continuous improvement and learning</strong>. By gathering and acting on feedback at every stage of the software delivery process, teams practicing DevOps can deliver better quality more efficiently.</p>\n<h2>The Principle of Continuous Improvement.</h2>\n<p>The third principle is about creating a culture of continual learning and experimentation. Fast feedback enables us to learn from mistakes, but we also want to <em>continually improve the entire organization</em>. <strong>Continuous improvement</strong> involves regular reflection and analysis of the delivery process to identify areas for improvement, such as bottlenecks, inefficiencies, or areas where quality can be improved. Teams use metrics and data to measure the effectiveness of the delivery process and identify areas for improvement.</p>\n<p>When teams are consistently too busy or occupied to improve their daily work and resort to workarounds, they accumulate technical debt. This not only leads to persistent struggles but also causes problems that can snowball over time. As a result, processes deteriorate, productivity declines, and throughput diminishes, and flow of work is threatened. Embracing continuous improvement can ensure that teams are delivery high-quality.</p>\n<h2>Conclusion</h2>\n<p>Easy peasy, thinking like DevOps can mean a huge transformation to your org, or just a huge personal transformation in how you get your work done. DevOps is more than a set of tools and technologies and practices. DevOps is a cultural shift, it is a mindset, a different way of thinking about solving the problems of building software. DevOps enables a company to ship value to customers more frequently and more efficiently. Just remember the three F's; the principles of <em>Flow</em>, <em>Feedback</em>, and <em>Fine Tuning</em>.</p>\n",
				"date_published": "2023-04-24T14:55:00-08:00",
				"url": "https://roylindauer.dev/2023/04/24/the-three-core.html",
				"tags": ["devops"]
			},
			{
				"id": "http://royldev.micro.blog/2023/03/03/how-to-use.html",
				"title": "How To Use Docker Compose Effectively as a Development Tool",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<p>development:\n&laquo;: *default\nurl: &ldquo;postgresql://postgres:password@127.0.0.1/docker_compose_example&rdquo;</p>\n<p>test:\n&laquo;: *default\nurl: &ldquo;postgresql://postgres:password@127.0.0.1/docker_compose_example_test&rdquo;</p>\n<p>production:\n&laquo;: *default\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>volumes:\ndbdata:<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>Sidekiq.configure_client do |config|\nconfig.redis = { url: &lsquo;redis://localhost:6379/0&rsquo; }\nend<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>Rails.application.routes.draw do</p>\n<h1 id=\"define-your-application-routes-per-the-dsl-in-httpsguidesrubyonrailsorgroutinghtml\">Define your application routes per the DSL in <a href=\"https://guides.rubyonrails.org/routing.html\">https://guides.rubyonrails.org/routing.html</a></h1>\n<h1 id=\"defines-the-root-path-route-\">Defines the root path route (&quot;/&quot;)</h1>\n<h1 id=\"root-articlesindex\">root &ldquo;articles#index&rdquo;</h1>\n<p>mount Sidekiq::Web =&gt; &lsquo;/sidekiq&rsquo;\nend\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>redis:\nimage: redis:alpine\nports:\n- 6379:6379</p>\n<p>volumes:\ndbdata:<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>ARG RAILS_ENV=production</p>\n<p>RUN apt-get update -qq &amp;&amp; <br>\nDEBIAN_FRONTEND=noninteractive apt-get install -y &ndash;no-install-recommends  libvips libvips-dev libvips-tools libpq-dev &amp;&amp; <br>\nrm -rf /var/lib/apt/lists/* /var/cache/apt</p>\n<p>ARG BUNDLER_VERSION=2.3.26\nRUN gem install &ldquo;bundler:${BUNDLER_VERSION}&rdquo; &ndash;no-document &amp;&amp; <br>\ngem update &ndash;system &amp;&amp; <br>\ngem cleanup</p>\n<p>WORKDIR /app</p>\n<p>COPY Gemfile Gemfile.lock ./\nRUN bundle install &ndash;jobs &ldquo;$(nproc)&rdquo;</p>\n<p>COPY . .</p>\n<p>CMD [&ldquo;bin/rails&rdquo;, &ldquo;s&rdquo;, &ldquo;-b&rdquo;, &ldquo;0.0.0.0&rdquo;]</p>\n<p>EXPOSE 3000\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>development:\n&laquo;: *default\nurl: &ldquo;postgresql://postgres:password@postgresql/docker_compose_example&rdquo;</p>\n<p>test:\n&laquo;: *default\nurl: &ldquo;postgresql://postgres:password@postgresql/docker_compose_example_test&rdquo;</p>\n<p>production:\n&laquo;: *default<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>Sidekiq.configure_client do |config|\nconfig.redis = { url: &lsquo;redis://redis:6379/0&rsquo; }\nend<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n",
				"content_text": "<div class=\"wizzy\">\n        <p class=\"c-lead\">Let's use Docker Compose to help us build software! If you are new to the world of containers then I hope you will find this tutorial insightful. We will explore setting up a new Ruby on Rails project, creating a docker-compose.yml file, then will add services like PostgreSQL and Redis, and finally, running and developing our new application in Docker.</p>\n<p>I choose to use Docker Compose to avoid installing and juggling multiple versions of tools and services on my workstation. For example, let's say you have an application that needs a MySQL database, another that needs PostgreSQL 11, and another that needs PostgreSQL 13. Installing MySQL and PostgreSQL together is easy enough, but trying to run multiple versions of the same database server is a lot harder to do. Aside from any difficulty you may find there you will likely now have services running that you don't actually need consuming precious resources on your workstation.</p>\n<p>Setup and teardown of a project can be a real pain and you might not even bother doing it because it's not an easy thing to do. A docker-based workflow simplifies the process of setting up and tearing down a project. Easy setup processes mean you can onboard others to your project more quickly, can easily transfer to another workstation, and can uncover configuration issues.</p>\n<p>If you are working in a team environment you may have experienced issues where \"it works on my machine\" but not on<em>their</em> machine. Inconsistencies between development environments can lead to bugs and troubleshooting headaches. Using a docker-based development system does mitigate many of those sorts of issues.</p>\n<p>The goal of this tutorial is to illustrate how you can use Docker Compose in your development workflow, as either a way to run ancillary and supporting services such as database and caching servers or by moving the entire development workflow into Docker Compose.</p>\n<h2>Table of Contents</h2>\n<ol>\n<li><a href=\"#getting-started\">Getting Started</a></li>\n<li><a href=\"#add-service\">Add Our First Service to Docker Compose - PostgreSQL</a></li>\n<li><a href=\"#additional-supporting-services\">Additional Supporting Services</a></li>\n<li><a href=\"#developing-in-docker\">Developing Our Application in Docker</a></li>\n<li><a href=\"#utilities\">Utilities for working with Docker Compose</a></li>\n</ol>\n<h2><a name=\"getting-started\"></a>Getting Started</h2>\n<p>For this tutorial, you should have some familiarity with your terminal, access to a text editor, and though not required at all, maybe some experience with Ruby on Rails (my example application will be Ruby on Rails). You will also need to install Docker. If you are going to follow along you will need Ruby installed.</p>\n<p>To get started you will first need to install Docker and Docker Compose. The easiest and quickest way to do this is to install<a href=\"https://web.archive.org/web/20231210114644/https://www.docker.com/products/docker-desktop/\" target=\"_blank\" rel=\"noopener\">Docker Desktop</a>. Compose is installed automatically when you install Docker Desktop.</p>\n<p>After you have installed the package you can confirm that Docker Compose is installed by running the following in your terminal:</p>\n<pre class=\"language-shell\"><code>docker compose version</code></pre>\n<h3><a name=\"clone-project\"></a>Clone the example repo.</h3>\n<p>Clone the example repo to see the complete example.</p>\n<pre class=\"language-shell\"><code>git clone https://github.com/roylindauer/tutorial-docker-compose-development.git docker-compose-example</code></pre>\n<h3><a name=\"create-project\"></a>Create a new project.</h3>\n<p>If you want to follow along then go ahead and create a new Rails project. You will need to have Ruby installed. <a href=\"https://web.archive.org/web/20231210114644/https://guides.rubyonrails.org/getting_started.html#creating-a-new-rails-project-installing-rails\" target=\"_blank\" rel=\"noopener\">Please refer to the Rails getting started guide if you need help getting Rails installed</a>.</p>\n<pre class=\"language-shell\"><code>gem install rails\nrails --version</code></pre>\n<p>That should return something like \"Rails 7.0.0\". If so you are good to create a new rails project.</p>\n<pre class=\"language-shell\"><code>rails new docker-compose-example</code></pre>\n<p>Confirm that rails is setup by running the rails server and visiting <a href=\"https://web.archive.org/web/20231210114644/http://localhost:3000/\" target=\"_blank\" rel=\"noopener\">http://localhost:3000</a></p>\n<pre class=\"language-shell\"><code>cd docker-compose-example\nbundle exec rails s</code></pre>\n\n\n<img src=\"https://roylindauer.dev/uploads/2024/screen-shot-rails-7-running.png\" width=\"600\" height=\"390\" alt=\"\">\n\t\t\t\t\t\t \n<p>Okay so that's great, but it's not doing much yet. By default, Rails uses SQLite as the database. Let's use PostgreSQL as the database. To do that we will create a new file called <strong>docker-compose.yml</strong> and add our first Docker Compose service to that file.</p>\n<h2><a name=\"add-service\"></a>Add Our First Service to Docker Compose - PostgreSQL</h2>\n<p>Create the file <strong>docker-compose.yml</strong> in the root of the project. The compose file is a YAML file that defines <em>services</em>, <em>networks</em>, and <em>volumes</em>. The service definition, which is where are going to put most of our effort, contains the configuration for our containers. If you want a deep dive into the various configurations and format of the docker-compose.yml file check out the <a href=\"https://web.archive.org/web/20231210114644/https://docs.docker.com/compose/compose-file/compose-file-v3/\" target=\"_blank\" rel=\"noopener\">compose file v3 documentation</a>.</p>\n<p>Add the following to docker-compose.yml:</p>\n<pre class=\"language-shell\"><code>version: \"3\"\nservices:\n  postgresql:\n    image: postgres:13.10</code></pre>\n<p>What this is doing is adding a new service, \"postgresql\", and we are telling compose to use the public docker image \"<a href=\"https://web.archive.org/web/20231210114644/https://hub.docker.com/_/postgres\" target=\"_blank\" rel=\"noopener\">postgres:13.10</a>\". Compose will pull the image from the docker hub when we start the service. Let's do that now. From your terminal run the command</p>\n<pre class=\"language-shell\"><code>docker compose up</code></pre>\n<p>This command will bring up all of the services defined in docker-compose.yml. If you want to be explicit you can run \"<code>docker compose up postgresql</code>\" to bring up only that service.</p>\n<p><img src=\"https://roylindauer.dev/uploads/2024/start-docker-compose-first-time.png\" width=\"100%\"alt=\"\"></p>\n<p>The image is downloaded but we got an error when starting the container; \"Database is uninitialized and superuser password is not specified\". We are told how to resolve this, we have to pass in an environment variable to set the \"POSTGRES_PASSWORD\". Environment variables can be defined when running a container with \"docker run\" by passing in the \"-e\" flag. But we are using compose so we have to instead add environment configuration to our \"postgresql\" service in the docker-compose.yml file.</p>\n<p>Update docker-compose.yml and then run \"<code>docker compose up</code>\" again.</p>\n<pre class=\"language-shell\"><code>version: \"3\"\nservices:\n  postgresql:\n    image: postgres:13.10\n    environment:\n      - POSTGRES_PASSWORD=password</code></pre>\n<p><img src=\"https://roylindauer.dev/uploads/2024/start-docker-compose-fix-env-errors.png\" alt=\"\" width=\"100%\"></p>\n<p>Great! The postgresql container is working!</p>\n<p>But how can we connect to it? In order for us to be able to connect to the service we need to expose ports on the container to the host. We do that by adding a ports configuration to our service definition. The default port for PostgreSQL is 5432. Stop Docker Compose by pressing \"<code>ctrl+c</code>\". Update docker-compose.yml:</p>\n<pre class=\"language-shell\"><code>version: \"3\"\nservices:\n  postgresql:\n    image: postgres:13.10\n    environment:\n      - POSTGRES_PASSWORD=password\n    ports:\n      - 5432:5432</code></pre>\n<p>What this is saying is to map port 5432 on the host (your workstation) to port 5432 on the container (\"postgresql\"). This means that requests to localhost:5432 or 127.0.0.1:5432 will be routed to the port 5432 on the container. Let's update our Rails app to use PostgreSQL and connect to the database.</p>\n<p>We will update the Rails app by installing the \"<code>pg</code>\" Ruby gem and changing the database configuration to use postgresql as the database driver. In order to install the <code>pg</code> gem you will need to have installed PostgreSQL headers and -dev packages. On MacOS with brew, you would run:</p>\n<pre class=\"language-shell\"><code>brew install postgresql</code></pre>\n<p>Once that is installed you can now update Rails.</p>\n<pre class=\"language-shell\"><code>bundle remove sqlite3\nbundle add pg</code></pre>\n<p>Next, you need to edit \"config/database.yml\"</p>\n<pre class=\"language-shell\"><code>default: &default\n  adapter: postgresql\n  encoding: unicode\n  pool: <%= ENV.fetch(\"RAILS_MAX_THREADS\") { 5 } %>\n\ndevelopment:\n  <<: *default\n  url: \"postgresql://postgres:password@127.0.0.1/docker_compose_example\"\n\ntest:\n  <<: *default\n  url: \"postgresql://postgres:password@127.0.0.1/docker_compose_example_test\"\n\nproduction:\n  <<: *default\n</code></pre>\n<p>Startup Docker Compose again, initialize the database, then start the rails server.</p>\n<pre class=\"language-shell\"><code>docker-compose up -d\nbundle exec rails db:setup\nbundle exec rails s</code></pre>\n<p>The \"-d\" flag tells Docker Composeto run in the background. If all went well you will see the familiar Rails logo welcome screen at <a href=\"https://web.archive.org/web/20231210114644/http://localhost:3000/\" target=\"_blank\" rel=\"noopener\">http://localhost:3000</a>. We are now connecting to the PostgreSQL database running in Docker! As Docker Compose was detached and is running in the background you can stop it with the command:</p>\n<pre class=\"language-xml\"><code>docker compose stop</code></pre>\n<p>Other important commands are:</p>\n<ul>\n<li><code>docker compose up</code> - Creates or recreates your containers and starts them up</li>\n<li><code>docker compose start</code> - Simply starts the containers that already exist.</li>\n<li><code>docker compose stop</code> - Stop the containers</li>\n<li><code>docker compose down</code> - Stops and removes containers</li>\n</ul>\n<p>Go ahead and run <em>down</em> now to stop and remove the PostgreSQL container.</p>\n<pre class=\"language-shell\"><code>docker compose down</code></pre>\n<p>Then bring it all back up again.</p>\n<pre class=\"language-shell\"><code>docker compose up -d\nbundle exec rails s</code></pre>\n<p>Hmm, something has gone wrong. The database is gone.</p>\n<p><img src=\"https://roylindauer.dev/uploads/2024/persistent-storage.png\" width=\"100%\" alt=\"Auto-generated description: A screenshot displays an error message indicating an ActiveRecord::NoDatabaseError with related program code and stack trace details.\"></p>\n<p>When we ran \"<code>docker compose down</code>\" it destroyed the PostgreSQL container and all of the data inside of it. That may not be desired, especially when it comes to databases, so we will want to introduce some persistent storage to our configuration. Do this by defining a volume and attaching it to the container. Update docker-compose.yml:</p>\n<pre class=\"language-shell\"><code>version: \"3\"\nservices:\n  postgresql:\n    image: postgres:13.10\n    environment:\n      - POSTGRES_PASSWORD=password\n    ports:\n      - 5432:5432\n    volumes:\n      - \"dbdata:/var/lib/postgresql/data\"\n\nvolumes:\n  dbdata:</code></pre>\n<p>We add a new section, \"volumes\", that defines the various volumes we wish to attach to containers. In the services section, a new configuration is added that mounts our volume to a path in the container. In this case, mounting \"dbdata\" to the location where PostgreSQL stores its data. Now we can bring down our containers without losing data.</p>\n<p>Reset the project by running the command \"<code>docker compose down</code>\" to remove any containers then run the following to bring up the PostgreSQL service, create our database, and start rails.</p>\n<pre class=\"language-shell\"><code>docker-compose up -d\nbundle exec rails db:setup\nbundle exec rails s</code></pre>\n<p>Check that Rails loads by visiting <a href=\"https://web.archive.org/web/20231210114644/http://localhost:3000/\" target=\"_blank\" rel=\"noopener\">http://localhost:3000</a>.</p>\n<p>Stop rails and run \"<code>docker compose down</code>\" again. This removes the PostgreSQL container, but because we have a volume with our data, when we create a new container with \"<code>docker compose up</code>\" our database will still be there. Give it a shot!</p>\n<pre class=\"language-shell\"><code>docker-compose up -d\nbundle exec rails s</code></pre>\n<p>Check again that Rails loads by visiting <a href=\"https://web.archive.org/web/20231210114644/http://localhost:3000/\" target=\"_blank\" rel=\"noopener\">http://localhost:3000</a>.Our database persisted!</p>\n<h2><a name=\"additional-supporting-services\"></a><a name=\"add-mailcatcher\"></a>Additional Supporting Services</h2>\n<p>We can add more services to support our application. When you see Ruby on Rails projects, you might also find Sidekiq to process background jobs, with Redis as the database for those jobs (also for caching). Add \"sidekiq\" to Rails by running the command:</p>\n<pre class=\"language-xml\"><code>bundle add sidekiq</code></pre>\n<p>Create the file \"<em>config/initializers/sidekiq.rb</em>\" with:</p>\n<pre class=\"language-ruby\"><code>Sidekiq.configure_server do |config|\n  config.redis = { url: 'redis://localhost:6379/0' }\nend\n\nSidekiq.configure_client do |config|\n  config.redis = { url: 'redis://localhost:6379/0' }\nend</code></pre>\n<p>Edit the file \"<em>config/routes.rb</em>\":</p>\n<pre class=\"language-ruby\"><code>require 'sidekiq/web'\n\nRails.application.routes.draw do\n  # Define your application routes per the DSL in https://guides.rubyonrails.org/routing.html\n\n  # Defines the root path route (\"/\")\n  # root \"articles#index\"\n  mount Sidekiq::Web => '/sidekiq'\nend\n</code></pre>\n<p>Then add Redis to docker-compose.yml</p>\n<pre class=\"language-shell\"><code>version: \"3\"\nservices:\n  postgresql:\n    image: postgres:13.10\n    environment:\n      - POSTGRES_PASSWORD=password\n    ports:\n      - 5432:5432\n    volumes:\n      - \"dbdata:/var/lib/postgresql/data\"\n\n  redis:\n    image: redis:alpine\n    ports:\n      - 6379:6379\n\nvolumes:\n  dbdata:</code></pre>\n<p>If Docker Composeis running, stop it with \"<code>docker compose stop</code>\". Now bring up both PostgreSQL and Redis with \"<code>docker compose up postgresql redis -d</code>\". To see that your containers are running you can run the command \"<code>docker compose ps</code>\"</p>\n<p><img src=\"https://roylindauer.dev/uploads/2024/docker-compose-ps.png\" alt=\"\" width=\"100%\"></p>\n<p>You can also see the logs from your containers by running the command \"<code>docker compose logs</code>\"</p>\n<p><img src=\"https://roylindauer.dev/uploads/2024/docker-compose-logs.png\" alt=\"\" width=\"100%\"></p>\n<p>Now start up Sidekiq and confirm that you are connected to Redis.</p>\n<pre class=\"language-shell\"><code>bundle exec sidekiq</code></pre>\n<p><img src=\"https://roylindauer.dev/uploads/2024/sidekiq-running.png\" alt=\"\" width=\"100%\"></p>\n<p>You should see that Sidekiq is connected to Redis. If you start rails from another terminal you can also confirm by going to <a href=\"https://web.archive.org/web/20231210114644/http://localhost:3000/sidekiq\" target=\"_blank\" rel=\"noopener\">http://localhost:3000/sidekiq</a>.</p>\n<p><img src=\"https://roylindauer.dev/uploads/2024/sidekiq-dashboard.png\" alt=\"\" width=\"100%\"></p>\n<p>Things are shaping up! A common thing you may have to do when developing an application is test emails. You may not want to actually send emails when you are in development but still want to ensure they are formatted correctly, are triggered when they should be, and so on. A service you could add is called \"Mailcatcher\". It's a mail server with a web UI to view the email it has received.</p>\n<p>Add mailcatcher to the services section of the docker-compose.yml file.</p>\n<pre class=\"language-shell\"><code>  mailcatcher:\n    image: dockage/mailcatcher:0.8.2\n    ports:\n      - 1080:1080\n      - 1025:1025</code></pre>\n<p>We expose the ports 1080, which is the web UI, and 1025, which is the port you need to use in your mail config. You can start the service by running the command \"<code>docker compose up -d</code>\" and it will create and start any services that are not already running.</p>\n<p><img src=\"https://roylindauer.dev/uploads/2024/mailcatcher.png\" alt=\"\" width=\"100%\"></p>\n<p>You can effectively now run any service that you need to support your development efforts with Docker Compose. Maybe you want to add an <a href=\"https://web.archive.org/web/20231210114644/https://github.com/willnorris/imageproxy\" target=\"_blank\" rel=\"noopener\">imageproxy</a> service to handle image translations. Or a URL shortening service. You could also run an Nginx container to test your Nginx proxy configurations. Really you can add anything and you don't have to do much more than add the service to docker-compose.yml.</p>\n<p>There are many images available in the <a href=\"https://web.archive.org/web/20231210114644/https://hub.docker.com/\" target=\"_blank\" rel=\"noopener\">Docker hub</a>. These are public and free to use.</p>\n<h2><a name=\"upgrading-service\"></a></h2>\n<h2><a name=\"developing-in-docker\"></a>Developing our Application in Docker</h2>\n<p>It is also possible to run and develop your application with Docker Compose. Recall that we added a volume to our postgresql service. We mounted a defined docker volume to a path in the container. We can also mount a local path into the container. This means you can create a container for the Rails app that has the specific version of Ruby installed that the app requires and whatever other libraries and gems and such, and then mount your local Rails application into that container. Changes you make locally appear in the container.</p>\n<p>There are some caveats.</p>\n<p>Docker compose runs services within a virtual network. So far we have been connecting to services over the local network because our application is running locally. Once we run it as its own container we can no longer connect to Redis over localhost, because localhost now refers to the <em>container</em>. Docker Composewill allow us to reference our containers, within the compose network, by their service name. So while locally we can connect to redis at \"redis://localhost:6379/0\" when we run our app in docker we would instead connect at \"redis://redis:6379/0\". Because we named the redis service \"redis\" that is the hostname we would use.</p>\n<p>Mounting local volumes into a container can sometimes be slow. Depending on the type of application, your operating system, and how many files are in the project, it may not even be worth doing. Docker containers are running Linux kernels. If you are running Docker on a Mac then you may experience some performance issues with sharing files over the virtual network. YMMV. If you are running a Linux desktop then you won't need to worry it's going to run fine and be performant. The filesystem and network layer between the MacOS kernel, the thin Docker Desktop VM, and the Docker container slows things down. It's something I think you should be aware of as you explore this path.</p>\n<p>This does pose another kinda interesting problem. You have a choice to make; all in on docker, or supporting services only in docker. I like having more options though, and I don't like to force other engineers to change their workflow to suit my own, so I configure my projects to be able to do both; run on bare-metal locally, and run in docker on the Docker Composenetwork. Doing so requires a little bit of application configuration to be able to determine which environment you are running so that the proper service routes are defined.</p>\n<p>For demo purposes though let's go ahead and add our Rails app as a service to docker-compose.yml</p>\n<pre class=\"language-shell\"><code>  rails:\n    build:\n      context: ./\n      dockerfile: development.Dockerfile\n    ports:\n      - 3000:3000\n    depends_on:\n      - postgresql\n      - redis\n    volumes:\n      - ./:/app</code></pre>\n<p>This is a little different than the other services. There's no <em>image</em> attribute, and we have added <em>build</em> and <em>depends_on</em>.</p>\n<p><em>depends_on</em> simply says, this container requires these other containers. When you start the rails service, those other containers will automatically be started.</p>\n<p>We removed the image attribute because we are going to build our own image. The build section tells Docker Composehow to build the image. It will look for a file called \"<em>development.Dockerfile</em>\" in the current directory. Create that file now:</p>\n<pre class=\"language-shell\"><code>FROM ruby:3.1.3\n\nARG RAILS_ENV=production\n\nRUN apt-get update -qq && \\\n    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends  libvips libvips-dev libvips-tools libpq-dev && \\\n    rm -rf /var/lib/apt/lists/* /var/cache/apt\n\nARG BUNDLER_VERSION=2.3.26\nRUN gem install \"bundler:${BUNDLER_VERSION}\" --no-document && \\\n    gem update --system && \\\n    gem cleanup\n\nWORKDIR /app\n \nCOPY Gemfile Gemfile.lock ./\nRUN bundle install --jobs \"$(nproc)\"\n\nCOPY . .\n\nCMD [\"bin/rails\", \"s\", \"-b\", \"0.0.0.0\"]\n\nEXPOSE 3000\n</code></pre>\n<p>To build your image:</p>\n<pre class=\"language-shell\"><code>docker compose build rails</code></pre>\n<p>Start the rails container with:</p>\n<pre class=\"language-shell\"><code>docker compose up rails</code></pre>\n<p>We expose port 3000 and can load the app at <a href=\"https://web.archive.org/web/20231210114644/http://localhost:3000/\" target=\"_blank\" rel=\"noopener\">http://localhost:3000</a>. You may notice a small problem, we cannot connect to the database.</p>\n<p><img src=\"https://roylindauer.dev/uploads/2024/rails-docker-error.png\" alt=\"\" width=\"100%\"></p>\n<p>We have not updated the Rails configuration. As the app is now running in a container those data services are not available at \"localhost\". We need to use the Docker Composeservice routes.</p>\n<p>Update \"config/database.yml\":</p>\n<pre class=\"language-shell\"><code>default: &default\n  adapter: postgresql\n  encoding: unicode\n  pool: <%= ENV.fetch(\"RAILS_MAX_THREADS\") { 5 } %>\n\ndevelopment:\n  <<: *default\n  url: \"postgresql://postgres:password@postgresql/docker_compose_example\"\n\ntest:\n  <<: *default\n  url: \"postgresql://postgres:password@postgresql/docker_compose_example_test\"\n\nproduction:\n  <<: *default</code></pre>\n<p>Also want to update \"config/initializers/sidekiq.rb\"</p>\n<pre class=\"language-ruby\"><code>Sidekiq.configure_server do |config|\n  config.redis = { url: 'redis://redis:6379/0' }\nend\n\nSidekiq.configure_client do |config|\n  config.redis = { url: 'redis://redis:6379/0' }\nend</code></pre>\n<p>Stop Docker Composewith \"<code>ctrl+c</code>\" and restart:</p>\n<pre class=\"language-shell\"><code>docker compose up rails</code></pre>\n<p>If you visit <a href=\"https://web.archive.org/web/20231210114644/http://localhost:3000/\" target=\"_blank\" rel=\"noopener\">http://localhost:3000</a> you should see the now very familiar Rails welcome page. Congrats, your Rails app is running in docker and connected to your services!</p>\n<p>Next, let's run sidekiq with compose. We are going to use the same dockerfile to create a new sidekiq docker image that we used to create the rails image. Then we will override the startup command that Docker Compose will run when it starts the container. By default compose will execute the command defined by the Dockerfile used to create the image. But we can override that with the service configuration in docker-compose.yml. And since this is a background job processer we will remove the ports since we are not going to need them.</p>\n<p>Add a new service to docker-compose.yml</p>\n<pre class=\"language-shell\"><code>  sidekiq:\n    build:\n      context: ./\n      dockerfile: development.Dockerfile\n    command: bundle exec sidekiq\n    depends_on:\n      - postgresql\n      - redis\n    volumes:\n      - ./:/app</code></pre>\n<p>When you start up the whole stack you will have Rails and Sidekiq running, connected to your data services.</p>\n<pre class=\"language-shell\"><code>docker compose up </code></pre>\n<p><img src=\"https://roylindauer.dev/uploads/2024/all-services-running.png\" alt=\"\" width=\"100%\"></p>\n<h2><a name=\"utilities\"></a>Utilities for Working with Docker Compose</h2>\n<p>There will be times when you need to execute commands inside a container. For example, with our Rails app, you will want to run \"rails\" commands to generate database migrations, run migrations, or other tasks. You can execute commands inside a container with Docker Compose.</p>\n<p>If it's not already running go ahead and start the Rails container in the background:</p>\n<pre class=\"language-shell\"><code>docker-compose up rails -d</code></pre>\n<p>Now say you want to run a rails command. For example, I will run the rails command to show me all available rails commands.</p>\n<pre class=\"language-shell\"><code>docker compose exec rails rails -T</code></pre>\n<p>This command says to execute the command <code>rails -T</code> on the <code>rails</code>container.</p>\n<p>It could be tedious to have to type all that out all the time. You could create a utility under the \"bin\" directory called \"drails\" that will execute commands in the container for you.</p>\n<pre class=\"language-shell\"><code>#!/bin/sh\ndocker-compose run --rm rails env EDITOR=vi bin/rails $@\n</code></pre>\n<p>This small shell script would be used as you would use <code>bin/rails</code>, you just use <code>bin/drails</code> instead. This small additional means only a minor change in your existing workflow.</p>\n<h2>Wrap Up</h2>\n<p>This has been a bit heavy on Rails, but the concepts apply to any framework and project. You could be running Laravel or NodeJS, or using MongoDB or ElasticSearch, and you'd still have the same types of dependencies, and configurations, and even probably creating helper utilities to smooth out your workflow. Docker Compose is a powerful tool to add to your workflow. I hope you have found this all helpful!</p>\n      </div>\n\n\n",
				"date_published": "2023-03-03T13:11:00-08:00",
				"url": "https://roylindauer.dev/2023/03/03/how-to-use.html"
			},
			{
				"id": "http://royldev.micro.blog/2023/02/03/running-ruby-on.html",
				"title": "Running Ruby on Rails on Docker",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<p>events {\nworker_connections 1024;\n}</p>\n<p>http {\ninclude /etc/nginx/mime.types;\ndefault_type application/octet-stream;\nserver_tokens off;\nclient_max_body_size 10m;\nkeepalive_timeout 120;\nsendfile on;\ntcp_nodelay on;\nssl_prefer_server_ciphers on;\nssl_session_cache shared:SSL:2m;\ngzip on;\ngzip_vary on;\nlog_format main &lsquo;$remote_addr - $remote_user [$time_local] &ldquo;$request&rdquo; '\n&lsquo;$status $body_bytes_sent &ldquo;$http_referer&rdquo; '\n&lsquo;&quot;$http_user_agent&quot; &ldquo;$http_x_forwarded_for&rdquo;';</p>\n<p>access_log /var/log/nginx/access.log main;</p>\n<p>upstream app_server {\nserver 127.0.0.1:3000 fail_timeout=0;\n}</p>\n<p>server {\nlisten 80 default_server;\nlisten [::]:80 default_server;\nroot /app/public;\nadd_header X-Frame-Options &ldquo;DENY&rdquo;;\nadd_header X-Content-Type-Options nosniff;\nadd_header Strict-Transport-Security &ldquo;max-age=31536000; includeSubDomains&rdquo; always;\nadd_header X-Xss-Protection &ldquo;1; mode=block&rdquo; always;\nadd_header Referrer-Policy &ldquo;origin-when-cross-origin&rdquo; always;</p>\n<pre><code>location = /favicon.ico {\n  log_not_found off;\n  access_log off;\n}\n\nlocation = /robots.txt {\n  allow all;\n  log_not_found off;\n  access_log off;\n}\n\nlocation ~ ^/(assets|packs) {\n  expires max;\n  gzip_static on;\n  log_not_found off;\n}\n\nlocation ~ /\\.ht {\n  deny all;\n}\n\n\nlocation / {\n  proxy_set_header X-Forwarded-Ssl on; \n  proxy_set_header X-Real-IP $remote_addr;\n  proxy_set_header X-Forwarded-Server $host;\n  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n  proxy_set_header Host $http_host;\n  proxy_redirect off;\n  proxy_read_timeout 300s;\n  if (!-f $request_filename) {\n    proxy_pass http://app_server;\n    break;\n  }\n}\n\nerror_page 500 502 503 504 /500.html;\nlocation = /500.html {\n  root /app/public;\n}\n</code></pre>\n<p>}\n}\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>###############################################################################</p>\n<h1 id=\"heading\"></h1>\n<h1 id=\"docker\">Docker</h1>\n<h1 id=\"heading-1\"></h1>\n<p>###############################################################################</p>\n<p>STDOUT.sync = true</p>\n<p>require &lsquo;dotenv&rsquo;\nrequire &lsquo;logger&rsquo;\nrequire &lsquo;json&rsquo;</p>\n<p>class Docker</p>\n<p>def initialize\n@config_url = false\n@pids = []\n@pid_commands = {}\n@quit = false</p>\n<pre><code>Signal.trap('QUIT') do\n  quit_all\nend\n\nSignal.trap('TERM') do\n  quit_all\nend\n</code></pre>\n<p>end</p>\n<p>def call\nlogger.info &ldquo;Starting container (rails_env: #{rails_env}, deploy_env: #{deploy_env})&rdquo;</p>\n<pre><code>if ENV['ENABLE_WEB'].to_i == 1\n  start_webpack_dev_server if rails_env == 'development'\n  start_rails_s\n  start_nginx\nend\n\nif ENV['ENABLE_WORKERS'].to_i == 1\n  start_worker\nend\n\nwait_all\n</code></pre>\n<p>end</p>\n<p>private</p>\n<p>def deploy_env\n@deploy_env ||= ENV.fetch(&lsquo;DEPLOY_ENV&rsquo;){File.file?('/.dockerenv&rsquo;) ? &lsquo;docker&rsquo; : &lsquo;local&rsquo;}\nend</p>\n<p>def logger\n@logger ||= Logger.new(STDOUT)\nend</p>\n<p>def load_environment(config_file)</p>\n<pre><code>unless File.file?(config_file)\n  logger.info 'Using local environment'\n  return\nend\n\nlogger.info &quot;Loading environment from `#{config_file}`&quot;\nDotenv::Parser.new(IO.read(config_file)).call.each_pair do |key, value|\n  ENV[key] = value.to_s\nend\n</code></pre>\n<p>end</p>\n<p>def rails_env\n@rails_env ||= ENV.fetch(&lsquo;RAILS_ENV&rsquo;, &lsquo;development&rsquo;)\nend</p>\n<p>def spawn_(command, env = {})\n@pids &laquo; Process.spawn(env, command)</p>\n<pre><code>command = &quot;#{command} `#{env.to_json}`&quot;\nlogger.info &quot;Started[#{@pids.last}] #{command}&quot;\n@pid_commands[@pids.last] = command\n</code></pre>\n<p>end</p>\n<p>def system_(command, env = {})\nlogger.info &ldquo;Running #{command}&rdquo;\nsystem(env, command)\nend</p>\n<p>def start_nginx\nspawn_ &lsquo;nginx&rsquo;\nend</p>\n<p>def start_webpack_dev_server\nspawn_ &lsquo;bin/webpack-dev-server&rsquo;\nend</p>\n<p>def start_rails_s\nsystem &lsquo;rm -f /app/tmp/pids/server.pid&rsquo;\nspawn_ &lsquo;bin/rails s -b 0.0.0.0 -p 3000&rsquo;\nend</p>\n<p>def start_worker\nspawn_ &ldquo;bundle exec sidekiq -C config/sidekiq.yml -t 60 -c #{ENV.fetch(&ldquo;RAILS_MAX_THREADS&rdquo;, 20)}&rdquo;\nend</p>\n<p>def quit_all\n@quit = true</p>\n<pre><code>@pids.each do |pid|\n  Process.kill 'TERM', pid\nend\n</code></pre>\n<p>end</p>\n<p>def wait_all\nuntil @pids.empty?\npid = Process.wait\nlogger.info &ldquo;Exited[#{pid}] #{$?.exitstatus} - #{@pid_commands[pid]}&rdquo;\n@pids -= [pid]</p>\n<pre><code>  if !@quit\n    logger.info 'Shutting down container due to component failure'\n    quit_all\n  end\nend\n</code></pre>\n<p>end</p>\n<p>end</p>\n<p>Docker.new.call\n<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->When you put this all together you can build and run your docker image with the command:<!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "<p class=\"c-lead\">I have been developing Ruby on Rails apps in Docker for several years now. I couldn't imagine not using Docker at this point!</p><h2>An Introduction to Docker</h2><p>Docker an open-source project for automating the deployment of applications as portable self sufficient <em>containers</em> that run in cloud or on premises. Docker is also a company that owns this technology. The underlying technology that powers Docker has been part of Linux for many years. Docker has built some tooling to make working with that technology seamless and easy.</p><p>A container is a sandboxed process on your machine that is isolated from all other processes on the host machine. The isolation leverages long time Linux features kernel namespaces and cgroups.</p><p>You build a Docker <em>image</em> with a file called a <em>Dockerfile</em>. This file contains all of the commands required to build the image. An image is basically a custom file system for a container. It contains everything needed to run an application, all dependencies, configs, scripts, binaries, etc. The image informs how a container should instantiate and run.</p><p>Docker is not a Virtual Machine and is not a replacement for it either. It's an entirely different thing, though some concepts may overlap. With a VM you run an entire operating system with resources allocated from the host through a layer called the hypervisor. With docker you share the hosts operating system and run just your app over a thin layer called the docker engine. VMs are heavy in terms of resource allocation, while Docker containers are lightweight. Docker images are ephemeral and are meant to be immutable. You \"compile\", ie: build, your image and run it. When you want to make changes you build a new image and run a new container. Deploying an app to a VM usually entails copying your apps files to the machine.</p><h2>Building and Running a Docker Image For Your Rails App</h2><p>What you want to do in your Dockerfile is to install any dependencies your app requires, install bundler, install ruby gems, run any asset compilation, expose any ports you wish to open, then tell the docker engine how to start your services.</p><p>My Dockerfile is a little different than <a href=\"https://web.archive.org/web/20231002014808/https://twitter.com/dhh/status/1604856555848884225\">what is proposed for Rails</a>, but it's tailored for my apps and how I run Rails and things in production. And I think that the point is that how you run your application may be different than me, or DHH, or whomever, and so you need to tailor your tooling to support that. I slice \"microservices\" from a single Docker image of my Rails app. I run a web instance, separate sidekiq instances based on queues and workload, and then some one-off tasks to perform things like database migrations. I control which services are started based on environment variables I pass into the docker container when I start them up. I don't use Procfiles to do this, but you might tailor your start up command accordingly.</p><p>Generally in all of my rails projects I run NGinx in front of Puma because I want static assets served by NGinx. It's faster overall and I can leave threads open on Puma to do real work. In the Dockerfile I install NGinx and copy a config for the server. </p><p><strong>Dockerfile</strong></p>\n\n<pre><code>\nFROM ruby:3.1.3\n\nARG RAILS_ENV=production\nENV RAILS_MASTER_KEY=\n\n# Install core packages\nRUN apt-get update -qq && \\\n    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends imagemagick nginx libvips libvips-dev libvips-tools libpq-dev && \\\n    rm -rf /var/lib/apt/lists/* /var/cache/apt\n\n# Installer bundler and update\nARG BUNDLER_VERSION=2.3.26\nRUN gem install \"bundler:${BUNDLER_VERSION}\" --no-document && \\\n    gem update --system && \\\n    gem cleanup\n\n# Install NodeJS (https://github.com/nodejs/docker-node/blob/main/14/bullseye/Dockerfile)\nARG NODE_VERSION=16.14.2\nRUN ARCH= && dpkgArch=\"$(dpkg --print-architecture)\" \\\n  && case \"${dpkgArch##*-}\" in \\\n    amd64) ARCH='x64';; \\\n    ppc64el) ARCH='ppc64le';; \\\n    s390x) ARCH='s390x';; \\\n    arm64) ARCH='arm64';; \\\n    armhf) ARCH='armv7l';; \\\n    i386) ARCH='x86';; \\\n    *) echo \"unsupported architecture\"; exit 1 ;; \\\n  esac \\\n  # gpg keys listed at https://github.com/nodejs/node#release-keys\n  && set -ex \\\n  && for key in \\\n    4ED778F539E3634C779C87C6D7062848A1AB005C \\\n    94AE36675C464D64BAFA68DD7434390BDBE9B9C5 \\\n    74F12602B6F1C4E913FAA37AD3A89613643B6201 \\\n    71DCFD284A79C3B38668286BC97EC7A07EDE3FC1 \\\n    8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600 \\\n    C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8 \\\n    C82FA3AE1CBEDC6BE46B9360C43CEC45C17AB93C \\\n    DD8F2338BAE7501E3DD5AC78C273792F7D83545D \\\n    A48C2BEE680E841632CD4E44F07496B3EB3C1762 \\\n    108F52B48DB57BB0CC439B2997B01419BD92F80A \\\n    B9E2F5981AA6E0CD28160D9FF13993A75599653C \\\n  ; do \\\n      gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \"$key\" || \\\n      gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \"$key\" ; \\\n  done \\\n  && curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-$ARCH.tar.xz\" \\\n  && curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" \\\n  && gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\\n  && grep \" node-v$NODE_VERSION-linux-$ARCH.tar.xz\\$\" SHASUMS256.txt | sha256sum -c - \\\n  && tar -xJf \"node-v$NODE_VERSION-linux-$ARCH.tar.xz\" -C /usr/local --strip-components=1 --no-same-owner \\\n  && rm \"node-v$NODE_VERSION-linux-$ARCH.tar.xz\" SHASUMS256.txt.asc SHASUMS256.txt \\\n  && ln -s /usr/local/bin/node /usr/local/bin/nodejs \\\n  # smoke tests\n  && node --version \\\n  && npm --version\n\n# Install Yarn Package Manager\nARG YARNPCKG_VERSION=1.22.18\nRUN set -ex \\\n  && for key in \\\n    6A010C5166006599AA17F08146C2130DFD2497F5 \\\n  ; do \\\n    gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \"$key\" || \\\n    gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \"$key\" ; \\\n  done \\\n  && curl -fsSLO --compressed \"https://yarnpkg.com/downloads/$YARNPCKG_VERSION/yarn-v$YARNPCKG_VERSION.tar.gz\" \\\n  && curl -fsSLO --compressed \"https://yarnpkg.com/downloads/$YARNPCKG_VERSION/yarn-v$YARNPCKG_VERSION.tar.gz.asc\" \\\n  && gpg --batch --verify yarn-v$YARNPCKG_VERSION.tar.gz.asc yarn-v$YARNPCKG_VERSION.tar.gz \\\n  && mkdir -p /opt \\\n  && tar -xzf yarn-v$YARNPCKG_VERSION.tar.gz -C /opt/ \\\n  && ln -s /opt/yarn-v$YARNPCKG_VERSION/bin/yarn /usr/local/bin/yarn \\\n  && ln -s /opt/yarn-v$YARNPCKG_VERSION/bin/yarnpkg /usr/local/bin/yarnpkg \\\n  && rm yarn-v$YARNPCKG_VERSION.tar.gz.asc yarn-v$YARNPCKG_VERSION.tar.gz \\\n  # smoke test\n  && yarn --version\n\n# Copy and Build Rails App\nWORKDIR /app\n\n# Install the Gems before copying the app code in\n# This will speed up future builds by ensuring that regular code changes do not require a full bundle install \nCOPY Gemfile Gemfile.lock ./\nRUN bundle install --jobs \"$(nproc)\"\n\n# Similar logic for npm packages. We install before copying code over. \nCOPY package.json yarn.lock ./\nRUN yarn install\n\nCOPY . .\n\nRUN if [ \"$RAILS_ENV\" = \"production\" ] ; then bin/rails assets:precompile ; fi\n\n# I run Nginx in front of Puma. I want Nginx to serve static assets. \nRUN ln -sf /dev/stderr /var/log/nginx/error.log\nRUN ln -sf /dev/stdout /var/log/nginx/access.log\nCOPY config/nginx.conf /etc/nginx/.\n\n# The docker run command starts up nginx and rails\nCMD [\"docker/run\"]\n\nEXPOSE 3000\nEXPOSE 80\n</code></pre>\n<p>The external files I am referencing in the Dockerfile:</p><p><strong>config/nginx.conf</strong></p>\n<pre><code>\nworker_processes auto;\ndaemon off;\npcre_jit on;\nerror_log /var/log/nginx/error.log warn;\npid /app/tmp/pids/nginx.pid;\ninclude /etc/nginx/modules/*.conf;\n\nevents {\n  worker_connections 1024;\n}\n\nhttp {\n  include /etc/nginx/mime.types;\n  default_type application/octet-stream;\n  server_tokens off;\n  client_max_body_size 10m;\n  keepalive_timeout 120;\n  sendfile on;\n  tcp_nodelay on;\n  ssl_prefer_server_ciphers on;\n  ssl_session_cache shared:SSL:2m;\n  gzip on;\n  gzip_vary on;\n  log_format main '$remote_addr - $remote_user [$time_local] \"$request\" '\n      '$status $body_bytes_sent \"$http_referer\" '\n      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n  access_log /var/log/nginx/access.log main;\n\n  upstream app_server {\n    server 127.0.0.1:3000 fail_timeout=0;\n  }\n\n  server {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    root /app/public;\n    add_header X-Frame-Options \"DENY\";\n    add_header X-Content-Type-Options nosniff;\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n    add_header X-Xss-Protection \"1; mode=block\" always;\n    add_header Referrer-Policy \"origin-when-cross-origin\" always;\n\n    location = /favicon.ico {\n      log_not_found off;\n      access_log off;\n    }\n\n    location = /robots.txt {\n      allow all;\n      log_not_found off;\n      access_log off;\n    }\n\n    location ~ ^/(assets|packs) {\n      expires max;\n      gzip_static on;\n      log_not_found off;\n    }\n\n    location ~ /\\.ht {\n      deny all;\n    }\n\n\n    location / {\n      proxy_set_header X-Forwarded-Ssl on; \n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-Forwarded-Server $host;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n      proxy_redirect off;\n      proxy_read_timeout 300s;\n      if (!-f $request_filename) {\n        proxy_pass http://app_server;\n        break;\n      }\n    }\n\n    error_page 500 502 503 504 /500.html;\n    location = /500.html {\n      root /app/public;\n    }\n  }\n}\n</code></pre>\n<p><strong>docker/run</strong></p><p>I control which services to start based on the environment variables I use to run the docker container. This file, `docker/run` executes a ruby script which does the heavy lifting of starting and controlling services. This is a flexible system that lets me use a single docker image to run a Rails web instance, separate Sidekiq processes, and also a service to perform database migrations for me, by simply changing some environment variables.</p>\n<pre><code>\n#!/bin/sh\nexec bundle exec ruby config/docker.rb\n</code></pre><p><strong>config/docker.rb</strong></p>\n<pre><code class=\"language-ruby\">\n\n###############################################################################\n#\n# Docker\n#\n###############################################################################\n\nSTDOUT.sync = true\n\nrequire 'dotenv'\nrequire 'logger'\nrequire 'json'\n\nclass Docker\n\n  def initialize\n    @config_url = false\n    @pids = []\n    @pid_commands = {}\n    @quit = false\n\n    Signal.trap('QUIT') do\n      quit_all\n    end\n\n    Signal.trap('TERM') do\n      quit_all\n    end\n  end\n\n  def call\n    logger.info \"Starting container (rails_env: #{rails_env}, deploy_env: #{deploy_env})\"\n\n    if ENV['ENABLE_WEB'].to_i == 1\n      start_webpack_dev_server if rails_env == 'development'\n      start_rails_s\n      start_nginx\n    end\n\n    if ENV['ENABLE_WORKERS'].to_i == 1\n      start_worker\n    end\n\n    wait_all\n  end\n\n  private\n\n  def deploy_env\n    @deploy_env ||= ENV.fetch('DEPLOY_ENV'){File.file?('/.dockerenv') ? 'docker' : 'local'}\n  end\n\n  def logger\n    @logger ||= Logger.new(STDOUT)\n  end\n\n  def load_environment(config_file)\n    \n    unless File.file?(config_file)\n      logger.info 'Using local environment'\n      return\n    end\n\n    logger.info \"Loading environment from `#{config_file}`\"\n    Dotenv::Parser.new(IO.read(config_file)).call.each_pair do |key, value|\n      ENV[key] = value.to_s\n    end\n  end\n\n  def rails_env\n    @rails_env ||= ENV.fetch('RAILS_ENV', 'development')\n  end\n\n  def spawn_(command, env = {})\n    @pids << Process.spawn(env, command)\n\n    command = \"#{command} `#{env.to_json}`\"\n    logger.info \"Started[#{@pids.last}] #{command}\"\n    @pid_commands[@pids.last] = command\n  end\n\n  def system_(command, env = {})\n    logger.info \"Running #{command}\"\n    system(env, command)\n  end\n\n  def start_nginx\n    spawn_ 'nginx'\n  end\n\n  def start_webpack_dev_server\n    spawn_ 'bin/webpack-dev-server'\n  end\n\n  def start_rails_s\n    system 'rm -f /app/tmp/pids/server.pid'\n    spawn_ 'bin/rails s -b 0.0.0.0 -p 3000'\n  end\n\n  def start_worker\n    spawn_ \"bundle exec sidekiq -C config/sidekiq.yml -t 60 -c #{ENV.fetch(\"RAILS_MAX_THREADS\", 20)}\"\n  end\n\n  def quit_all\n    @quit = true\n\n    @pids.each do |pid|\n      Process.kill 'TERM', pid\n    end\n  end\n\n  def wait_all\n    until @pids.empty?\n      pid = Process.wait\n      logger.info \"Exited[#{pid}] #{$?.exitstatus} - #{@pid_commands[pid]}\"\n      @pids -= [pid]\n\n      if !@quit\n        logger.info 'Shutting down container due to component failure'\n        quit_all\n      end\n    end\n  end\n\nend\n\nDocker.new.call\n</code></pre><p>When you put this all together you can build and run your docker image with the command:</p>\n<pre><code class=\"language-shell\">\ndocker build -t my-rails-app .\ndocker run --rm -it -p80:80 --env RAILS_MASTER_KEY=[see config/master.key] --env ENABLE_WEB=1 --env ENABLE_WORKERS=1 my-rails-app\n</code></pre><p>What this command is saying is, build a docker image named \"my-rails-app\" from the current directory. Then run the image \"my-rails-app\", mapping local port 80 to port 80 in the container, and set these environment variables in the container. The flags \"--rm\" will delete the container when you stop it, and \"-it\" instructs Docker to allocate a pseudo-TTY connected to the containers stdin; creating an interactive bash shell in the container, which is useful if you wish to connect to the container to do \"things\". If all went well you can visit http://localhost to see your app running.</p><p>At this point you have an image you can run in a production environment. How you get it there is another exercise. The Docker ecosystem is <em>complicated</em>. You could adopt <a href=\"https://web.archive.org/web/20231002014808/https://github.com/rails/mrsk\">mrsk</a> and give \"capistrano for docker\" a go. Personally, I run my own docker swarm cluster because I am a masochist, or I use AWS ECS if I am doing work for clients.</p><h2>Wrap It Up..</h2><p>I recommend you checkout <a href=\"https://web.archive.org/web/20231002014808/https://github.com/rubys/dockerfile-rails\">dockerfile-rails</a> for a jumpstart on building Docker images for your rails project. If you have a rails project you want to containerize and have some questions about how to get there I would be <a href=\"/web/20231002014808/https://www.roylindauer.com/contact.html\">happy to chat with you</a>! I think Docker is a lot of fun, and extremely useful, but am not blind to the complexity it may present at first glance. However, once you start down the path of using containers for your projects I can promise that you won't look back!</p>\n",
				"date_published": "2023-02-03T13:07:00-08:00",
				"url": "https://roylindauer.dev/2023/02/03/running-ruby-on.html",
				"tags": ["devops","sysadmin","docker","ruby-on-rails"]
			},
			{
				"id": "http://royldev.micro.blog/2022/11/05/getting-familiar-with.html",
				"title": "Getting Familiar with RSpec in Rails",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"color:#960050;background-color:#1e0010\">$</span> rails <span style=\"color:#66d9ef\">new</span> rspec<span style=\"color:#f92672\">-</span>example<span style=\"color:#f92672\">-</span>app <span style=\"color:#f92672\">-</span>T\n</code></pre></div><!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\">group <span style=\"color:#e6db74\">:development</span>, <span style=\"color:#e6db74\">:test</span> <span style=\"color:#66d9ef\">do</span>\n  gem <span style=\"color:#e6db74\">&#39;rspec-rails&#39;</span>, <span style=\"color:#e6db74\">&#39;~&gt; 6.0.0&#39;</span>\n<span style=\"color:#66d9ef\">end</span>\n</code></pre></div><!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-shell\" data-lang=\"shell\">$ bundle install\n</code></pre></div><!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-shell\" data-lang=\"shell\">$ rails generate rspec:install\n    create  .rspec\n    create  spec\n    create  spec/spec_helper.rb\n    create  spec/rails_helper.rb\n</code></pre></div><!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-shell\" data-lang=\"shell\">$ bin/rails g model User email:string password:string\n  Running via Spring preloader in process <span style=\"color:#ae81ff\">41305</span>\n      invoke  active_record\n      create    db/migrate/20221107061213_create_users.rb\n      create    app/models/user.rb\n      invoke    rspec\n      create      spec/models/user_spec.rb\n\n$ bin/rails db:migrate\n$ rspec\n\n*\n\nPending: <span style=\"color:#f92672\">(</span>Failures listed here are expected and <span style=\"color:#66d9ef\">do</span> not affect your suite<span style=\"color:#960050;background-color:#1e0010\">&#39;</span>s status<span style=\"color:#f92672\">)</span>\n\n  1<span style=\"color:#f92672\">)</span> User add some examples to <span style=\"color:#f92672\">(</span>or delete<span style=\"color:#f92672\">)</span> /Users/roylindauer/Development/rspec-example-app/spec/models/user_spec.rb\n     <span style=\"color:#75715e\"># Not yet implemented</span>\n     <span style=\"color:#75715e\"># ./spec/models/user_spec.rb:4</span>\n\n\nFinished in 0.00074 seconds <span style=\"color:#f92672\">(</span>files took 0.38603 seconds to load<span style=\"color:#f92672\">)</span>\n<span style=\"color:#ae81ff\">1</span> example, <span style=\"color:#ae81ff\">0</span> failures, <span style=\"color:#ae81ff\">1</span> pending\n</code></pre></div><!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">User</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#66d9ef\">ApplicationRecord</span>\n  validates <span style=\"color:#e6db74\">:email</span>, <span style=\"color:#e6db74\">presence</span>: <span style=\"color:#66d9ef\">true</span>\n<span style=\"color:#66d9ef\">end</span>\n</code></pre></div><!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\">require <span style=\"color:#e6db74\">&#39;rails_helper&#39;</span>\n\n<span style=\"color:#66d9ef\">RSpec</span><span style=\"color:#f92672\">.</span>describe <span style=\"color:#66d9ef\">User</span>, <span style=\"color:#e6db74\">type</span>: <span style=\"color:#e6db74\">:model</span> <span style=\"color:#66d9ef\">do</span>\n  context <span style=\"color:#e6db74\">&#34;when email address is missing&#34;</span> <span style=\"color:#66d9ef\">do</span>\n    it <span style=\"color:#e6db74\">&#34;should return a validation error&#34;</span> <span style=\"color:#66d9ef\">do</span>\n      user <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">User</span><span style=\"color:#f92672\">.</span>new<span style=\"color:#f92672\">.</span>save\n      expect(user)<span style=\"color:#f92672\">.</span>to eq(<span style=\"color:#66d9ef\">false</span>)\n    <span style=\"color:#66d9ef\">end</span>\n  <span style=\"color:#66d9ef\">end</span>\n<span style=\"color:#66d9ef\">end</span>\n</code></pre></div><!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-shell\" data-lang=\"shell\">$ rspec\n.\n\nFinished in 0.0165 seconds <span style=\"color:#f92672\">(</span>files took 0.45219 seconds to load<span style=\"color:#f92672\">)</span>\n<span style=\"color:#ae81ff\">1</span> example, <span style=\"color:#ae81ff\">0</span> failures\n</code></pre></div><!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-shell\" data-lang=\"shell\">--require spec_helper\n--format documentation\n</code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-shell\" data-lang=\"shell\">$ rspec\n\nUser\n  when email address is missing\n    should <span style=\"color:#66d9ef\">return</span> a validation error\n\nFinished in 0.00796 seconds <span style=\"color:#f92672\">(</span>files took 0.48131 seconds to load<span style=\"color:#f92672\">)</span>\n<span style=\"color:#ae81ff\">1</span> example, <span style=\"color:#ae81ff\">0</span> failures\n</code></pre></div><!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-shell\" data-lang=\"shell\">$ bin/rails g migration AddActiveFieldToUser active:boolean\n$ bin/rails db:migrate\n</code></pre></div><!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">User</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#66d9ef\">ApplicationRecord</span>\n  validates <span style=\"color:#e6db74\">:email</span>, <span style=\"color:#e6db74\">presence</span>: <span style=\"color:#66d9ef\">true</span>\n\n  scope <span style=\"color:#e6db74\">:active_users</span>, <span style=\"color:#f92672\">-&gt;</span> { where(<span style=\"color:#e6db74\">active</span>: <span style=\"color:#66d9ef\">true</span>) }\n<span style=\"color:#66d9ef\">end</span>\n</code></pre></div><div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\">describe <span style=\"color:#e6db74\">&#34;:active_users scope&#34;</span> <span style=\"color:#66d9ef\">do</span>\n  it <span style=\"color:#e6db74\">&#34;returns active users&#34;</span> <span style=\"color:#66d9ef\">do</span>\n    <span style=\"color:#66d9ef\">User</span><span style=\"color:#f92672\">.</span>new(<span style=\"color:#e6db74\">email</span>: <span style=\"color:#e6db74\">&#39;hello@example.org&#39;</span>, <span style=\"color:#e6db74\">active</span>: <span style=\"color:#66d9ef\">true</span>)<span style=\"color:#f92672\">.</span>save!\n    <span style=\"color:#66d9ef\">User</span><span style=\"color:#f92672\">.</span>new(<span style=\"color:#e6db74\">email</span>: <span style=\"color:#e6db74\">&#39;test@example.org&#39;</span>, <span style=\"color:#e6db74\">active</span>: <span style=\"color:#66d9ef\">false</span>)<span style=\"color:#f92672\">.</span>save!\n    expect(<span style=\"color:#66d9ef\">User</span><span style=\"color:#f92672\">.</span>active_users<span style=\"color:#f92672\">.</span>length)<span style=\"color:#f92672\">.</span>to eq(<span style=\"color:#ae81ff\">1</span>)\n  <span style=\"color:#66d9ef\">end</span>\n<span style=\"color:#66d9ef\">end</span>\n</code></pre></div><!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-shell\" data-lang=\"shell\">$ rspec\n\nUser\n  when email address is missing\n    should <span style=\"color:#66d9ef\">return</span> a validation error\n  :active_users scope\n    returns active users\n\nFinished in 0.01023 sec\n</code></pre></div><!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>I spent some time over the weekend getting familiar with RSpec. Gonna brain dump (with just a little bit of structure) the process and what I did and learned. To start I set up in a new rails project and kinda tweaked it into a place where I can be productive.</p>\n\n<p>What <em>is</em> RSpec though? It is a testing framwork. But it's a little different than the Minitest testing framework that ships with Rails. RSpec is a \"Behavior Driven Development\" tool. You may have heard of TDD, or \"Test Driven Development\". BDD is a close relative. BDD informs TDD and encourages collaboration in the projets development by defining and formalizing a system's behavior in a common language. With RSpec you are writing the specification for how your application actually works. As a proponent of TDD this is super exciting!</p>\n\n<h2>Installation</h2>\n\n<p>I started with a clean install of Rails. I passed in the <code>-T</code> flag to skip generation of Minitest unit test files. Since I'm gonna RSpec I do not need them.</p>\n\n```ruby\n$ rails new rspec-example-app -T\n```\n<p>Then I installed RSpec. I added <code>rspec-rails</code> to both the <code>:test</code> and <code>:development</code> sections of the projects Gemfile. The reason to add it to both the <code>:development</code> and <code>:test</code> sections is so that the rails generator will correctly generate spec files. Otherwise you have to use <code>RAILS_ENV=test</code> when using generators.</p>\n\n```ruby\ngroup :development, :test do\n  gem 'rspec-rails', '~> 6.0.0'\nend\n```\n<p>Then, in your project directory install the gem:</p>\n\n```shell\n$ bundle install\n```\n<p>Next, generate boilerplate configuration files:</p>\n\n```shell\n$ rails generate rspec:install\n    create  .rspec\n    create  spec\n    create  spec/spec_helper.rb\n    create  spec/rails_helper.rb\n```\n<h2>Configuring RSpec</h2>\n\n<p>Out of the box RSpec works just fine; As described in <code>spec/spec_helper.rb</code>:</p>\n\n<blockquote>(the defaults) provide a good initial experience with RSpec, but feel free to customize to your heart's content</blockquote>\n\n<p>But let's take a look anyways!</p>\n\n<p><strong>.rspec</strong></p>\n\n<p>You can store command line options in this file. Running the <code>rspec</code> command will read them as though you typed them on the command line. By default, when we generated boilerplate configuration, the file will only contain a single line, <code>--require spec_helper</code>. See available options with <code>rspec --help</code>. I did not add any additional flags to this file just yet.</p>\n\n<p><strong>spec/spec_helper.rb</strong></p>\n\n<p>This file is automatically loaded when we run <code>rspec</code> (because of the flag in <code>.rspec</code>).</p>\n\n<p>In this file you configure RSpec. By default RSpec uses <code>rspec-expectations</code> as the assertion/expectation library, and <code>rspec-mocks</code> as the test double library. You can specify alternate libraries in this file, such as <a href=\"https://rubygems.org/gems/wrong\" target=\"_parent\">wrong</a> or <code>stdlib/minitest</code> for expectations, or <a href=\"https://rubygems.org/gems/mocha\" target=\"_parent\">Mocha</a> or <a href=\"https://rubygems.org/gems/bogus\" target=\"_blank\">Bogus</a> as a double library. I did not change any of those defaults though. There are a lot of configuration choices you can make here. I would recommend not changing much until you need to.</p>\n\n<p>For reference here are configuration options for the <code>rspec-core</code>, <code>rspec-mocks</code>, and <code>rspec-expectations</code> gems that are installed when you installed <code>rspec-rails</code>.</p>\n\n<ul>\n  <li><a href=\"https://www.rubydoc.info/github/rspec/rspec-core/RSpec/Core/Configuration\" target=\"_blank\"><code>rspec-core</code></a></li>\n  <li><a href=\"https://www.rubydoc.info/github/rspec/rspec-mocks/RSpec/Mocks/Configuration\" target=\"_blank\"><code>rspec-mocks</code></a></li>\n  <li><a href=\"https://www.rubydoc.info/github/rspec/rspec-expectations/RSpec/Expectations/Configuration\" target=\"_blank\"><code>rspec-expectations</code></a></li>\n</ul>\n\n<p><strong>spec/rails_helper.rb</strong></p>\n\n<p>Define how rails run RSpec in this file.</p>\n\n<p>The default configuration is minimal. It defines a path to fixtures, enables transactional fixtures so that the database is rolled back after each test, and toggles on a flag to have RSPec infer the type of test it is by the location of the test file. What that means is, for example, if you are testing a model you can tell RSpec it is a model by specifying it like <code>RSpec.describe User, type: :model do</code> when creating a spec, OR you can put your test in the file <code>specs/models/user_spec.rb</code> and RSPec will automatically apply the type <code>:model</code> because it is in the models directory.</p>\n\n<p>By default RSpec is also configured to filter lines from Rails gems in backtraces.</p>\n\n<h2>Writing Specs</h2>\n\n<p>Rails will create spec files when you use a generator. For example, I created a new User model to start testing and Rails generated <code>spec/models/user_spec.rb</code> for me. Run migrations, then run <code>rspec</code> to see that it works. I haven't written a test yet but it should still show that things are working.</p>\n\n```shell\n$ bin/rails g model User email:string password:string\n  Running via Spring preloader in process 41305\n      invoke  active_record\n      create    db/migrate/20221107061213_create_users.rb\n      create    app/models/user.rb\n      invoke    rspec\n      create      spec/models/user_spec.rb\n\n$ bin/rails db:migrate\n$ rspec\n\n*\n\nPending: (Failures listed here are expected and do not affect your suite's status)\n\n  1) User add some examples to (or delete) /Users/roylindauer/Development/rspec-example-app/spec/models/user_spec.rb\n     # Not yet implemented\n     # ./spec/models/user_spec.rb:4\n\n\nFinished in 0.00074 seconds (files took 0.38603 seconds to load)\n1 example, 0 failures, 1 pending\n```\n<p>I used a feature of RSpec called <code>contexts</code> to start organizing tests. A context in RSpec groups related checks together. Context seems good for <em>when</em> or <em>if</em> conditions, such as <em>when a required parameter is missing</em></p>\n\n<p><strong>Add validation for the email field of the User model</strong></p>\n\n```ruby\nclass User < ApplicationRecord\n  validates :email, presence: true\nend\n```\n<p><strong>The first spec for the User model to ensure that email is required</strong></p>\n\n```ruby\nrequire 'rails_helper'\n\nRSpec.describe User, type: :model do\n  context \"when email address is missing\" do\n    it \"should return a validation error\" do\n      user = User.new.save\n      expect(user).to eq(false)\n    end\n  end\nend\n```\n<p><strong>Result:</strong></p>\n\n```shell\n$ rspec\n.\n\nFinished in 0.0165 seconds (files took 0.45219 seconds to load)\n1 example, 0 failures\n```\n<p>RSpec can give us a nicer output though. I learned about the command line flag <code>--format</code>. You can control the formatting of test output. I've updated <code>.rspec</code> to set the formatter to <code>documentation</code></p>\n\n<p><strong>.rspec</strong></p>\n\n```shell\n--require spec_helper\n--format documentation\n```\n```shell\n$ rspec\n\nUser\n  when email address is missing\n    should return a validation error\n\nFinished in 0.00796 seconds (files took 0.48131 seconds to load)\n1 example, 0 failures\n```\n<p>That's pretty good looking!</p>\n\n<p>You can also organize tests with <code>describe</code>. Describe seems good for <em>who</em> or <em>what</em>, such as <code>describe \"method name\"</code> or <code>describe \"scope :scope_name\"</code></p>\n\n<p>I've added a field to my model so I could test a scope.</p>\n\n```shell\n$ bin/rails g migration AddActiveFieldToUser active:boolean\n$ bin/rails db:migrate\n```\n<p>The updated model and the new test added:</p>\n\n```ruby\nclass User < ApplicationRecord\n  validates :email, presence: true\n\n  scope :active_users, -> { where(active: true) }\nend\n```\n```ruby\ndescribe \":active_users scope\" do\n  it \"returns active users\" do\n    User.new(email: 'hello@example.org', active: true).save!\n    User.new(email: 'test@example.org', active: false).save!\n    expect(User.active_users.length).to eq(1)\n  end\nend\n```\n<p><strong>Rspec output:</strong></p>\n\n```shell\n$ rspec\n\nUser\n  when email address is missing\n    should return a validation error\n  :active_users scope\n    returns active users\n\nFinished in 0.01023 sec\n```\n<p>Couple of things, I am not using fixtures yet and can already tell that is going to be a nightmare to maintain. Fixtures are necessary. RSpec feels pretty good to write. It almost feels like the tests write themselves, and I think that may be why Behavior Drive Development exists. It also almost feels like taking user acceptance criteria and codifying it in specs, which may also have been the point. Regardless, so far it has a nice overall vibe.</p>\n\n<h2>Wrapping up</h2>\n\n<p>This is getting long winded. I hope it was helpful in some way. Next time I am going to tackle a few things:</p>\n\n<ul>\n  <li>Testing ActiveJob</li>\n  <li>Setting up a Fixture library with <code>factory_bot</code></li>\n  <li>Dig into database cleaning with <code>database_cleaner</code></li>\n</ul>\n\n<p><strong>Some Resources:</strong></p>\n\n<ul>  \n  <li><a href=\"https://devhints.io/rspec\" target=\"_blank\">RSpec Cheatsheet</a></li>\n  <li><a href=\"https://www.rubydoc.info/github/rspec/rspec-core/RSpec/Core/Configuration\" target=\"_blank\">rspec-core</a></li>\n  <li><a href=\"https://www.rubydoc.info/github/rspec/rspec-mocks/RSpec/Mocks/Configuration\" target=\"_blank\">rspec-mocks</a></li>\n  <li><a href=\"https://www.rubydoc.info/github/rspec/rspec-expectations/RSpec/Expectations/Configuration\" target=\"_blank\">rspec-expectations</a></li>\n</ul>\n",
				"date_published": "2022-11-05T09:19:51-08:00",
				"url": "https://roylindauer.dev/2022/11/05/getting-familiar-with.html",
				"tags": ["ruby-on-rails","ruby","testing"]
			},
			{
				"id": "http://royldev.micro.blog/2022/09/30/execute-workflows-with.html",
				"title": "Execute Workflows with Path Filtering in CircleCI",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4\"><code class=\"language-ruby\" data-lang=\"ruby\">@path_filter_mapping <span style=\"color:#f92672\">||=</span> {\n  <span style=\"color:#e6db74\">&#39;.circleci/.*&#39;</span> <span style=\"color:#f92672\">=&gt;</span> { <span style=\"color:#e6db74\">&#39;run-ops-workflow&#39;</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#66d9ef\">true</span> },\n  <span style=\"color:#e6db74\">&#39;bin/.*&#39;</span> <span style=\"color:#f92672\">=&gt;</span> { <span style=\"color:#e6db74\">&#39;run-ops-workflow&#39;</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#66d9ef\">true</span> },\n  <span style=\"color:#e6db74\">&#39;build/.*&#39;</span> <span style=\"color:#f92672\">=&gt;</span> { <span style=\"color:#e6db74\">&#39;run-ops-workflow&#39;</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#66d9ef\">true</span> },\n  <span style=\"color:#e6db74\">&#39;infrastructure/.*&#39;</span> <span style=\"color:#f92672\">=&gt;</span> { <span style=\"color:#e6db74\">&#39;run-infrastructure-workflow&#39;</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#66d9ef\">true</span> },\n  <span style=\"color:#e6db74\">&#39;src/project1/.*&#39;</span> <span style=\"color:#f92672\">=&gt;</span> { <span style=\"color:#e6db74\">&#39;run-project1-workflow&#39;</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#66d9ef\">true</span> },\n  <span style=\"color:#e6db74\">&#39;src/project2/.*&#39;</span> <span style=\"color:#f92672\">=&gt;</span> { <span style=\"color:#e6db74\">&#39;run-project2-workflow&#39;</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#66d9ef\">true</span> }\n}\n</code></pre></div><!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>As I often love to tell people, I love monorepos and I use a monorepo for my own projects. But there is not really an out of the box solution for monorepos so you often end up having to write scripts to glue everything together. Sometimes it really does feel like wadding up a bunch of projets into a loose ball then duct taping and hot-gluing them together into a much bigger ball. It's just that over time the glue gets stronger and the tape much more pleasing to the eye and overall the techniques become more sophisticated.</p>\n\n<p>Some of the glue required for a monorepo is around building only projects that have changes. As the monorepo grows you are going to run into isues where every minor change is building, testing, and deploying every single project. This can start to become quite costly in terms of both money and time. My solution here is to use CircleCI \"pipeline parameters\" and path filtering to selectively run workflows. This way we only run builds for projects that have changes.</p>\n\n<p>The basic life cycle of a build in my monorepo looks as such:</p>\n\n<ol>\n  <li>Commit to the github repository triggers a build at CircleCI</li>\n  <li>CircleCI build runs a single workflow that calculates which pipeline parameters to enable based on path filtering, then makes an API call to CircleCI to create a new build</li>\n  <li>New build runs only the workflows enabled by the pipeline parameters</li>\n</ol>\n\n<p>This is very specific to my monorepo, so it does not include forked pull requests but you could easily update the script to support those builds as well.</p>\n\n<p>I've created a demo project on <a href=\"https://github.com/roylindauer/circleci-utils\" target=\"_blank\">Github</a> and on <a href=\"https://app.circleci.com/pipelines/github/roylindauer/circleci-utils\" target=\"_blank\">CircleCI</a></p>\n\n<p>The glue of all of this is path filtering. To be able to get a list of files that have changed we have to determine what is our <code>base revision</code> and then perform a <code>git diff</code> between the current commit and that base revision. The logic I have settled into is as such: if the current branch is a main branch (which is develop, staging, or prod in my monorepo) then set the base revision to the commit of the last successful build for this branch. If it's not a main branch then try to find the first commit that is shared in multiple branches, or the parent commit of the current branch. If that cannot be identified then we default the base revision to <code>HEAD~1</code>. We can override this entirely by setting the ENV var <code>BASE_REVISION</code> to the exact base revision we want to use. With a base revision then we can perform a <code>git diff</code> to get a list of paths, then we iterate over the paths to match them to our path filtering map. That gives us the list of pipeline parameters to enable which we use to make an API call to CircleCI to create a new build.</p>\n\n<p>The map is pretty simple. The key is the path we want to match and the value is a hash of the pipeline parameter and the value to set.</p>\n\n```ruby\n@path_filter_mapping ||= {\n  '.circleci/.*' => { 'run-ops-workflow' => true },\n  'bin/.*' => { 'run-ops-workflow' => true },\n  'build/.*' => { 'run-ops-workflow' => true },\n  'infrastructure/.*' => { 'run-infrastructure-workflow' => true },\n  'src/project1/.*' => { 'run-project1-workflow' => true },\n  'src/project2/.*' => { 'run-project2-workflow' => true }\n}\n```\n<p>There is more glue that we need to add to our wadded up monorepo ball. CircleCI has a featured called \"auto-cancel redundant builds\". You will need to disable this feature and use a custom script to take over that responsibility. The reason is that CircleCI cancels redundant builds for the entire branch, when what we want is to only cancel redundant workflows. We may want 5 builds happening for prod, but each build is a different project.</p>\n\n<p>Definitely checkout the demo project at <a href=\"https://github.com/roylindauer/circleci-utils\" target=\"_blank\">Github</a> and on <a href=\"https://app.circleci.com/pipelines/github/roylindauer/circleci-utils\" target=\"_blank\">CircleCI</a> to see each of the scripts and the circleci configuration.</p>\n\n<h2>Example:</h2>\n\n<p>I committed a change to the main branch to a file under <code>bin</code>. The workflow <code>ci</code> determined that the only workflow we need to run is <code>ops</code></p>\n\n<p><img src=\"https://roylindauer.dev/uploads/2024/01-init-build.png\" width=\"100%\"  alt=\"Auto-generated description: Two GitHub pull requests are shown with Approve status and timestamps from 4m ago.\"></p>\n\n<p>I then made a change to two projects. The <code>ci</code> workflow determined that we needed to run <code>project1</code> and <code>project2</code> workflows.</p>\n\n<p><img src=\"https://roylindauer.dev/uploads/2024/02-build-with-fails.png\" width=\"100%\" alt=\"\"></p>\n\n<p>The build failed and so I fixed the issue. Because I committed to the main branch the <code>ci</code> workflow set the <code>base revision</code> to the commit of the last successful build for the <code>main</code> branch, which was the build prior to the one that failed, and so it determined that it needed to run three workflows, <code>ops</code>, <code>project1</code>, and <code>project2</code></p>\n\n<p><img src=\"https://roylindauer.dev/uploads/2024/03-build-fixed.png\" width=\"100%\" alt=\"\"></p>\n",
				"date_published": "2022-09-30T10:30:49-08:00",
				"url": "https://roylindauer.dev/2022/09/30/execute-workflows-with.html",
				"tags": ["devops","pipeline","circleci"]
			},
			{
				"id": "http://royldev.micro.blog/2022/09/20/the-difference-between.html",
				"title": "The difference between length, size, and count in Ruby on Rails",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>I was asked recently if I could explain the difference between length, size, and count for ActiveRecord models in Ruby on Rails. Unfortunately I had no answer. But I wanted to really understand so I dug into the API docs.</p>\n\n<p>On a Ruby Array the methods <code>size</code>, <code>count</code>, and <code>length</code> are as such; <code>size</code> is an alias for length, <code>length</code> simply returns a count of the all elements in the array, and <code>count</code> is pretty neat in that you can pass in a block to count elements based on some condition. If you do not pass any arguments to count then it returns a count of all items.</p>\n\n<p><em>eg: count all even numbers from array</em></p>\n<pre><code>arr = [1, 2, 3, 4, 5, 6, 7]\narr.count { |x| x%2 == 0 } \n=> 3\n</code></pre>\n\n<p>But on Rails models these operate a bit differently. It's good to know the differences because each have their utility.</p>\n\n<p>I have a test app with a User model with one record. On the model itself only <code>count</code> works.</p>\n\n<pre><code>> User.count\nUser Count (0.7ms)  SELECT COUNT(*) FROM \"users\"\n=> 1\n\nUser.length\n>  undefined method `length' for User:Class (NoMethodError)\n\nUser.size\n> undefined method `size' for User:Class (NoMethodError)\n</code></pre>\n\n<p>But on a collection:</p>\n\n<pre><code>user = User.all\nUser.count\n> User Count (2.9ms)  SELECT COUNT(*) FROM \"users\"\n=> 1\n\nuser.size\n=> 1\n\nuser.length\n=> 1\n</code></pre>\n\n<p>Here is a call to size on a collection that is not yet loaded. It executes a <code>COUNT(*)</code>.</p>\n\n<pre><code>> User.all.size\nUser Count (4.0ms)  SELECT COUNT(*) FROM \"users\"\n=> 1\n</code></pre>\n\n<p><code>count</code> on a collection will always execute a <code>COUNT(*)</code> SQL statement to get an accurate count value. <code>size</code> returns the size of the collection, but will execute a <code>COUNT(*)</code> SQL statement if it's not already loaded. <code>length</code> returns the size of the collection from memory. If the collection has already been loaded and is in memory <code>length</code> and <code>size</code> are equivalent.</p>\n\n<p>Relying on what's in memory may give you inaccurate results as the database could be modified while you are working on that collection. But making a bunch of expensive database calls to get an accurate count is probably something you want to avoid. There may be times when a <code>count</code> is preferred, but when in doubt it's probably better to use <code>length</code>.</p>\n",
				"date_published": "2022-09-20T08:55:41-08:00",
				"url": "https://roylindauer.dev/2022/09/20/the-difference-between.html",
				"tags": ["ruby-on-rails","ruby"]
			},
			{
				"id": "http://royldev.micro.blog/2022/09/14/aws-acm-terraform.html",
				"title": "AWS ACM Terraform Module with Variable SANs",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>Here is a a flexible terraform module for creating an AWS ACM with a variable number of additional SANs.</p>\n\n<p>Our infrastructure architecture is such that we have application load balancers that may serve multiple apps, and there was a need to create SSL certificates with multiple SANs to support those apps. While possible to add multiple SSL certificates to an application load balancer there is in fact a limit and so I wanted to avoid that altogether. There may be argument that we are exposing data unecessarily by including what seem to be unrelated domains in the SSL cert but for a QA or staging environment it felt less critical or a concern. At any rate, this solved a problem and reduced the total number of certificates we need to create for the various environments.</p>\n\n<p><a href=\"https://github.com/roylindauer/terraform-variable-sans-aws-acm\" target=\"_blank\">GitHub Repo</a></p>\n\n<p>This module will create the appropriate Route53 validation records for all of the defined domains and subject alternative names. The module outputs the ARN of the ACM it creates.</p>\n\n<p>The important bit is in <code>locals</code> in <strong>main.tf</strong>. From the variables <code>domain_name</code> and <code>subject_alternative_names</code> we create a map of domains, distinct zones, certificate sans, and all certificate validation records that need to be created.</p>\n\n<pre><code># Create a data source for each distinct domain zone found\n  data \"aws_route53_zone\" \"domain\" {\n    count = length(local.distinct_zones)\n  \n    name         = local.distinct_zones[count.index]\n    private_zone = false\n  }\n\n# Define local data resources\nlocals {\n  all_domains = concat([var.domain_name.domain], [\n    for v in var.subject_alternative_names : v.domain\n  ])\n  \n  all_zones = concat([var.domain_name.zone], [\n    for v in var.subject_alternative_names : v.zone\n  ])\n\n  distinct_zones      = distinct(local.all_zones)\n  zone_name_to_id_map = zipmap(local.distinct_zones, data.aws_route53_zone.domain[*].zone_id)\n  domain_to_zone_map  = zipmap(local.all_domains, local.all_zones)\n\n  cert_san = reverse(sort([\n    for v in var.subject_alternative_names : v.domain\n  ]))\n\n  cert_validation_domains = [\n    for v in aws_acm_certificate.certificate.domain_validation_options : tomap(v)\n  ]\n}\n</code></pre>\n\n<p>The rest of the module is very simple. </p>\n\n<pre><code>\n# Request a certificate from ACM\nresource \"aws_acm_certificate\" \"certificate\"{\n  domain_name               = var.domain_name.domain\n  subject_alternative_names = local.cert_san\n  validation_method         = \"DNS\"\n\n  tags = var.tags\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\n# Create route53 records for each validation record discovered\nresource \"aws_route53_record\" \"validation_records\" {\n  count = length(distinct(local.all_domains))\n\n  zone_id = lookup(local.zone_name_to_id_map, lookup(local.domain_to_zone_map, local.cert_validation_domains[count.index][\"domain_name\"]))\n  name    = local.cert_validation_domains[count.index][\"resource_record_name\"]\n  type    = local.cert_validation_domains[count.index][\"resource_record_type\"]\n  ttl     = 60\n\n  allow_overwrite = true\n\n  records = [\n    local.cert_validation_domains[count.index][\"resource_record_value\"]\n  ]\n}\n\n# This resource represents a successful validation of an ACM certificate in concert with other resources.\nresource \"aws_acm_certificate_validation\" \"cert_validation\" {\n  count = 1\n\n  certificate_arn         = aws_acm_certificate.certificate.arn\n  validation_record_fqdns = local.cert_validation_domains[*][\"resource_record_name\"]\n}\n</code>\n</pre>\n\n<p>Example usage to create an ACM with the primary domain and a wildcard domain.</p>\n\n<pre><code>module \"ssl_cert_default\" {\n  source = \"../modules/multi-domain-acm/\"\n\n  domain_name = {\n    zone   = \"roylindauer.com\"\n    domain = \"roylindauer.com\"\n  }\n\n  subject_alternative_names = [{ \"zone\" : \"roylindauer.com\", \"domain\" : \"*.roylindauer.com\" }]\n\n  tags = { \n    \"Environment\" = \"prod\",\n    \"Description\" = \"Managed by Terraform\",\n    \"Creator\" = \"Terraform\",\n    \"Name\" = \"Prod Cluster - Roycom ACM\"\n  }\n}\n</code>\n</pre>\n\n<p>Here's an example for creating an SSL certificate with two distinct domain names and zones. For example, for an application load balancer serving multiple apps in a qa or test environment.</p>\n\n<pre><code>module \"ssl_cert_develop_alb\" {\n  source = \"./modules/multi-domain-acm/\"\n\n  domain_name = {\n    zone   = \"roylindauer.com\"\n    domain = \"*.develop.roylindauer.com\"\n  }\n\n  subject_alternative_names = [\n    {\n      \"zone\" : \"roylindauer.art\",\n      \"domain\" : \"*.develop.roylindauer.art\"\n    }\n  ]\n\n  tags = { \n    \"Environment\" = \"develop\",\n    \"Description\" = \"Managed by Terraform\",\n    \"Creator\" = \"Terraform\",\n    \"Name\" = \"Develop Cluster - Default ACM\"\n  }\n}\n</code></pre>\n",
				"date_published": "2022-09-13T20:25:42-08:00",
				"url": "https://roylindauer.dev/2022/09/13/aws-acm-terraform.html",
				"tags": ["devops","terraform","aws","certificates"]
			},
			{
				"id": "http://royldev.micro.blog/2022/09/13/a-terraform-directory.html",
				"title": "A Terraform Directory Structure",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>Here is a directory structure that I am using for Terraform that I think works pretty well. The quick and dirty of it is to think of your configurations in terms of a organizational and systems hierarchy, and to design your plans to support that hierarchy. Each tier depends on and builds upon the previous tier.</p>\n\n<p>I create a separate terraform project for each tier of my stack. I am nearly 100% in AWS these days so this is going to feel a little AWS centric, but the basic idea is the same regardless of cloud provider. </p>\n\n<p>I like this structure because it's pretty clear where things are. It also keeps my terraform plans smallish. However it is a bit tedious as it's just a bunch of different terraform configurations to manage, but also it's actually pretty fast to make changes. It keeps me honest about design since foundational tiers need to be <em>stable</em> because other tiers depend on them. I think it's created better habits in my terraform work.</p>\n\n<p><strong>Terraform Directory Struture</strong></p>\n\n<pre><code>terraform/\n  01-organization/\n  02-network/\n  03-security/\n  04-data/\n  05-application/\n</code></pre>\n\n<p>I think these are pretty self explanatory, but for the sake of being thorough-ish I expand a bit below.</p>\n\n<p>The first tier is the organization and account level stuff. For example setting up account level billing alerts.</p>\n\n<p>Second tier is networking. Defining the overall network topology that the rest of the application hangs on. This is your VPC, subnets, nat, route tables, peering connections.</p>\n\n<p>Third tier is basic security baseline, including VPC security groups, IAM groups, roles, and policies, and so forth.</p>\n\n<p>The fourth tier is for database servers, caching servers, the various data services that your application depend on and that are evergreen.</p>\n\n<p>Tier five is application specific. For example, I configure docker image repositories here, logging systems, cloudwatch events, and various other services of which my applications depend. It's the infrastructure required to run our Rails, Laravel, NextJS, and Angular artifacts.</p>\n\n<p>From this I build application environment specific configurations. For example a Docker cluster for a QA environment. It would have it's own directory and configurations but is be built upon the foundation we have here. </p>\n\n<p>Each tier has basically the same inner structure.</p>\n\n<p><strong>Terraform Directory Inner Struture</strong></p>\n\n<pre><code>terraform/\n  01-organization/\n    modules/\n      billing/\n      ...\n    backend.tf\n    data.tf\n    main.tf\n    providers.tf\n    README.md\n  02-network/\n  ...\n</code></pre>\n\n<p>So far it's served me well. I hope that if you are using vanilla terraform that you also find it useful when designing your infrastructure and IaC.</p>\n",
				"date_published": "2022-09-13T14:00:58-08:00",
				"url": "https://roylindauer.dev/2022/09/13/a-terraform-directory.html",
				"tags": ["devops","terraform"]
			},
			{
				"id": "http://royldev.micro.blog/2022/09/05/enforcing-interfaces-in.html",
				"title": "Enforcing Interfaces in Ruby",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>Ruby doesn't have the concept of an interface. Unlike say, PHP. In PHP you can specify that a class has to act like or implement specific methods. If your class fails to honor the contract of that interface and does not implement the methods of the interface, you get an error during runtime. Ruby does not have this. But we can ensure our code honors the contract of an interface by writing unit tests for our classes. I think about it in terms of \"acts as\". My class needs to act as a \"notifier\" and thus has to respond to calls to <code>send_notification</code>, for example.</p>\n  \n<p>The issue here is that you do not get a run time error if your class is supposed to act like a thing, but does not honor the contract of that thing by implementing those specific methods. If you assume that you can call a method on a class but fail to implement the method you will only get an error when your code tries to call a method that does not exist. While in PHP the contract is enforced during runtime, in Ruby you enforce it with testing.</p>\n\n<p>Take for example a system that defines an array of notification objects and iterates over them and calls a method named <code>send_notification</code> on each of those objects. Each of those notification objects is assumed to act as a notififer and has to implement <code>send_notification</code>. To ensure that our notification classes act as notifiers we create tests to confirm that the contract is honored and the interface is implemented.</p>\n\n<p>The first version of an interface test for two of our notification classes might look like:</p>\n\n<pre><code>class TestSlack < Minitest::Test\n  def setup\n    @slack = MyModule::Slack.new\n  end\n\n  def test_implements_the_sender_interface\n    assert_respond_to(@slack, :send_notification)\n  end\nend\n\nclass TestSns < Minitest::Test\n  def setup\n    @sns = MyModule::Sns.new\n  end\n\n  def test_implements_the_sender_interface\n    assert_respond_to(@sns, :send_notification)\n  end\nend\n</code></pre>\n\n<p>Executing our tests show that our classes correctly implement our interface and \"acts as\" a notifier. Our tests ensure that our notification objects implement the notificaiton sender interface.</p>\n\n<p>A refactor of this would be to create a shared test module that we can include in our test classes. This is kinda like defining an interface and \"implementing\" that interface in our class.</p>\n\n<p>Ruby is a very flexible object oriented programming language. We can extend the functionality of our classes through mixins, or Ruby modules. This applies to our test classes as well. This is called class composition and is really the Ruby way. Simply, you can include a module in a class and that class now has access to those methods defined in the module.</p> \n\n<pre><code>module NotificationSenderInterfaceTest\n  def test_implements_the_sender_interface\n    assert_respond_to(@object, :send_notification)\n  end\nend\n\nclass TestSlack < Minitest::Test\n  include NotificationSenderInterfaceTest\n\n  def setup\n    @slack = @object = MyModule::Slack.new\n  end\nend\n\nclass TestSns < Minitest::Test\n  include NotificationSenderInterfaceTest\n\n  def setup\n    @sns = @object = MyModule::Sns.new\n  end\nend\n</code></pre>\n\n<p>When I wish to create a new notification tool I first create a test class and include the interface tests required.</p>\n\n<p>A shared set of test modules helps me to ensure my classes correctly implement an interface and act as the things they need to act as. It also keeps my tests <strong>DRY</strong>, and is also a pretty good way to document code through the tests. </p>\n",
				"date_published": "2022-09-05T11:22:31-08:00",
				"url": "https://roylindauer.dev/2022/09/05/enforcing-interfaces-in.html",
				"tags": ["ruby","testing"]
			},
			{
				"id": "http://royldev.micro.blog/2022/08/08/multiplatform-git-diff.html",
				"title": "Multi-Platform Git Diff and Merge Tools",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>Maintain a single <code>.gitconfig</code> between different operating systems by using proxy scripts for <code>git diff</code> and <code>git merge</code> tools.</p>\n\n<p>We first need to know which operating system we are using. I do this by by extracting the value from <code>uname</code> and then setting the value to an environtment variable.</p>\n\n<p>On MacOS this will return <code>darwin</code>, on most Linux distributions it should return <code>linux</code>.</p>\n\n<pre><code>export DOTFILES_OS=`uname | awk '{print tolower($0)}'`\n</code></pre>\n\n<p>In your <code>.gitconfig</code> you would would typically define difftool and mergetool to whatever tools you have installed on your local system. If you are like me though you use your favorite tools <em>for the system</em> you are working on, and often those are going to be different. For example, I use meld on Linux and Kaleidescope on MacOS.</p>\n\n<p>What we are going to do instead is to configure <code>difftool</code> and <code>mergetool</code> to reference a custom bash script that will act as proxy. Here's the relevant parts of my <code>.gitconfig</code></p>\n\n<pre>\n<code>[difftool]\n    prompt = false\n[difftool \"git-diff-tool\"]\n    cmd = git-diff-tool \"$LOCAL\" \"$REMOTE\" \"$MERGED\"\n[diff]\n    tool = git-diff-tool\n\n[mergetool]\n    keepBackup = false\n    prompt = false\n[mergetool \"git-merge-tool\"]\n    cmd = git-merge-tool \"$LOCAL\" \"$BASE\" \"$REMOTE\" \"$MERGED\"\n    trustExitCode = true\n[merge]\n    tool = git-merge-tool</code>\n</pre>\n\n<p>Now on to the custom proxy scripts.</p>\n\n<p><strong>git-diff-tool</strong></p>\n\n<pre><code>#!/usr/bin/env bash\n\nLOCAL=\"$1\"\nREMOTE=\"$2\"\nMERGED=\"$3\"\n\nif [ $DOTFILES_OS == 'darwin' ]; then\n  ksdiff --partial-changeset --relative-path \"$MERGED\" -- \"$LOCAL\" \"$REMOTE\"\nfi\n\nif [ $DOTFILES_OS == 'linux' ]; then\n  meld \\\"$LOCAL\\\" \\\"$REMOTE\\\"\nfi</code>\n</pre>\n\n<p><strong>git-merge-tool</strong></p>\n\n<pre><code>#!/usr/bin/env bash\n\nLOCAL=$1\nBASE=$2\nREMOTE=$3\nMERGED=$4\n\nif [ $DOTFILES_OS == 'darwin' ]; then\n  ksdiff --merge --output \"$MERGED\" --base \"$BASE\" -- \"$LOCAL\" --snapshot \"$REMOTE\" --snapshot\nfi\n\nif [ $DOTFILES_OS == 'linux' ]; then\n  meld \"$LOCAL\" \"$BASE\" \"$REMOTE\" --output \"$MERGED\"\nfi</code></pre>\n\n<p>Ensure these scripts are executable and that they in your <code>$PATH</code></p>\n\n<p>I think this is a neat solution to working across multiple workstations while maintaining a single gitconfig. </p>\n",
				"date_published": "2022-08-07T16:00:00-08:00",
				"url": "https://roylindauer.dev/2022/08/07/multiplatform-git-diff.html",
				"tags": ["devops","sysadmin","git"]
			},
			{
				"id": "http://royldev.micro.blog/2022/02/22/my-personal-monorepo.html",
				"title": "My Personal Monorepo \u0026 Pipeline",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<p>roylart:\nstart_by_default: true\nbuild_by_default: true\nafter_build:\n- &ldquo;cmd to run after building&rdquo;\nbefore_build:\n- &ldquo;cmd to run before building&rdquo;\nbuild_type: capistrano\ntags:\n- wordpress</p>\n<p>roycom:\nbuild_type: middleman</p>\n<p>lbr:\nbuild_type: middleman</p>\n<p>matomo:\nbuild_type: capistrano</p>\n<p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<pre><code>def deploy\n  BuildSystem.logger.info &quot;Deploying #{@params[:service_name]}&quot;\n  BuildSystem.system! &quot;cd src/#{@params[:service_name]}; #{@build_args.join(' ') if @build_args} bundle exec cap #{BuildSystem.branch} deploy&quot;\nend\n</code></pre>\n<p>end\nend\n<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>A <a href=\"https://en.wikipedia.org/wiki/Monorepo\" target=\"_blank\">monorepo</a> is a software development strategy where code for many projects is stored in the same repository. The code doesn't necessarily have to be related.</p>\n\n<p>Okay, but why use a monorepo? Gathering all of my personal projects into a single repository makes it easier for me to manage and maintain the code. One repository is easier to deal with than a dozen. I can develop a common interface for building and deploying projects in the monorepo. One build tool to rule them all. Versioning is not a concern for me here, but if it were, we also get the advantage of <a href=\"https://en.wikipedia.org/wiki/Atomic_commit#Revision_control\" target=\"_blank\">atomic commits</a>.</p>\n\n<p>Another advantage of the monorepo to consider would be the ease of re-using code between projects. For example, you could have a docker image that acts as the base of several projects. That base image is just another project managed in the monorepo, but that other projects in the monorepo are themselves built upon. Or, maybe you have shared 3rd party dependencies that you could simply download once and the other projects would then consume. Simpler dependency management is pretty great!</p>\n\n<p>So what is my monorepo's directory structure?</p>\n\n<pre><code>/bin\n/build\n/config\n/infrastructure\n/src\n  /lunchboxradio\n  /roylart\n  /roycom\n  /matomo\n</code></pre>\n\n<p>Let's break this down.</p>\n<dl>\n  <dt>/bin</dt>\n  <dd>Contains utilities for working with the monorepo (eg: setup and teardown utilities).</dd>\n\n  <dt>/build</dt>\n  <dd>This contains code related to building and deploying projects in the monorepo, the common cli.</dd>\n\n  <dt>/config</dt>\n  <dd>Global configuration for the monorepo (eg: env files for docker-compose could go here)</dd>\n\n  <dt>/infrastructure</dt>\n  <dd>Code related to building and provisioning infrastructure. Think terraform and ansible.</dd>\n\n  <dt>/src</dt>\n  <dd>Home to the individual projects</dd>\n\n</dl>\n\n<p>It's great to have everything under one roof, but for me the real power in the monorepo is being able to write tooling to build, deploy, and develop each of the projects contained within. To build this system I have chosen Ruby and Rake. I like writing code with Ruby, it's very pleasant and fun, and Rake is a nice interface for writing the sort of tool I am thinking of. You could do this with any tool and language you prefer!</p>\n\n<p>The glue for my monorepo is this build system. Build system is expressed as Rake tasks. Very simply it works like this: I load project data from a file called <code>build.yml</code> then I generate build & deploy tasks for each project from that data. There is a builder class for each type of project that has methods for building and deploying the project.</p>\n\n<p>build.yml example:</p>\n<p><pre><code>all:\n  startable: false\n\nroylart:\n  start_by_default: true\n  build_by_default: true\n  after_build:\n    - \"cmd to run after building\"\n  before_build: \n    - \"cmd to run before building\"\n  build_type: capistrano\n  tags:\n    - wordpress\n\nroycom: \n  build_type: middleman\n\nlbr: \n  build_type: middleman\n\nmatomo:\n  build_type: capistrano\n\n</code></pre></p>\n\n<p>The generated rake tasks look like:</p>\n\n<p><pre><code>$ rake -T\nrake buildsystem:build                      # Build everything\nrake buildsystem:deploy                     # Deploy everything\nrake buildsystem:lbr:build                  # Build the lbr project\nrake buildsystem:lbr:deploy                 # Deploy the lbr project\nrake buildsystem:matomo:build               # Build the matomo project\nrake buildsystem:matomo:deploy              # Deploy the matomo project\nrake buildsystem:roycom:build               # Build the roycom project\nrake buildsystem:roycom:deploy              # Deploy the roycom project\nrake buildsystem:roylart:build              # Build the roylart project\nrake buildsystem:roylart:deploy             # Deploy the roylart project\n</code></pre></p>\n\n<p>Adding a new project is easy: add the src files, add the project to build.yml, incldue a builder class if it does not already exist.</p>\n\n<p>Now to build and deploy I simply run:</p>\n\n<p><pre><code>rake buildsystem:roycom:build buildsystem:roycom:deploy</code></pre></p>\n\n<p>I run a more complex version of this on a more important monorepo that builds docker images so there are additional tasks that tag and push images to a private docker repository with support for multiple docker clusters and environments. But I could also expand this to tar up a build and \"tag\" it with a version number, then \"push\" to an s3 bucket, where then deploy is to pull that tar onto some servers someplace. Really, it's a super flexible system that allows me to do anything I want.</p>\n\n<p>The builder class looks something like:</p>\n\n<p><pre><code>module BuildSystem\n  class Capistrano < Base\n    def build\n      BuildSystem.logger.info \"Bundle Install #{@params[:service_name]}\"\n      BuildSystem.system! \"cd src/#{@params[:service_name]}; bundle install\"\n    end\n\n    def deploy\n      BuildSystem.logger.info \"Deploying #{@params[:service_name]}\"\n      BuildSystem.system! \"cd src/#{@params[:service_name]}; #{@build_args.join(' ') if @build_args} bundle exec cap #{BuildSystem.branch} deploy\"\n    end\n  end\nend\n</code></pre></p>\n\n<p>With all of this in place it's now trivial to hook up to something like CircleCI, or GitHub actions, or Gitlab to build and deploy when you commit your changes, it's just running the commands you have designed.</p>\n\n<p>So that's my monorepo in a nutshell. I think this is a super flexible system and a pretty decent way to manage a bunch of projects.</p>\n",
				"date_published": "2022-02-21T16:00:00-08:00",
				"url": "https://roylindauer.dev/2022/02/21/my-personal-monorepo.html",
				"tags": ["devops","pipeline","monorepo"]
			},
			{
				"id": "http://royldev.micro.blog/2021/03/11/deploying-a-simple.html",
				"title": "Deploying a Simple Rails App with Ansible",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "<p>Ruby on Rails is quickly becoming my framework of choice for my personal websites and projects. It's a pleasure to work with and has been easy to learn. But no framework is without its challenges. One of those challenges is of course deploying the app to a server. There are a lot of options for hosting and deploying a Rails app. But, I like to run my own servers which means I have to also take care of deploying to those servers. I'd prefer to be deploying images to AWS ECS but I don't need that kind of infrastructure for my personal website. It's just a blog it can suffer seconds of downtime when I deploy updates. So my approach these days is to use Ansible to handle the deploy steps.</p>\n\n<pre><code>\n---\n- name: Deploy Rails\n  hosts: app.roylindauer.com\n  \n  vars:\n    branch: \"main\"\n    homedir: \"/myapp\"\n\n  handlers:\n    - name: restart workers\n      become: true\n      become_user: root\n      service: \n        name: rails-workers\n        state: restarted\n\n    - name: restart web\n      become: true\n      become_user: root\n      service: \n        name: rails-web\n        state: restarted\n\n    - name: restart nginx\n      become: true\n      become_user: root\n      service: \n        name: nginx\n        state: restarted\n\n  tasks:\n    - name: Get Latest Source\n      git: \n        repo: git@github.com:roylindauer/roylindauer.com.git\n        dest: /myapp\n        update: yes\n        version: \"{{ branch }}\"\n        force: yes\n      notify:\n        - restart workers\n\n    - name: Bundle Install\n      shell:\n        chdir: \"{{ homedir }}\"\n        cmd: \"bin/bundle install --without development --path vendor/bundle\"\n\n    - name: Update Yarn\n      shell:\n        chdir: \"{{ homedir }}\"\n        cmd: \"bin/yarn install\"\n    \n    - name: Check for Pending Migrations\n      shell:\n        chdir: \"{{ homedir }}\"\n        cmd: \"bin/rails db:migrate:status | grep -e '^  down' | wc -l\"\n      register: pending_migrations\n\n    - debug:\n        msg: \"Pending Migrations: {{ pending_migrations['stdout'] }}\"\n\n    - name: Stop Workers\n      become: true\n      become_user: root\n      service: \n        name: rails-workers\n        state: stopped\n      when: pending_migrations['stdout'] != '0'\n\n    - name: DB Migrations & Restart Services\n      shell:\n        chdir: \"{{ homedir }}\"\n        cmd: \"bin/rake db:migrate\"\n      when: pending_migrations['stdout'] != '0'\n      notify:\n        - restart workers\n\n    - name: Webpack\n      shell:\n        chdir: \"{{ homedir }}\"\n        cmd: \"bin/webpack\"\n      notify:\n        - restart web\n        - restart nginx\n\n    - name: Precompile Assets\n      shell:\n        chdir: \"{{ homedir }}\"\n        cmd: \"bin/rails assets:precompile\"\n      notify:\n        - restart web\n        - restart nginx\n</code></pre>\n<p>Basically, do a git pull, update gems, then check for pending migrations. If there are migrations, stop workers, run migrations. After that compile assets, then finally restart services.<br><br>For hosting a simple rails app on a single server this sort of deploy works very well.</p>\n",
				"date_published": "2021-03-10T16:00:00-08:00",
				"url": "https://roylindauer.dev/2021/03/10/deploying-a-simple.html",
				"tags": ["devops","sysadmin","ruby-on-rails","ansible","ruby"]
			},
			{
				"id": "http://royldev.micro.blog/2021/03/07/how-to-run.html",
				"title": "How to Run Rails App Server with Systemd and Ansible",
				"content_html": "<!-- raw HTML omitted -->\n<p>&hellip;snip&hellip;</p>\n<p>vars:\nrails_root: &ldquo;/myapp&rdquo;\nrails_user: &ldquo;webuser&rdquo;</p>\n<p>tasks:\n- name: Setup Rails Web Service\ntemplate:\ndest: /usr/lib/systemd/system/rails-web.service\nsrc: templates/rails-web.systemd.j2</p>\n<pre><code>- name: Enable Rails Web Service\n  systemd:\n    name: rails-web\n    daemon_reload: yes\n    enabled: yes\n    masked: no\n</code></pre>\n<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>[Service]\nType=simple\nSyslogIdentifier=rails-web\nUser={{ rails_user }}\nPIDFile={{ rails_root }}/tmp/pids/web.pid\nWorkingDirectory={{ rails_root }}\nExecStart=/bin/bash -l -c &ldquo;{{ rails_root }}/bin/rails s -b 0.0.0.0 -p 3000&rdquo;\nExecReload=/bin/kill -s USR2 $MAINPID\nExecStop=/bin/kill -s QUIT $MAINPID</p>\n<p>[Install]\nWantedBy=multi-user.target\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n",
				"content_text": "<p>Create a systemd service to run your rails app server.<br><br>Ansible tasks to create the service:</p>\n<pre><code>\n---\n\n...snip...\n\n  vars:\n    rails_root: \"/myapp\"\n    rails_user: \"webuser\"\n\n  tasks:\n    - name: Setup Rails Web Service\n      template:\n        dest: /usr/lib/systemd/system/rails-web.service\n        src: templates/rails-web.systemd.j2\n\n    - name: Enable Rails Web Service\n      systemd:\n        name: rails-web\n        daemon_reload: yes\n        enabled: yes\n        masked: no\n</code></pre>\n<p>The ansible template \"rails-web.systemd.j2\":</p><pre><code>[Unit]\nDescription=Rails Web\n\n[Service]\nType=simple\nSyslogIdentifier=rails-web\nUser={{ rails_user }}\nPIDFile={{ rails_root }}/tmp/pids/web.pid\nWorkingDirectory={{ rails_root }}\nExecStart=/bin/bash -l -c \"{{ rails_root }}/bin/rails s -b 0.0.0.0 -p 3000\"\nExecReload=/bin/kill -s USR2 $MAINPID\nExecStop=/bin/kill -s QUIT $MAINPID\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><br>But you can also do this manually by creating the file `/usr/lib/systemd/system/rails-web.service` and then runninh `systemctl daemon-reload` to let systemd know about your service. Now you can start it up with <strong>systemctl start rails-web</strong>!</p>\n",
				"date_published": "2021-03-06T16:00:00-08:00",
				"url": "https://roylindauer.dev/2021/03/06/how-to-run.html",
				"tags": ["devops","sysadmin","ruby-on-rails","ansible","ruby"]
			},
			{
				"id": "http://royldev.micro.blog/2021/03/03/simplify-terraform-by.html",
				"title": "Simplify Terraform By Generating Configurations",
				"content_html": "<!-- raw HTML omitted -->\n<pre><code>out_file = File.open(&quot;#{@output_directory}/main.tf&quot;, 'w')\nout_file.puts(rendered)\n\nformat_terraform\n</code></pre>\n<p>end</p>\n<p>def format_terraform\nsystem &ldquo;terraform fmt #{@output_directory}&rdquo;\nend</p>\n<p>def create_output_directory\ndirectory = &ldquo;terraform/#{@config[&lsquo;config_src&rsquo;]}&rdquo;</p>\n<pre><code>unless File.directory?(directory)\n  FileUtils.mkdir_p(directory)\nend\n\ndirectory\n</code></pre>\n<p>end</p>\n<p>def main(*argv)\nconfig_src = argv.last</p>\n<pre><code>@config           = YAML.load_file(&quot;configs/#{config_src}.yaml&quot;)\n@tf_template      = 'main.tf.erb'\n@output_directory = create_output_directory\n@context          = binding\n\noutput_terraform\n</code></pre>\n<p>end</p>\n<p>main(*ARGV)\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>region: us-west-1</p>\n<p>tfstate: develop</p>\n<p>cluster:\nname: DEVELOP</p>\n<p>load_balancers:\nfrontend:\nssl_certs:\n- example.com\nbackend:\nssl_certs:\n- example.com</p>\n<p>ssl_certs:\ndefault:\ndomain_name: &lsquo;example.com&rsquo;\nsubject_alternative_names:\n- &lsquo;*.example.com&rsquo;</p>\n<p>services:</p>\n<p>my_service:\ntask_count: 2\ncpu: 256\nmemory: 512\nhttps:\nload_balancer: frontend\nurl: my_service.example.com\nhealth_check:\npath: /\nautoscaling:\nmin: 2\nmax: 8\ncpu: 75</p>\n<p>my_other_service:\ntask_count: 2\ncpu: 256\nmemory: 512\nhttps:\nload_balancer: backend\nurl: my_other_service.example.com\nhealth_check:\npath: /\nautoscaling:\nmin: 2\nmax: 4\ncpu: 60\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n<!-- raw HTML omitted -->\n<p>backend &ldquo;s3&rdquo; {\nbucket = &ldquo;terraform-state&rdquo;\nkey    = &ldquo;&lt;%= @config[&lsquo;tfstate&rsquo;] %&gt;/terraform.tfstate&rdquo;\nregion = &ldquo;&lt;%= @config[&lsquo;region&rsquo;] %&gt;&rdquo;</p>\n<pre><code># Force encryption\nencrypt = true\n</code></pre>\n<p>}</p>\n<p>required_providers {\naws = {\nversion = &ldquo;~&gt; 3.24.1&rdquo;\n}\n}\n}</p>\n<p>provider &ldquo;aws&rdquo; {\nregion = &ldquo;&lt;%= @config[&lsquo;region&rsquo;] %&gt;&rdquo;\n}\n&lt;% @config[&lsquo;services&rsquo;].each do |service, values| %&gt;\n&hellip;\n&lt;% end %&gt;</p>\n<p>&hellip; etc &hellip;\n<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n",
				"content_text": "<p>Terraform is an awesome tool. To make it more awesome though we have wrapped it with some custom Ruby ERB templating to generate our terraform configurations from Yaml configurations.<br><br>Terraform uses a declarative language. You describe the state you want and it figures out how to get there. The declarative nature of Terraform does not afford us the same control that a language like Ruby can provide, which is fine, but I have found that I end up managing _massive_ Terraform configurations. It's easy to make mistakes. Generating our Terraform configurations allows us to create more robust services in a shorter amount of time. It's much easier to edit an 80 line yaml file than a 5000 line Terraform configuration.<br><br>We have a script, let's call it \"Terraform Generator\", or `tg`. We pass to `tg` an environment configuration like `develop`.</p><pre>tg develop</pre><p><br>This will generate the terraform configuration `<strong>terraform/develop/main.yml</strong>` from a yaml file at `<strong>configs/develop.yml</strong>`<br><br>The code to generate the file is pretty simple. By running the generated plan through `terraform fmt` we can also do some validation to ensure it's not broken! Simply put we read in the environments yaml file, parse an erb template with the content from that yaml file, output a terraform configuration file, then validate that file.<br><br>`tg` does basically the following:</p>\n<pre><code>\ndef output_terraform\n    renderer = ERB.new(File.open(\"templates/#{@tf_template}\").read)\n    rendered = renderer.result(@context)\n\n    out_file = File.open(\"#{@output_directory}/main.tf\", 'w')\n    out_file.puts(rendered)\n\n    format_terraform\n  end\n\n  def format_terraform\n    system \"terraform fmt #{@output_directory}\"\n  end\n\n  def create_output_directory\n    directory = \"terraform/#{@config['config_src']}\"\n\n    unless File.directory?(directory)\n      FileUtils.mkdir_p(directory)\n    end\n\n    directory\n  end\n\n  def main(*argv)\n    config_src = argv.last\n\n    @config           = YAML.load_file(\"configs/#{config_src}.yaml\")\n    @tf_template      = 'main.tf.erb'\n    @output_directory = create_output_directory\n    @context          = binding\n\n    output_terraform\n  end\n\n  main(*ARGV)\n</code></pre>\n<p><br>The yaml file may look something like:</p>\n<pre><code>\nversion: 1\n\nregion: us-west-1\n\ntfstate: develop\n\ncluster:\n  name: DEVELOP\n\nload_balancers:\n  frontend:\n    ssl_certs:\n      - example.com\n  backend:\n    ssl_certs:\n      - example.com\n\nssl_certs:\n  default:\n    domain_name: 'example.com'\n    subject_alternative_names:\n      - '*.example.com'\n\nservices:\n\n  my_service:\n    task_count: 2\n    cpu: 256\n    memory: 512\n    https:\n      load_balancer: frontend\n      url: my_service.example.com\n      health_check:\n        path: /\n    autoscaling:\n      min: 2\n      max: 8\n      cpu: 75 \n\n  my_other_service:\n    task_count: 2\n    cpu: 256\n    memory: 512\n    https:\n      load_balancer: backend\n      url: my_other_service.example.com\n      health_check:\n        path: /\n    autoscaling:\n      min: 2\n      max: 4\n      cpu: 60\n</code></pre>\n<p><br>The ERB template:<br><br></p>\n<pre><code>\nterraform {\n  required_version = \">= 0.14.5\"\n\n  backend \"s3\" {\n    bucket = \"terraform-state\"\n    key    = \"<%= @config['tfstate'] %>/terraform.tfstate\"\n    region = \"<%= @config['region'] %>\"\n\n    # Force encryption\n    encrypt = true\n  }\n\n  required_providers {\n    aws = {\n      version = \"~> 3.24.1\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = \"<%= @config['region'] %>\"\n}\n<% @config['services'].each do |service, values| %>\n   ...\n<% end %>\n\n... etc ...\n</code></pre>\n",
				"date_published": "2021-03-02T16:00:00-08:00",
				"url": "https://roylindauer.dev/2021/03/02/simplify-terraform-by.html",
				"tags": ["devops","terraform"]
			},
			{
				"id": "http://royldev.micro.blog/2018/02/14/profiling-and-debugging.html",
				"title": "Profiling and Debugging a PHP app with Xdebug and Docker",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<h2>Profiling and Debugging a PHP app with Xdebug and Docker</h2>\n\n<p>I have started using an IDE again (PHPStorm) so that I could debug some applications and do some basic app profiling. I want to use Xdebug to profile my PHP apps. I am using Docker Compose on Windows 10. I have made this very complicated for myself but here we go.</p>\n\n<p>The directory structure of my app looks like:</p>\n\n<pre class=\"code\"><code>/build/docker/php/Dockerfile\n/build/docker/php/php.ini\n/build/docker/nginx/Dockerfile\n/build/docker/nginx/default.conf\n/web (contains my php app)\ndocker-compose.yml\n</code></pre>\n\n<p>First thing is to get Xdebug setup in the PHP container.\nI am using a custom Dockerfile for my PHP container where I install a ton of additional modules and packages, install wp-cli, and copy a custom php.ini to the container.</p>\n\n<p>Here is the entire <strong>Dockerfile</strong> for the PHP container:</p>\n\n<pre class=\"code\"><code>FROM php:7.0-fpm\n\n# Install some required tools\nRUN apt-get update && apt-get install -y sudo less\n\n# Install PHP Extensions\nRUN apt-get update && apt-get install -y \\\nbzip2 \\\nlibbz2-dev \\\nlibc-client2007e-dev \\\nlibjpeg-dev \\\nlibkrb5-dev \\\nlibldap2-dev \\\nlibmagickwand-dev \\\nlibmcrypt-dev \\\nlibpng12-dev \\\nlibpq-dev \\\nlibxml2-dev \\\nmysql-client \\\nimagemagick \\\nxfonts-base \\\nxfonts-75dpi \\\n&& pecl install imagick \\\n&& pecl install oauth-2.0.2 \\\n&& pecl install redis-3.0.0 \\\n&& pecl install xdebug \\\n&& docker-php-ext-configure gd --with-png-dir=/usr --with-jpeg-dir=/usr \\\n&& docker-php-ext-configure imap --with-imap-ssl --with-kerberos \\\n&& docker-php-ext-configure ldap --with-libdir=lib/x86_64-linux-gnu/ \\\n&& docker-php-ext-enable imagick \\\n&& docker-php-ext-enable oauth \\\n&& docker-php-ext-enable redis \\\n&& docker-php-ext-enable xdebug \\\n&& docker-php-ext-install \\\nbcmath \\\nbz2 \\\ncalendar \\\ngd \\\nimap \\\nldap \\\nmcrypt \\\nmbstring \\\nmysqli \\\nopcache \\\npdo \\\npdo_mysql \\\nsoap \\\nzip \\\n&& apt-get -y clean \\\n&& apt-get -y autoclean \\\n&& apt-get -y autoremove \\\n&& rm -rf /var/lib/apt/lists/* && rm -rf && rm -rf /var/lib/cache/* && rm -rf /var/lib/log/* && rm -rf /tmp/*\n\n# Custom PHP Conf\nCOPY ./php.ini /usr/local/etc/php/conf.d/custom.ini\n\n# WP-CLI\nRUN curl -O https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar \\\n&& mv wp-cli.phar /usr/local/bin \\\n&& chmod +x /usr/local/bin/wp-cli.phar \\\n&& ln -s /usr/local/bin/wp-cli.phar /usr/local/bin/wp\n\n# Xdebug\nRUN mkdir /tmp/xdebug\nRUN chmod 777 /tmp/xdebug\n\n# Run this container as \"webuser\"\nRUN groupadd -r webuser && useradd -r -g webuser webuser\nRUN usermod -aG www-data webuser\nUSER webuser\n</code></pre>\n\n<p>custom <strong>php.ini</strong> under ./build/docker/php/:</p>\n\n<pre class=\"code\"><code>file_uploads = On\nmemory_limit = 512M\nupload_max_filesize = 256M\npost_max_size = 256M\nmax_execution_time = 600\ndisplay_errors = On\nerror_reporting = E_ALL\n\n[XDebug]\nxdebug.profiler_output_dir = \"/tmp/xdebug/\"\nxdebug.profiler_output_name = \"cachegrind.out.%t-%s\"\nxdebug.profiler_append = 1\nxdebug.profiler_enable_trigger = 1\nxdebug.trace_output_dir = \"/tmp/xdebug/\"\nxdebug.remote_enable = on\nxdebug.remote_autostart = true\nxdebug.remote_handler = dbgp\nxdebug.remote_mode = req\nxdebug.remote_port = 9999\nxdebug.remote_log = /tmp/xdebug/xdebug_remote.log\nxdebug.idekey = MYIDE\nxdebug.remote_connect_back = 1</code></pre>\n\n<p>Some important things here. I am creating a directory to store Xdebug output (/tmp/xdebug) which will be used by another container to parse and display the output. In the custom php.ini we tell Xdebug to store its output to this directory. We also configure Xdebug to enable remote debugging so that we can debug from our IDE. If you do not want to debug EVERY request you should disable <em>remote_autostart</em>. If you do this you need to pass in a specific GET/POST parameter to trigger the debugger (typically <em>XDEBUG_PROFILE</em>). Make note of the <em>remote_port</em> and <em>idekey</em> values. We need these when we configure our IDE.</p>\n\n<p>In your IDE you would configure Xdebug to listen on port <strong>9999</strong> for connections and to use the IDE Session Key <strong>MYIDE</strong> to ensure you are only debugging requests that use that session key (really only necessary for complicated setups with multiple apps on the same server).</p>\n\n<p>There are two environment variables that I set on the PHP container that are required to make this all work.</p>\n\n<p><strong>docker-compose.yml</strong></p>\n\n<pre class=\"code\"><code>php:\nbuild: ./build/docker/php/\nexpose:\n- 9000\nlinks:\n- mysql\nvolumes:\n- .:/var/www/html\n- /tmp/xdebug\nenvironment:\nXDEBUG_CONFIG: \"remote_host=192.168.99.1\"\nPHP_IDE_CONFIG: \"serverName=XDEBUG\"</code></pre>\n\n<p><em>XDEBUG_CONFIG</em> is required to tell Xdebug where the client is running. To be honest, I am not sure if this is actually required or is only required because of PHPStorm. I am using Docker Toolbox and am using the IP from the VirtualBox VM where the Docker env is running. It would be great to not have to have this param as it would be more portable.</p>\n\n<p>The variable <em>PHP_IDE_CONFIG</em> though is required for PHPStorm, and it tells my IDE which server configuration to use.</p>\n\n<p>Neither of these may be required if you are using native docker and a different IDE. /shrug</p>\n\n<p>The first part of this is done. We can now debug an app from our IDE. The second thing I wanted to do was run a profiler and inspect the results. Xdebug will output cachegrind files. We just need a way to inspect them. There are some desktop apps you can use, like <a href=\"http://kcachegrind.sourceforge.net/html/Home.html\">KCacheGrind</a>, <a href=\"https://sourceforge.net/projects/qcachegrindwin/\">QCacheGrind</a>, <a href=\"https://sourceforge.net/projects/wincachegrind/\">WinCacheGrind</a>, etc. Your IDE may even be able to parse them (PHPStorm is currently no able to for some reason). Or you can use a web based system. I opted for a web based system using <a href=\"https://github.com/jokkedk/webgrind\">WebGrind</a>. There is, conveniently, a docker container for this.</p>\n\n<p>I configured the php container to expose /tmp/xdebug as a shared volume, which is where Xdebug is configured too output cachegrind files. Then I configured the webgrind container to mount that volume. Also I pass an environment variable to tell WebGrind where to find the cachegrind files:</p>\n\n<p><strong>docker-compose.yml</strong></p>\n\n<pre class=\"code\"><code>webgrind:\n    image: devgeniem/webgrind\n    ports:\n        - 8081:80\n    volumes_from:\n        - php\n    environment:\n        XDEBUG_OUTPUT_DIR: \"/tmp/xdebug\"\n</code></pre>\n\n<p>With that we can go to http://192.168.99.100:8081 and start digging into the app profiles.</p>\n\n</p>--</p>\n\n<p>Complete <strong>docker-compose.yml</strong></p>\n\n<pre class=\"code\"><code>mysql:\n    image: mysql:latest\n    volumes_from:\n        - mysql_data\n    environment:\n        MYSQL_ROOT_PASSWORD: secret\n        MYSQL_DATABASE: project\n        MYSQL_USER: project\n        MYSQL_PASSWORD: project\n    expose:\n        - 3306\n\nmysql_data:\n    image: tianon/true\n    volumes:\n        - /var/lib/mysql\n\nnginx:\n    build: ./build/docker/nginx/\n    ports:\n        - 80:80\n    links:\n        - php\n    volumes:\n        - .:/var/www/html\n\nphp:\n    build: ./build/docker/php/\n    expose:\n        - 9000\n    links:\n        - mysql\n    volumes:\n        - .:/var/www/html\n        - /tmp/xdebug\n    environment:\n        XDEBUG_CONFIG: \"remote_host=192.168.99.1\"\n        PHP_IDE_CONFIG: \"serverName=XDEBUG\"\n\nphpmyadmin:\n    image: phpmyadmin/phpmyadmin\n    ports:\n        - 8080:80\n    links:\n        - mysql\n    environment:\n        PMA_HOST: mysql\n\nwebgrind:\n    image: devgeniem/webgrind\n    ports:\n        - 8081:80\n    volumes_from:\n        - php\n    environment:\n        XDEBUG_OUTPUT_DIR: \"/tmp/xdebug\"\n</code></pre>\n",
				"date_published": "2018-02-13T16:00:00-08:00",
				"url": "https://roylindauer.dev/2018/02/13/profiling-and-debugging.html",
				"tags": ["troubleshooting","php","debug","xdebug","docker"]
			},
			{
				"id": "http://royldev.micro.blog/2018/01/29/wp-transients-must.html",
				"title": "WP Transients must be used responsibly",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>We ran into an interesting issue with WooCommerce at work. First, here is the subject of the support request we got from our hosting provider:</p>\n\n<blockquote><p>The site is generating ~150MB/sec of transaction logs, filling 500GB of diskspace</p></blockquote>\n\n<p>Holy. Shit. A WordPress site should not be generating that much data. 150MB per second? Wow.</p>\n\n<p>How? Why?</p>\n\n<p>The simple explanation is that there is a bottleneck in WooCommerce with the filtered layer nav query objects using single transient record.</p>\n\n<pre class=\"code\"><code>// We have a query - let's see if cached results of this query already exist.\n$query_hash    = md5( $query );\n$cached_counts = (array) get_transient( 'wc_layered_nav_counts' );\n\nif ( ! isset( $cached_counts[ $query_hash ] ) ) {\n    $results                      = $wpdb->get_results( $query, ARRAY_A );\n    $counts                       = array_map( 'absint', wp_list_pluck( $results, 'term_count', 'term_count_id' ) );\n    $cached_counts[ $query_hash ] = $counts;\n    set_transient( 'wc_layered_nav_counts', $cached_counts, DAY_IN_SECONDS );\n}</code></pre>\n\n<p>What is happening here is that a sql query based on the currently selected filters is hashed and shoved into an array that is saved to a single transient record. This means that every single interaction with the filters requires a read and possible write to a single transient record. A site with any sort of traffic and let's say 9 filter widgets (with around 50 total options) will potentially generate a huge amount of unique queries. It is no wonder why we are pushing 150MB/s.</p>\n\n<p>Our quick, temporary, patch was to simply remove the transient.</p>\n\n<pre class=\"code\"><code>// We have a query - let's see if cached results of this query already exist.\n$query_hash    = md5( $query );\n$cached_counts = array();\n$results                      = $wpdb->get_results( $query, ARRAY_A );\n$counts                       = array_map( 'absint', wp_list_pluck( $results, 'term_count', 'term_count_id' ) );\n$cached_counts[ $query_hash ] = $counts;\n</code></pre>\n\n<p>You can see the massive improvement in performance after removing the transients. We applied the patch around 9:47 am.</p>\n\n<p><img src=\"https://roylindauer.dev/uploads/2024/woocommerce-post-patch.png\" width=\"100%\" alt=\"\"></p>\n\n<p>Object caching would probably help. I was surprised at how much of an improvement we saw by simply removing the transient.</p>\n\n<p>I think a good solution here would be to use unique transients for each hashed query, and not a single transient for EVERY hashed query. It would work find on small WP installs and would scale.</p>\n\n<p>I will try it out and see what we get and if the results are good I will submit a PR to the woocommerce devs.</p>\n\n<p><strong>update:</strong></p>\n\n<p>I said we should use transients responsibly. In this case, I would be creating potentially 15k additional (tiny) transient records. Is that more responsible than 1 massive 1mb transient?</p>\n\n<p>WooCommerce devs has asked that I run some performance tests. Going to do so and report back!</p>\n\n<p><strong>update 2:</strong></p>\n\n<p>Not having any transients at all is better at scale in our case since the SQL query that is executed is not that heavy and we have some decent page caching via varnish. Also our MySQL server is well tuned. Every single request to a page with the layered nav will make N requests for the transient data. If data has to be written, that is N updates per request. This single record becomes a bottleneck as the field is locked while it is being written to. Redis or Memcache would be a a better solution. WP Transients are just bad on their own.</p>\n\n\n<p><strong>update 3:</strong></p>\n\n<p>I've been able to work with WooCommerce developers and I got a PR approved and merged into WooCommerce core!</p>\n",
				"date_published": "2018-01-28T16:00:00-08:00",
				"url": "https://roylindauer.dev/2018/01/28/wp-transients-must.html",
				"tags": ["sysadmin","troubleshooting","wordpress","php","woocommerce"]
			},
			{
				"id": "http://royldev.micro.blog/2017/07/25/order-posts-via.html",
				"title": "Order Posts via ",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>A request we are getting more often is to show a list of posts, to elevate some of those posts above others, and to show the posts in a random order. Imagine a post type called \"Sponsors\". Sponsors are tiered, like \"Platinum\", \"Gold\", \"Silver\", etc. We want the Platinum sponsors to appear before Gold, Gold before Silver, and so on. We don't want to favor one particular Platinum sponsor though, we want them to be randomized but ordered by the tier.</p>\n\n<p>The number one rule we had for implementing this feature is that we could not break existing WordPress functionality. Meaning, pagination has to work, and any sort of post filtering that existed on the site must also continue to work. An additional requirement is that we could not have duplicate posts show up as we paginated results. Hence, pseudo random.</p>\n\n<p>This is achieved by ordering the result set by a weight value (aka the tier) and then to randomize those results, using the MySQL RAND() method. MySQL's RAND() method takes an optional $SEED parameter. If we pass the same $SEED value to our query for each request we can maintain the same random order as we paginate through posts. This ensures that we do not have duplicates. I am generating the seed value using \"date('ymd')\". The value will change daily, and create a new randomness to the posts each day. Weight is derived from the tier that the posts are assigned. In my case, we use ACF and an ACF field that allows a user to select a single value from a Tier taxonomy. Knowing this, I used the postmeta value of the term id that is selected to get the slug of the term itself. I then used a CASE statement in my query to assign a weight value based on the slug of the selected taxonomy term. Tier1 is assigned 1, tier2 is assigned 2, if there is no term, weight is 9999 (so that these posts always show up after a tiered post). The CASE statement looks like:</p>\n\n<pre class=\"code\"><code>SELECT wp_posts.*, CASE weightterms.slug WHEN 'tier1' THEN 1 WHEN 'tier2' THEN 2 ELSE 9999 END AS weight FROM wp_posts ...</code></pre>\n\n<p>In order for this to work we need to JOIN the wp_terms table based on the metavalue of the selected tier taxonomy.</p>\n\n<pre class=\"code\"><code>LEFT JOIN wp_postmeta as weightmeta ON (weightmeta.post_id = wp_posts.ID AND weightmeta.meta_key = \"sk_tier\")\nLEFT JOIN wp_terms as weightterms ON (weightmeta.meta_value = weightterms.term_id)</code></pre>\n\n<p>The query basically looks like this when it is compiled (this is a simplified example of how the resulting MySQL query is going to look):</p>\n\n<pre class=\"code\"><code>SELECT\n    wp_posts.*,\n    CASE weightterms.slug WHEN 'tier1' THEN 1 WHEN 'tier2' THEN 2 ELSE 9999 END AS weight\nFROM\n    wp_posts\nLEFT JOIN wp_postmeta as weightmeta ON (weightmeta.post_id = wp_posts.ID AND weightmeta.meta_key = \"sk_tier\")\nLEFT JOIN wp_terms as weightterms ON (weightmeta.meta_value = weightterms.term_id)\nWHERE wp_posts.post_type = 'sponsors'\nORDER BY\n    weight ASC,\n    RAND(20170725);</code></pre>\n\n<p>The goal is to make WordPress write this query for us in the loop. We can do this using filters that modify the WP_Query. The first thing we need to do is to be able to identify the WP_Query so that we do not alter _other_ queries on the site. We only want to change the query that loads posts from our custom Sponsors post type. To do this we add a custom query_var to WordPress and then check for that query_var in our wp_query. Add the query var:</p>\n\n<pre class=\"code\"><code>add_filter( 'query_vars', array( $this, 'theme_query_vars_filter' ), 10, 2 );\nfunction theme_query_vars_filter( $qvars ) {\n    $qvars[] = 'is_pseudorandom_query';\n    return $qvars;\n}</code></pre>\n\n<p>We now inject this param into our main query using \"pre_get_posts\". We do not want our listing of sponsor posts in the admin area of WordPress to be ordered randomly, so we need to check that we are not is_admin().</p>\n\n<pre class=\"code\"><code>add_filter( 'pre_get_posts', 'theme_pre_get_posts', 10, 2 );\nfunction theme_pre_get_posts( $wp_query ) {\n    if ( !is_admin() && isset( $wp_query->query['post_type'] ) && $wp_query->query['post_type'] == 'sponsors' ) {\n        $wp_query->set( 'is_pseudorandom_query', true );\n    }\n}</code></pre>\n\n<p>This function checks the wp_query object passed to it. If the post type is our custom post type we set the \"is_pseudorandom_query\" query var to true. With this set we can now setup our wp_query filters. If you are using a custom WP_Query object you can pass \"is_pseudorandom_query\" as one of the $args:</p>\n\n<pre class=\"code\"><code>$my_custom_query = new \\WP_Query( [ \"is_pseudorandom_query\" => true ] );</code></pre>\n\n<p>Now to the WP_Query filters. The three filters we need are <em>posts_fields</em> to add our CASE statement, <em>posts_join</em> to add our custom LEFT JOINS, and <em>posts_orderby</em> to order by our new weight value and then by RAND().</p>\n\n<pre class=\"code\"><code>add_filter( 'posts_fields', 'theme_sponsor_posts_fields', 10, 2 );\nadd_filter( 'posts_join', 'theme_sponsor_posts_join', 10, 2 );\nadd_filter( 'posts_orderby', 'theme_sponsor_posts_orderby', 10, 2 );</code></pre>\n\n<p>The functions:</p>\n\n<pre class=\"code\"><code>function theme_sponsor_posts_fields( $select, $wp_query ) {\n    if ( $wp_query->get( 'is_pseudorandom_query' ) == true ) {\n        $select .= \", CASE weightterms.slug WHEN 'tier1' THEN 1 WHEN 'tier2' THEN 2 ELSE 9999 END AS weight\";\n    }\n    return $select;\n}\n\nfunction theme_sponsor_posts_join( $join, $wp_query ) {\n    if ( $wp_query->get( 'is_pseudorandom_query' ) == true ) {\n        $join .= ' LEFT JOIN wp_postmeta as weightmeta ON (weightmeta.post_id = wp_posts.ID AND weightmeta.meta_key = \"sk_tier\")';\n        $join .= ' LEFT JOIN wp_terms as weightterms ON (weightmeta.meta_value = weightterms.term_id)';\n    }\n    return $join;\n}\n\nfunction theme_sponsor_posts_orderby( $orderby, $wp_query ) {\n    if ( $wp_query->get( 'is_pseudorandom_query' ) == true ) {\n        $orderby = 'weight ASC, RAND(' . date('ymd') . ') ASC';\n    }\n    return $orderby;\n}</code></pre>\n\n<p>In each function we inspect the passed $wp_query object to see if \"is_pseudorandom_query\" is set. If it is, then we modify the query.</p>\n\n<p>And there it is. We can now order posts by tier, and then randomize each tier.</p>\n",
				"date_published": "2017-07-24T16:00:00-08:00",
				"url": "https://roylindauer.dev/2017/07/24/order-posts-via.html",
				"tags": ["webdev","wordpress"]
			},
			{
				"id": "http://royldev.micro.blog/2016/12/20/regular-expression-of.html",
				"title": "Regular Expression of the day",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<pre class=\"code\"><code>(?!(([1,2,3,4,5,6,8,9,0])\\2{9}))(?!((1234567890|0987654321)))(((\\+?\\d{1,2}[\\s.-])?\\(?\\d{3}\\)?[\\s.-]\\d{3}[\\s.-]\\d{4})|(\\d{10,}))</code></pre>\n\n<p>Match generic US phone numbers. Ignore 1-9 repeating (ie: 4444444444, 1111111111, (except 7, we want 7777777777 for testing)). Allow for spaces, no spaces, dashes, or period delimiters.</p>\n\n<p>Of course we _should_ use a standard phone number regex, but of course this project calls for something unique.</p>\n\n<p>RegEx is weird, but fun.</p>\n",
				"date_published": "2016-12-19T16:00:00-08:00",
				"url": "https://roylindauer.dev/2016/12/19/regular-expression-of.html",
				"tags": ["webdev","regex"]
			},
			{
				"id": "http://royldev.micro.blog/2016/10/05/technical-documentation-for.html",
				"title": "Technical Documentation for a Web Development Project",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>Working on a large web development project requires that you have clear direction and good technical documentation. The designer needs to understand the functional requirements and the data model in order to deliver a successful design. The themer need to understand the UI/UX spec in order to deliver accurate and functional markup. The developers need to understand the functional requirements and data model in order to build out the CMS backend. Creating these documents, as a team, will help guide the project to success.</p>\n\n<p>The documents described below narrow the scope of the project, each one focusing deeper and deeper on each aspect of the project. The process of defining the scope in the lifecycle of a project follows this informational flow; <em>SOW -> Technical Requirements -> Functional Requirements -> Data Model -> Sitemap -> UI/UX Specifications -> QA Plan</em>. Each document is a closer view of the scope of the project. The SOW is very high level while the UI/UX spec is in the weeds. I am primarily a web developer and web architect so I will focus on the architects/developers needs around these documents.</p>\n\n<h2>Technical Requirements</h2>\n\n<p>The technical requirements document is short and sweet. It is the \"non functional\" requirements of the project. This document outlines the technologies and services upon which the system will be built. Common sections in this doc include:</p>\n\n<ul>\n \t<li><strong>Technologies</strong>\n<ul>\n \t<li>Include the CMS and version, requirements around the underlying programming languages and versions, operating system requirements, search technologies, etc.</li>\n</ul>\n</li>\n \t<li><strong>Services</strong>\n<ul>\n \t<li>A list of 3rd party services we are integrating with, or utilizing in some fashion. Usually this list implies we are interacting with an API, or in need of some sort of XML feed, or something similar.</li>\n</ul>\n</li>\n \t<li><strong>Web Hosting Partner</strong>\n<ul>\n \t<li>Information around the web host. Include information around backups, support, and redundancies here.</li>\n</ul>\n</li>\n \t<li><strong>Browser Support</strong>\n<ul>\n \t<li>Which browsers the site must support. Include mobile devices as well.</li>\n</ul>\n</li>\n \t<li><strong>Performance</strong>\n<ul>\n \t<li>Include specific requirements around performance metrics. For example, \"time to first byte must be between 200-400ms\".</li>\n \t<li>If you are using New Relic to monitor performance include the key metrics here.</li>\n \t<li>Include and expand on specific caching requirements, asset minification, caching, CDNs, etc.!</li>\n</ul>\n</li>\n</ul>\n\n<h2>Functional Requirements</h2>\n\n<p>The functional requirements document is the meat of the actual website. The functional requirements document describes the actual functions of the site. I tend to organize this document into the following:</p>\n\n<ul>\n \t<li><strong>Features</strong>\n<ul>\n \t<li>Describe the unique functionality of the site here. Describe how 3rd party integration's will work. Describe requirements around backend functionality. Include SEO requirements, schema markup, social sharing and integration. Does the site have an events calendar? Describe in detail the functions of the calendar.</li>\n \t<li>In project management terms, each feature here would correspond with a \"component\" in Jira (for example), that would be estimated, prioritized, and groomed until it was ready to be developed. Conversely, any component would have a corresponding set of requirements.</li>\n</ul>\n</li>\n \t<li><strong>Usability</strong>\n<ul>\n \t<li>Requirements around usability. For example, ADA requirements. Does the site have to meet any web accessibility standards? Should content be made available in multiple formats? Should the entire site be navigable via keyboard?</li>\n</ul>\n</li>\n \t<li><strong>Use Cases & Work Flows</strong>\n<ul>\n \t<li>Use this area to define expected workflows, such as content moderation, user sign up process, or complex form interactions.</li>\n</ul>\n</li>\n \t<li><strong>Legal</strong>\n<ul>\n \t<li>Cookie Notifications.</li>\n \t<li>Compliance review and process by a governing body.</li>\n \t<li>PII or HIPAA requirements.</li>\n</ul>\n</li>\n</ul>\n\n<h2>Data Modeling</h2>\n\n<p>The data model is often overlooked and just sorta dealt with. A designer will often times dictate the data model through their design. This will put impossible and/or expensive constraints on the developer. The data model informs design.</p>\n\n<p>The data model document is a diagram that clearly illustrates how the data in the system is organized, how it is categorized, how it is searched, and how the data relates to each other. You should include all of the fields for each content type that will be required. This might be one of the most important documents. It will have a large impact on all teams.</p>\n\n<h2>Sitemap</h2>\n\n<p>This is a client facing document that shows a sites overall organization, design goals, and information architecture strategy.</p>\n\n<h2>UI/UX Specifications</h2>\n\n<p>The UI/UX Specification document is a detailed document that explains how a component should operate from a UI/UX perspective. For example describing the behavior of a header. You would tell the developer that on page load the header will be transparent, when a user starts to scroll the header will animate to become opaque. The animation should happen in 200ms. As a user scrolls the header will be fixed to the top of the page. It is extremely helpful to include diagrams, flowcharts, and screenshots to illustrate the described behaviors.</p>\n\n<p>Remember that you are not telling the developer <em>how</em> to do something, you are telling them <em>what</em> it must do.</p>\n\n<h2>Summary</h2>\n\n<p>To summarize, each document informs the next, and each document helps clarify and define scope which should make estimation and timeline planning more accurate for the project manager, reduces the amount of assumptions a developer has to make, and defines some guardrails for creative. The project manager ultimately \"owns\" these documents, but it is absolutely a collaborative effort in creating them. In my perfect world it reduces guesswork and limits the amount of \"oh shit\" moments at the end of the project.</p>\n",
				"date_published": "2016-10-04T16:00:00-08:00",
				"url": "https://roylindauer.dev/2016/10/04/technical-documentation-for.html",
				"tags": ["webdev","documentation","project management"]
			},
			{
				"id": "http://royldev.micro.blog/2016/09/22/tls-peer-verification.html",
				"title": "TLS Peer Verification w/PHP 5.6 and WordPress SMTP Email plugin",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>We ran into an issue after upgrading from PHP 5.5 to 5.6. We were no longer able to send email via the awesome <a href=\"https://wordpress.org/plugins/wp-mail-smtp/\">WordPress SMTP Email plugin</a>. Turns out that PHP 5.6 introduced some <a href=\"http://php.net/manual/en/migration56.openssl.php\">enhancements to its OpenSSL implementation</a>. Stream wrappers now verify peer certificates and hostnames by default. This was causing the form submissions on our site to fail. Clearly there are some issues with our Postfix config and certs. While we sort out those issues, we were able to \"solve\" our immediate problem, and disabled peer verification in the plugin without editing core files. Thankfully the plugin developer included a filter that would allow us to modify the options array that is passed to PHPMailer.</p>\n\n<pre class=\"code\"><code>add_filter('wp_mail_smtp_custom_options', 'my_wp_mail_smtp_custom_options');\n\nfunction my_wp_mail_smtp_custom_options($phpmailer) {\n    $phpmailer->SMTPOptions = array(\n        'ssl' => array(\n            'verify_peer' => false,\n            'verify_peer_name' => false\n        )\n    );\n    return $phpmailer;\n}</code></pre>\n\n<p>Woot!</p>\n\n<p>Thanks random WordPress forum user for the solution!</p>\n",
				"date_published": "2016-09-21T16:00:00-08:00",
				"url": "https://roylindauer.dev/2016/09/21/tls-peer-verification.html",
				"tags": ["troubleshooting","webdev","php"]
			},
			{
				"id": "http://royldev.micro.blog/2016/06/13/whitelist-ips-in.html",
				"title": "Whitelist IPs in Nginx",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>I want to whitelist my clients IP addresses (and my office IPs) to allow them to view a site, while the rest of the world will be redirected to another site, using Nginx. My Nginx server is behind a load balancer.</p>\n\n<p>Using the geo module I am able to do this rather easily. By default, <strong>geo</strong> will use <strong>$remote_addr</strong> for the IP address. However, because our server is behind a load balancer this will not work, as it would always be the IP of the load balancer. You can pass in a parameter to geo to specify where it should get the IP value. In this case, we want to get the IP from</p>\n\n<p><strong>$http_x_forwarded_for</strong>.</p>\n\n<pre class=\"code\"><code>geo $http_x_forwarded_for $redirect_ips {\n  default     1;\n  1.2.3.4/32  0;\n  1.2.3.5/32  0;\n  9.8.7.6/32  0;\n}</code></pre>\n\n<p>What this is doing is assigning the variable $redirect_ips the value after the IP address. So, if my IP is 1.2.3.4, $redirect_ips will have a value of 0, or false. If my ip is not matched, it will get the default value of 1, or true;</p>\n\n<p>Ok, with that, my server directive now looks like:</p>\n\n<pre class=\"code\"><code># Site that is not quite ready for the public to see, but we want to test on prod\nserver {\n    listen 80;\n    server_name es.example.com;\n\n    if ( $redirect_ips ) {\n        return 302 https://us.example.com$request_uri;\n    }\n\n    # the rest of my server directive goes below this line...\n    # removed for clarity in this example.\n\n}</code></pre>\n",
				"date_published": "2016-06-12T16:00:00-08:00",
				"url": "https://roylindauer.dev/2016/06/12/whitelist-ips-in.html",
				"tags": ["sysadmin"]
			},
			{
				"id": "http://royldev.micro.blog/2016/05/17/capistrano-tasks-for.html",
				"title": "Capistrano tasks for Magento",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>Custom tasks for Capistrano that I am using to help manage a Magento website.</p>\n\n<pre class=\"code\"><code>set :linked_files, %w{app/etc/local.xml .htaccess robots.txt}\nset :linked_dirs, %w{sitemap var media}\n\nnamespace :mage do \n\n  task :restart do\n    on roles(:app) do\n      execute \"cd #{current_path} && rm -f maintenance.flag\"\n    end\n  end\n\n  task :disable do \n    on roles(:app) do\n      execute \"cd #{current_path} && touch maintenance.flag\"\n    end\n  end\n\n  task :enable do \n    on roles(:app) do\n      execute \"cd #{current_path} && rm -f maintenance.flag\"\n    end\n  end\n\n  task :clear_cache do \n    on roles(:app) do\n      execute \"cd #{shared_path} && rm -rf var/cache/*\"\n    end\n  end\n\n  task :reindex do \n    on roles(:app) do\n      execute \"cd #{release_path}/shell && php -f indexer.php -- reindexall\"\n    end\n  end\n\n  task :create_config do  \n    on roles(:app) do\n      execute :touch, shared_path.join('app/etc/local.xml')\n      execute :touch, shared_path.join('robots.txt')\n    end\n  end\nend\n\n\nbefore 'deploy:check:linked_files', 'mage:create_config'\nafter 'deploy:restart', 'mage:clear_cache'\n</code></pre>\n\n<p>Save this as mage.cap and put it under lib/capistrano/tasks/. Then in your Capfile add the following to load your custom tasks.</p>\n\n<pre class=\"code\"><code>Dir.glob('lib/capistrano/tasks/*.cap').each { |r| import r }</code></pre>\n\n<p>Now you can reindex the site, clear caches, put site into maintenance mode.</p>\n\n<pre class=\"code\"><code>cap production mage:reindex\ncap production mage:clear_cache</code></pre>\n",
				"date_published": "2016-05-16T16:00:00-08:00",
				"url": "https://roylindauer.dev/2016/05/16/capistrano-tasks-for.html",
				"tags": ["devops","sysadmin","webdev","capistrano","magento"]
			},
			{
				"id": "http://royldev.micro.blog/2016/05/06/creating-link-tags.html",
				"title": "Creating link tags with hreflang attributes in CraftCMS",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>I want to preface this with the following; I think CraftCMS is a poor CMS. I dislike many things that it does and cannot recommend it as a professional CMS. Having said, that, I am working with it a lot these days at work and recently had to add link tags with hreflang attributes to the head. For some reason the CMS does not do this for you. SproutSEO does not do this either.</p>\n\n<pre class=\"code\"><code>{% for locale in craft.i18n.getSiteLocales() %}\n  {% if entry is defined %}\n    {% set localeEntry = craft.entries.id(entry.id).locale(locale).first %}\n    {% if localeEntry.locale == locale %}\n      <link rel=\"alternate\" hreflang=\"{% if locale == 'en_us' %}x-default{% else %}{{ locale|replace('_','-') }}{% endif %}\" href=\"{{ craft.config.siteUrl[locale.getId()] }}/{{ (localeEntry.uri != '__home__' ? localeEntry.uri) }}\">\n    {% endif %}\n  {% else %}\n    <link rel=\"alternate\" hreflang=\"{% if locale == 'en_us' %}x-default{% else %}{{ locale|replace('_','-') }}{% endif %}\" href=\"{{ craft.config.siteUrl[locale.getId()] }}/{{ craft.request.getPath() }}\">\n  {% endif %}\n{% endfor %}</code></pre>\n\n<p>What this Twig snippet is doing is iterating over all of the defined site locales, looks up the current entries locale information to get the current entries specific locale slug. We grab the SiteURL parameter defined in the general config file and render the url with the locale specific entry slug and site url. And since this particular site wants to make en-us the default, we check for that and render accordingly if the locale matches. If there is no entry, we just take the request path and locale specific site url to render the url.</p>\n\n<p>General.php config example:</p>\n\n<pre class=\"code\" title=\"general.php siteUrl config example\"><code>    '*' => array(\n        'siteUrl'  => array(\n          'en_us' => 'http://us.example.org',\n          'es_es' => 'http://es.example.org',\n          'de_de' => 'http://de.example.org'\n        ),\n    )</code></pre>\n\n<p>It really bugs me that there isn't a Twig variable or method in Craft CMS to do this for me.</p>\n",
				"date_published": "2016-05-05T16:00:00-08:00",
				"url": "https://roylindauer.dev/2016/05/05/creating-link-tags.html",
				"tags": ["webdev","craftcms"]
			},
			{
				"id": "http://royldev.micro.blog/2016/02/09/setting-up-git.html",
				"title": "Setting up Git HTTP Backend for local collaboration",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<p>You should be able to make changes and push to your remote.</p>\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>You want to share a topic branch with a colleague but do not want to push that branch upstream to Github/BitBucket/GitLab, etc. How do you do this? You could create a patch and email it. Or you could do it in the most crazy way possible and use Apache and allow your colleague to pull from your repo directly. This does take a bit more time to setup, but it would also be absolutely crazy dumb for everyone involved. Basically, let's setup a git server on your workstation!</p>\n\n<p>First create a place to store your repos. Let's also create a test repo to work with to make sure everything is working.</p>\n\n<pre class=\"code\"><code>mkdir -p ~/Sites/git\ncd ~/Sites/git\nmkdir testproject.git\ncd testproject.git\ngit init --bare</code></pre>\n\n<p>Next let's setup Apache (I am using OS X El Capitan with Apache 2.4).</p>\n\n<p>Edit <em>/private/etc/apache2/httpd.conf. </em></p>\n\n<p>Ensure the following modules are being loaded.</p>\n\n<pre class=\"code\"><code>LoadModule cgi_module libexec/apache2/mod_cgi.so\nLoadModule env_module libexec/apache2/mod_env.so</code></pre>\n\n<p>Uncomment the following line:</p>\n\n<pre class=\"code \"><code>Include /private/etc/apache2/extra/httpd-vhosts.conf</code></pre>\n\n<p>Edit <em>/private/etc/apache2/extra/httpd-vhosts.conf</em>.</p>\n\n<p>I removed the existing virtualhosts since I actually do all of my development with Vagrant and Linux. So I really have no need to have anything more than a single virtualhost on my Mac.</p>\n\n<pre class=\"code\"><code><VirtualHost *:80>\n\n    DocumentRoot <strong>/Users/user/Sites</strong>\n\n    <Directory \"<strong>/Users/user/Sites/</strong>\">\n        Options +Indexes +MultiViews +FollowSymLinks +ExecCGI\n        AllowOverride All\n        Require all granted\n    </Directory>\n\n    <Directory \"/Library/Developer/CommandLineTools/usr/libexec/git-core/\">\n        Options +ExecCGI\n        Require all granted\n    </Directory>\n\n    <LocationMatch \"^/git/\">\n        Require all granted\n    </LocationMatch>\n\n    SetEnv GIT_PROJECT_ROOT <strong>/Users/user/Sites/git</strong>\n    SetEnv GIT_HTTP_EXPORT_ALL\n    SetEnv REMOTE_USER <strong>user</strong>\n    ScriptAlias /git/ /Library/Developer/CommandLineTools/usr/libexec/git-core/git-http-backend/\n\n</VirtualHost></code></pre>\n\n<p>Edit the the bolded parts to suit your local setup.</p>\n\n<p>Restart apache.</p>\n\n<pre class=\"code \"><code>sudo apachectl restart</code></pre>\n<p>Your git repos will now be available at <em>http://locahost/git/<<strong>REPONAME</strong>.git></em>.</p>\n\n<p>You should now be able to clone your empty repo.</p>\n\n<p>Let's test it out.</p>\n\n<pre class=\"code\"><code>cd ~/Sites\ngit clone http://localhost/git/testproject.git testproject\ncd testproject</code></pre>\nYou should be able to make changes and push to your remote.\n<pre class=\"code \"><code>echo '# README' > README.me\ngit add README.md\ngit commit -am 'Add Readme'\ngit push origin master</code></pre>\n\n<p>There it is!</p>\n\n<p>Now you can have a colleague pull changes directly from you. Simply provide them with your public address, for example http://192.168.1.120/git/testproject.git, and they should now be able to clone your repo, add your remote, pull changes, etc.</p>\n\n<p>If you want other people to be able to push to your repo you will have to explicitly allow this. In your testproject.git repo, set the http.receivepack value to true:</p>\n\n<pre class=\"code\"><code>git config http.receivepack true</code></pre>\n",
				"date_published": "2016-02-08T16:00:00-08:00",
				"url": "https://roylindauer.dev/2016/02/08/setting-up-git.html",
				"tags": ["devops","sysadmin","git"]
			},
			{
				"id": "http://royldev.micro.blog/2015/06/08/enable-status-for.html",
				"title": "Enable status for php-fpm",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>Accessing the PHP-FPM Status screen is easy enough. First, enable pm.status in your php pool:</p>\n\n<pre class=\"code\"><code>pm.status_path = /status</code></pre>\n\n<p>Then add the following block to your Nginx vhost conf:</p>\n\n<pre class=\"code\"><code>    location ~ ^/(status|ping)$ {\n        access_log off;\n        allow 127.0.0.1;\n        allow 192.168.1.0/24; ##### YOU WILL WANT TO CHANGE THIS TO YOUR IP ADDR #####\n        deny all;\n        include fastcgi_params;\n        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n        fastcgi_pass unix:/var/run/php-fpm-www.sock;\n    }</code></pre>\n\n<p>Restart php-fpm and nginx and then browse to http://<SERVERIP>/status. You will be presented with some usefulinformation on the current status of your PHP-FPM pool.</p>\n\n<p>By default /status will just show a short status, like an overview of all of the processes. To see output on each process append ?full to the url,http://<SERVERIP>/status?full. You can also pass ?json to get JSON output, if you wanted to feed the data into some other log or stats processing tool (thinking like, greylog or logstash?).</p>\n\n<p>Here is a breakdown of the stats presented to you:</p>\n\n<p>\n<strong>pool</strong> - the name of the pool.<br>\n<strong>process manage</strong>r - static, dynamic or ondemand.<br>\n<strong>start time</strong> - the date and time FPM has started.<br>\n<strong>start since</strong> - number of seconds since FPM has started.<br>\n<strong>accepted conn</strong> - the number of request accepted by the pool.<br>\n<strong>listen queue</strong> - the number of request in the queue of pendingconnections (see backlog in listen(2)).<br>\n<strong>max listen queue</strong> - the maximum number of requests in the queueof pending connections since FPM has started.<br>\n<strong>listen queue len</strong> - the size of the socket queue of pending connections.<br>\n<strong>idle processes</strong>- the number of idle processes.<br>\n<strong>active processes</strong> - the number of active processes.<br>\n<strong>total processes</strong> - the number of idle + active processes.<br>\n<strong>max active processes</strong> - the maximum number of active processes since FPMhas started.<br>\n<strong>max children reached</strong> - number of times the process limit has been reached.\n</p>\n\n<p>Use this information to tune your pool configuration.</p>\n",
				"date_published": "2015-06-07T16:00:00-08:00",
				"url": "https://roylindauer.dev/2015/06/07/enable-status-for.html",
				"tags": ["sysadmin","php"]
			},
			{
				"id": "http://royldev.micro.blog/2015/05/26/my-pantheon-jenkins.html",
				"title": "My Pantheon + Jenkins Process",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>Here is a rough outline of my Pantheon + Jenkins process. I like my code in <a href=\"https://bitbucket.org/\" target=\"_blank\">BitBucket</a>. I alsolike Pantheon (<a href=\"https://pantheon.io/\" target=\"_blank\">check them out</a>). The Pantheon workflow is all about being the source of truth for your code. This is fine, and actually I dig it because it promotes good practices. However, I, and my company,have many projects in BitBucket already, and amusing Jenkins more and more for some Continuous Integration functions. We want tokeep using BitBucket as our source of truth and our existing workflows, but also want to usePantheon.</p>\n\n<p>Already, this is problematic and managing it on a developer by developer process is going to be prone to error. You have to push branches to two remotes and deal with, probably, some ugly merging and potentially other issues.</p>\n\n<p>What Iwant to do is to push to BitBucket anduse a commit hook to trigger Jenkins to deploy our code to Pantheon automatically. I use Pantheon Multidev (at work) and this processassumes that the Mutlidev envalready exists. It will not create it for you (<em>yet</em>).</p>\n\n<h2>The BitBucketcommit hook</h2>\n\n<p>How this is going to work is we are going to setup a POST hook with BitBucket. We will set the POST url to be a PHP script (our receiver script) in our Jenkins server (or just setup a small ec2 instance to host it if you don't want to install PHP on your Jenkins server).The POST payload will include the list of modified branches. I have called my receiver script <em>jenk.php</em> because, heh. I have setup some environmentvariables with my Jenkins username and access token so that I can make API requests to the Jenkins server.</p>\n\n<p>The POST hook url looks something like:http://jenkins.server/jenk.php?project=<project-name>&token=BUILD-PROJECT</p>\n\n<p>Replace <project-name> with the name of your project, and replace BUILD-PROJECT with your build token.</p>\n\n<h2>jenk.php</h2>\n\n<p>This is the receiver script. It gets data from your commit, and submits a build request to Jenkins. Easy peasy.</p>\n\n<pre class=\"code\"><code><?php\nif (!getenv('JENKINS_USERNAME') || !getenv('JENKINS_ACCESS_TOKEN')) {\n    die('No jenkins access credentials available');\n}\n\nif (!isset($_GET['token'])) {\n    die('No build token');\n}\n\nif (!isset($_GET['project'])) {\n    die('No project');\n}\n\nif (!isset($_POST['payload'])) {\n    die('No payload, go home');\n}\n\n$token    = filter_input(INPUT_GET, 'token');\n$project  = filter_input(INPUT_GET, 'project');\n$payload  = json_decode($_POST['payload'], true);\n\n$jenkins = array(\n    'endpoint' => 'jenkins.server/job',\n    'username' => getenv('JENKINS_USERNAME'),\n    'access_token' => getenv('JENKINS_ACCESS_TOKEN')\n);\n\nif (!empty($payload['commits'])) {\n    foreach ($payload['commits'] as $commit) {\n        if (!empty($commit['branch'])) {\n\n            $url = 'http://' . $jenkins['username'] . ':' . $jenkins['access_token'] . '@' . $jenkins['endpoint'] . '/' . $project . '/buildWithParameters?token=' . $token . '&BRANCH_TO_BUILD=' . $commit['branch'] . '&delay=0sec';\n\n            $ch = curl_init();\n            curl_setopt($ch, CURLOPT_URL, $url);\n            curl_setopt($ch, CURLOPT_POST, 1);\n            $result = curl_exec($ch);\n            curl_close($ch);\n        }\n    }\n}</code></pre>\n\n<h2>The Jenkins + Pantheon Deploy Configuration</h2>\n\n<p>I have configureda parameterized build with Jenkins.The single defined parameter is \"<em>BRANCH_TO_BUILD</em>\".</p>\n\n<p>Under Source Code ManagementI have added the two git remotes, one for BitBucket called \"origin\" and one for Pantheon, called of course, \"pantheon\". In the <em>\"branches to build\" section</em>I addedin<span class=\"lang:default decode:true  crayon-inline\">remotes/origin/$BRANCH_TO_BUILD</span>.</p>\n\n<p>Under Build Triggers I used<em>\"BUILD-PROJECT\". </em></p>\n\n<p>Under Build I added an \"execute shell\" task to checkout our branchfrom origin so that we can push to the pantheon remote -<span class=\"lang:default decode:true  crayon-inline \">git checkout remotes/origin/$BRANCH_TO_BUILD</span></p>\n\n<p>Finally, I added a \"Git Publisher\" post build task configured to push the \"$BRANCH_TO_BUILD\" to the \"pantheon\" remote.</p>\n\n<p>To summarize, theprocess is make code changes, commit, push to BitBucket remote, hook is fired, and a POST is sent to our receiver script, which sends a POST to Jenkins with the $BRANCH_TO_BUILD parameter, and if the build passes, the branch is pushed to Pantheon. If everything worked you will see Pantheon converging your app in the dashboard! If it fails, well, check your console output from the build.</p>\n\n<p>And that's it. We can continue using our regular, non Pantheon, workflow, with Pantheon. The process and workflow stays consistent!</p>\n",
				"date_published": "2015-05-25T16:00:00-08:00",
				"url": "https://roylindauer.dev/2015/05/25/my-pantheon-jenkins.html",
				"tags": ["devops","git","pantheon","jenkins"]
			},
			{
				"id": "http://royldev.micro.blog/2015/05/21/a-wordpress-ajax.html",
				"title": "A WordPress ajax handler for custom themes",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<p></p>\n",
				"content_text": "\n<p>Something I have been noodling on is a better way to handle ajax requests in my custom themes. Seems to me that a relatively complex theme ends up with a lot of add_action calls for custom ajax handlers, and this could be simplified/reduced. Every time a new endpoint is required we have to add two new add_action calls to our theme. Maybe a better approach is to write a single ajax endpoint that will route requests to the proper classes/methods?</p>\n\n<p>The goal would be that all ajax requests are run through our custom ajax handler and routed to the appropriate controller & method for execution. This method allows us to encapsulatefunctionality into separate classes/modules instead of cluttering functions.php with ajax functions. Potentially this makes the code more reusable.</p>\n\n<p>With some sane configuration I think that this could be a good way to build a flexible ajax interface into your theme. To protect against malicious calls classes are namespaced (\\Ecs\\Modules\\), and the methods are prefixed (ajax*). This should stop most attempts to execute arbitrary theme code. There are issues though. There would be a fatal error if the class does not exist which could expose information about the environment. We have to make sure that the $_REQUEST params are well sanitized. This would need to be scrutinized and tested for security issues. We don't want someone to be able to craft a request to execute code we don't explicitly want executed.</p>\n\n<p>Here is an example structure in a hypothetical Theme class.</p>\n\n<pre class=\"code\"><code>class Theme\n{\n\n    public function __construct()\n    {\n        ... snip ...\n\n        add_action('wp_ajax_ecs_ajax', array(&$this, 'executeAjax'));\n        add_action('wp_ajax_nopriv_ecs_ajax', array(&$this, 'executeAjax'));\n    }\n\n    ... snip ...\n\n    /**\n     * Simple interface for executing ajax requests\n     *\n     * Usage: /wp-admin/admin-ajax.php?action=ecs_ajax&c=CLASS&m=METHOD&_wpnonce=NONCE\n     *\n     * Params for ajax request:\n     * c         = class to instantiate\n     * m         = method to run\n     * _wpnonce  = WordPress Nonce\n     * display   = json,html\n     *\n     * Naming Conventions\n     * Method names will be prefixed with \"ajax_\" and then run through the Inflector to camelize it\n     *     - eg: \"doThing\" would become \"ajaxDoThing\", so you need a method in your class called \"ajaxDoThing\"\n     *\n     * Classes can be whatever you want. They are expected to be namespaces to \\Ecs\\Modules\n     *\n     * Output can be rendered as JSON, or HTML\n     *\n     * Generate a nonce: wp_create_nonce('execute_ajax_nonce');\n     */\n    public function executeAjax()\n    {\n        try {\n            // We expect a valid wp nonce\n            if (!isset($_REQUEST['_wpnonce']) || !wp_verify_nonce($_REQUEST['_wpnonce'], 'execute_ajax_nonce')) {\n                throw new \\Exception('Invalid ajax request');\n            }\n\n            // Make sure we have a class and a method to execute\n            if (!isset($_REQUEST['c']) && !isset($_REQUEST['m'])) {\n                throw new \\Exception('Invalid params in ajax request');\n            }\n\n            // Make sure that the requested class exists and instantiate it\n            $c = filter_var($_REQUEST['c'], FILTER_SANITIZE_STRING);\n            $class = \"\\Ecs\\\\Modules\\\\$c\";\n\n            if (!class_exists($class)) {\n                throw new \\Exception('Class does not exist');\n            }\n\n            $Obj = new $class();\n\n            // Add our prefix and camelize the requested method\n            // eg: \"method\" becomes \"ajaxMethod\"\n            // eg: \"do_thing\" becomes \"ajaxDoThing\", or \"doThing\" becomes \"ajaxDoThing\"\n            $Inflector = new \\Ecs\\Core\\Inflector();\n            $m = $Inflector->camelize('ajax_' . filter_var($_REQUEST['m'], FILTER_SANITIZE_STRING));\n\n            // Make sure that the requested method exists in our object\n            if (!method_exists($Obj, $m)) {\n                throw new \\Exception('Ajax method does not exist');\n            }\n\n            // Execute\n            $result = $Obj->$m();\n\n            // Render the response\n            \\Ecs\\Helpers\\json_response($result);\n\n        } catch (\\Exception $e) {\n            \\Ecs\\Helpers\\json_response(array('error' => $e->getMessage()));\n        }\n\n        // Make sure this thing dies so it never echoes back that damn zero.\n        die();\n    }\n}</code></pre>\n\n",
				"date_published": "2015-05-20T16:00:00-08:00",
				"url": "https://roylindauer.dev/2015/05/20/a-wordpress-ajax.html",
				"tags": ["webdev","wordpress"]
			},
			{
				"id": "http://royldev.micro.blog/2015/05/19/my-vagrantfile.html",
				"title": "My VagrantFile",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n\n<p>This is the Vagrantfile I am using for my development box at home and work. It isdetermines how much ram is available and how I want to allocate, how many CPUs are available, and configures the VM for me. I use NFS for shared folders. Finally, starting to use \"hostupdater\" to keep my host machines hosts file current.</p>\n\n<p>I would love to make that more dynamic, based on the Apache vhosts I have configured in the VM. Something to work towards I suppose.</p>\n\n<p>The base box was provisionedusing Packer and Chef.</p>\n\n<pre class=\"code\"><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n#\n# Using the following plugins:\n# `vagrant plugin install vagrant-hostsupdater`\n#\n\nPARAMS = {\n    'projects_path' => \"#{ENV['HOME']}/Development/projects\",\n    'hostname' => \"#{ENV['USER']}-web\",\n    'ip' => '192.168.33.10',\n    'max_ram' => 3 # 4 = 1/4 of the system ram, 2= 1/3, 2 = 1/2, \n}\n\n# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!\nVAGRANTFILE_API_VERSION = \"2\"\n\nVagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n\n  config.vm.box = \"ns-centos65\"\n  config.vm.host_name = PARAMS['hostname']\n\n  # Setup network\n  config.vm.network :forwarded_port, guest: 22, host: 2222, id: \"ssh\", disabled: true\n  config.vm.network :forwarded_port, guest: 22, host: 2210, auto_correct: true # ssh\n  config.vm.network :forwarded_port, guest: 80, host: 8010 # http\n  config.vm.network :forwarded_port, guest: 3000, host: 3000 # rails/webrick\n  config.vm.network :forwarded_port, guest: 8080, host: 8080 # xhprof gui\n\n  config.vm.network \"private_network\", ip: PARAMS['ip']\n\n  # Setup synced folders\n  config.vm.synced_folder PARAMS['projects_path'], \"/var/www/projects\", nfs: true\n  \n  # Setup /etc/hosts on host\n  config.hostsupdater.aliases = %w(rlug.local roylindauer.local trap.local estimator.local)\n\n  # Setup VM params based on host resources\n  host = RbConfig::CONFIG['host_os']\n  # Provider-specific configuration so you can fine-tune various\n  # backing providers for Vagrant. These expose provider-specific options.\n  # Example for VirtualBox:\n  config.vm.provider \"virtualbox\" do |vb|\n  # Give VM 1/4 system memory & access to all cpu cores on the host\n  if host =~ /darwin/\n    cpus = `sysctl -n hw.ncpu`.to_i\n    # sysctl returns Bytes and we need to convert to MB\n    mem = `sysctl -n hw.memsize`.to_i / 1024 / 1024 / PARAMS['max_ram']\n  elsif host =~ /linux/\n    cpus = `nproc`.to_i\n    # meminfo shows KB and we need to convert to MB\n    mem = `grep 'MemTotal' /proc/meminfo | sed -e 's/MemTotal://' -e 's/ kB//'`.to_i / 1024 / PARAMS['max_ram']\n  else # sorry Windows folks, I can't help you\n    cpus = 2\n    mem = 1024\n  end\n\n    vb.gui = true\n    vb.name = \"Web_Development_LAMP\"\n    vb.customize [\"modifyvm\", :id, \"--memory\", mem]\n    vb.customize [\"modifyvm\", :id, \"--cpus\", cpus]\n    vb.customize [\"modifyvm\", :id, \"--natdnshostresolver1\", \"on\"]\n    vb.customize [\"modifyvm\", :id, \"--natdnsproxy1\", \"on\"]\n  end\n\nend\n</code></pre>\n",
				"date_published": "2015-05-18T16:00:00-08:00",
				"url": "https://roylindauer.dev/2015/05/18/my-vagrantfile.html",
				"tags": ["devops","vagrant"]
			},
			{
				"id": "http://royldev.micro.blog/2015/04/02/ohai-plugin-for.html",
				"title": "Ohai Plugin for OpenVZ VMs to get public IP Address",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<p>Node Name:   mydv.server.com\nEnvironment: production\nFQDN:        mydv.server.com\nIP:          10.20.30.40<!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n",
				"content_text": "\n\n<p>Media Temple uses the OpenVZ virtualization system and I have quite a few Media Temple servers under Chef management. The one thing that has made management difficult is that by default during a Chef run ohai returns 127.0.0.1 as the default IP address which means I cannot run knife to execute commands from my workstation.</p>\n\n<p>For example, when I run<code>knife node show mydv.server.com</code>I get the following:</p>\n\n<pre class=\"code\"><code>$ knife node show mydv.server.com\n\nNode Name:   mydv.server.com\nEnvironment: production\nFQDN:        mydv.server.com\nIP:          127.0.0.1</code></pre>\n\n<p>Kinda sucks. If try to execute on all of my MT servers, say something like<code>knife ssh 'tags:mt' 'sudo chef-client'</code> I will get a ton of failed connection errors because it is trying to connect to 127.0.0.1.</p>\n\n<p>The solution is to get ohai to return the correct IP. OpenVZ has virtual interfaces and the actual public IP is assigned to them, while the main interface, eth0, is given the ip 127.0.0.1. This ohai plugin will retrieve the correct IP.</p>\n\n<pre class=\"code\"><code>provides \"ipaddress\"\nrequire_plugin \"#{os}::network\"\nrequire_plugin \"#{os}::virtualization\"\n\nif virtualization[\"system\"] == \"openvz\"\n  network[\"interfaces\"][\"venet0:0\"][\"addresses\"].each do |ip, params|\n    if params[\"family\"] == \"inet\"\n      ipaddress ip\n    end\n  end\nend</code></pre>\n\n<p>Put this in your ohai plugins directory for chef: /etc/chef/plugins/openvz.rb and when chef runs, it will get the correct IP address.</p>\n\n<p>Now when I run <code>knife show mydb.server.com</code></p>\n<pre class=\"code\"><code>$ knife node show mydv.server.com\n\nNode Name:   mydv.server.com\nEnvironment: production\nFQDN:        mydv.server.com\nIP:          10.20.30.40</code></pre>\n",
				"date_published": "2015-04-01T16:00:00-08:00",
				"url": "https://roylindauer.dev/2015/04/01/ohai-plugin-for.html",
				"tags": ["devops","chef"]
			},
			{
				"id": "http://royldev.micro.blog/2015/02/19/what-to-do.html",
				"title": "What to do when your website is hacked/exploited",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n\n<p>So your website has been \"hacked\"? You load your website in your browser, and are redirected to some spammers website, or maybe you google'd yourself (naughty), and found a few thousand indexed pages for knock off prada gear? Ok, so how do you fix this, and more importantly, how do you learn how they did it so you can defend against it later.</p>\n\n<h2>Secure the scene</h2>\n\n<p>The first thing I do is take a snapshot of the hacked web site. I want the entire webroot, and the access and error logs so that I can review them.</p>\n\n<pre class=\"code\"><code>cd /var/www/vhosts/mysite.com\ntar -zcf 20150101-siteexploit-evidence.tar httpdocs logs/access_log* logs/error_log*</code></pre>\n\n<p>At this point I want to either shutdown the web server, or just restore the site and get back to the investigation. I would shut down if I felt the exploit was serious enough to warrant more serious action. Otherwise, just restore from backup, or checkout your repo, whatever you gotta do.</p>\n\n<p>Here is where the fun begins. Try to figure out when, and how, your site was exploited. First things first, take an inventory of the scene.\n<h2>Looking For Clues to how the site was hacked</h2>\nRun the <em>last</em> command for suspicious logins. \"last\" will show a listing of last logged in users. Maybe someone SSH'd into your machine (hopefully not!) or logged in via FTP. If you see a suspicious IP in last then chances are someone has initiated the attack by simply uploading a file through FTP or SCP.</p>\n\n<p>Check for modified files. If you are seeing files modified recently that you did not edit yourself they are suspect and probably contain malicious code. Check the modification timestamp on those files. That date will be useful when looking through the access logs. If you know that you have not made changes to your site in {n} days use that as the basis for your search:</p>\n\n<pre class=\"code\"><code>find . -type f -mtime -{n} -print > modified_files.txt</code></pre>\n\n<p>Replace {n} with an actual number.</p>\n\n<p>Scan all files in the webroot for common exploit signatures. If you find a lot of <em>eval(gzinflate</em> then you can almost be certain that it is malicious code. A good rule of thumb is eval is \"evil\", except when it's not (protip: it usually is).</p>\n\n<pre class=\"code\"><code>grep -ir 'gzinflate\\(base64_decode' *.php > suspect_files.txt</code></pre>\n\n<p>Here is a bash script that checks for common exploits:</p>\n\n<pre class=\"code \"><code>#!/bin/bash\n \npattern='r0nin|m0rtix|upl0ad|r57shell|c99shell|shellbot|phpshell|void\\.ru|phpremoteview|directmail|bash_history|\\.ru/|brute *force|multiviews|cwings|vandal|bitchx|eggdrop|guardservices|psybnc|dalnet|undernet|vulnscan|spymeta|raslan58|Webshell|get_pass|PhpConfigSpy|SubhashDasyam|(eval.*(base64_decode|gzinflate|\\$_))|\\$[0O]{4,}|FilesMan|JGF1dGhfc|IIIl|die\\(PHP_OS|posix_getpwuid|Array\\(base64_decode|document\\.write\\(\"\\\\u00|sh(3(ll|11))|earnmoneydo'\nsearchpath=/var/www/vhosts\ngrep $pattern $searchpath -roE --include=*.php* | sort</code></pre>\n\n<p>That will check all .php files under /var/www/vhosts for common php shells and exploits.</p>\n\n<h2>Collect Evidence</h2>\n\n<p>Ok, so you have a date, maybe a date range, and a list of potentially suspect files. With that information it's time to start looking at logs. The goal is to be able to determine when, and potentially how, the attacker exploited the site.</p>\n\n<p>There are <a href=\"http://www.symantec.com/connect/articles/five-common-web-application-vulnerabilities\" target=\"_blank\">5 common vulnerabilities</a> attackers exploit.</p>\n\n<ol>\n\t<li>Remote code execution</li>\n\t<li>SQL injection</li>\n\t<li>Format string vulnerabilities</li>\n\t<li>Cross Site Scripting (XSS)</li>\n\t<li>Username enumeration</li>\n</ol>\n\n<p>All web developers should be familiar with the <a href=\"https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project#tab=OWASP_Top_10_for_2013\" target=\"_blank\">OWASP top 10</a> too. Anyways, you want to look for signs of those types of attacks. I scan my access logs and typically check for the following things:<p>\n\n<ul>\n\t<li>Check for POST requests to suspicious locations, such as a file upload location, or other areas that are not accessed directly (like an includes directory, or theme directory).</li>\n\t<li>Check for GET or POST requests with a remote URL as a parameter, such as /page/?url=http://atackersite.com/malicious_script</li>\n\t<li>Check for GET or POST requests to the files you found when scanning the site for malicious code</li>\n\t<li>Check for very odd query parameters in GET and POST requests</li>\n\t<li>You should know your site, check for requests that are just out of the ordinary</li>\n</ul>\n\n<p>When you find suspicious entries in your logs, take note of the IP addresses. Scan your logs for all entries for that IP, you might be able to find when the exploit occurred. If you do, you then know the requests that might have been the culprit, or at least have a date range for when the exploit took place. Check the logs around that date range. It could be that the attacker initiated the exploit through a proxy, and finished the work through another one.</p>\n\n<h2>Take Action</h2>\n\n<p>I recently had a clients website get exploited. The attacker modified dozens of PHP scripts, uploaded about 30 different PHP shells, and just generally made a mess of things. Most of the modified files had the same modification timestamp, so using that I checked the logs and found that someone had been crafting POSTs to a file upload plugin for a wysiwyg editor. Looking into that plugin, I found that they were able to upload a PHP shell. I quickly removed the plugin, restored the modified files, and removed all of the uploaded PHP shells.</p>\n\n<p>The exploit showed me that the security of the website and server were severely lacking. Some fundamental security precautions were not implemented. Such as allowing script execution in a user upload directory. That should just never happen. I setup some rules to disable script execution in specific directories, disable direct access to other directories, secured file and directory permissions.</p>\n\n<p>I also setup fail2ban on the clients server. Fail2ban is great for monitoring logs and looking for certain signatures, and then banning the offending IP. You can define other actions to take, such as emailing you or something, but I just want to block their access to the server. So, ban via iptables. I noticed that this particular attacker was sending a lot of POST requests at the same time, so I setup a fail2ban filter to handle a postflood. You will probably want to setup filters for dealing with failed log in attempts to the server, and to address any of the other suspicious activity you found in your logs.</p>\n\n<p>You will definitely want to update your CMS and update any 3rd party dependencies. Doing that alone will go a long way towards keeping your site exploit free.</p>\n\n<h2>Wrap up</h2>\n\n<p>Security is hard. It's a never ending battle. And you will never, ever, make your site 100% safe and secure. The best thing to do is have a recovery plan. When your site is exploited can you recover? If you can answer yes to that, you are in a good place.</p>\n",
				"date_published": "2015-02-18T16:00:00-08:00",
				"url": "https://roylindauer.dev/2015/02/18/what-to-do.html",
				"tags": ["troubleshooting","webdev"]
			},
			{
				"id": "http://royldev.micro.blog/2014/09/26/virtualbox-bug-related.html",
				"title": "Virtualbox Bug related to SendFile",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n\n<p>I have been doing more web development with Vagrant and VirtualBox. It's a nice way to keep my dev environment nearly the same as my production environments. Recently I was doing some front-end coding and was running into the most bizarre errors with my JavaScript.</p>\n\n<p>It pays to read the documentation. Had I read it more thoroughly, I would have known about this before debugging it and wasting time. Oops!</p>\n\n<p>Turns out there is bug with VirtualBox's shared folder support and <a href=\"http://linux.die.net/man/2/sendfile\" target=\"_blank\">sendfile</a>. This bug was preventing the VM from serving new versions of any file in the shared directory. Obviously this is not good for web development.</p>\n\n<p>The solution is easy enough. You just have to disable sendfile in your web server.</p>\n\n<p>In apache:</p>\n\n<pre class=\"code\"><code>EnableSendFile Off\n</code></pre>\n\n\n<p>In nginx:</p>\n<pre class=\"code\"><code>sendfile off;\n</code></pre>\n\n<p><strong>Update:</strong></p>\n\n<p>The Vagrant documentation does include some information it: <a href=\"https://docs.vagrantup.com/v2/synced-folders/virtualbox.html\" target=\"_blank\">https://docs.vagrantup.com/v2/synced-folders/virtualbox.html</a></p>\n",
				"date_published": "2014-09-25T16:00:00-08:00",
				"url": "https://roylindauer.dev/2014/09/25/virtualbox-bug-related.html",
				"tags": ["vagrant","webdev","virtualbox"]
			},
			{
				"id": "http://royldev.micro.blog/2014/07/13/setup-development-environment.html",
				"title": "Setup Development Environment with Vagrant \u0026 Chef",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n\n<p>I use Chef to manage and provision new staging and production servers in our company. It takes a lot of the headache out of managing multiple servers and allows me to fire up new web & data servers for our projects with ease. I have several cookbooks that I use to configure servers and to setup/configure websites. In a nutshell, it's rad, and website deployments have never been easier.</p>\n\n<p>For my local development environment I currently run Ubuntu, with Apache, Nginx, PHP 5.3, Ruby 1.9.3, Ruby 2.0, MySQL 5.5, etc. Some of our projects use Node, Redis, MongoDB. Ideally I would offload all these different servers into virtual machines suited and designed for the task, identical in configuration ot the staging and production servers.</p>\n\n<p>Enter Vagrant. Vagrant is a tool to configure development environments.</p>\n\n<p>How I expect this to work:</p>\n\n<ul>\n    <li>I want to use my native development tools (NetBeans, Sublime Text, Git, etc) on my workstation.</li>\n    <li>I want to use the VM to serve the project files.</li>\n    <li>I do not want to have to deploy my local code to the VM for testing and review.</li>\n    <li>I will mount my project directory as a shared path in the VM.</li>\n    <li>I will build the VM using my Chef cookbooks.</li>\n</ul>\n\n<p>Ok, not so bad. Vagrant makes this really easy.</p>\n\n<p><a href=\"http://docs.vagrantup.com/v2/installation/\">Install Vagrant</a></p>\n<pre class=\"code\"><code>wget https://dl.bintray.com/mitchellh/vagrant/vagrant_1.6.3_x86_64.deb \nsudo dpkg -i vagrant_1.6.3_x86_64.deb\n</code></pre>\n\n<p>Once installed you want to add a box to build your VM from. There are many to choose from. I prefer CentOS for my servers, and will add one from <a href=\"http://www.vagrantbox.es/\">http://www.vagrantbox.es/</a>. All of our production machines use CentOS or RHEL, so my development VM should use CentOS.</p>\n\n<pre class=\"code\"><code>vagrant box add https://github.com/2creatives/vagrant-centos/releases/download/v6.4.2/centos64-x86_64-20140116.box --name centos-6.4\n</code></pre>\n\n<p>Now you need to create your Vagrant project. I have considered creating the Vagrant config file in my project and putting it under version control. Currently I just have a directory for Vagrant projects. Either way.</p>\n\n<pre class=\"code\"><code>cd ~/projects/mywebproject.com\nvagrant init centos-6.4</code></pre>\n\n<p>This will create a Vagrant config file. The Vagrant file describes how to build the VM. It defines the network settings, shared directories, and how to provision the machine using Chef. When creating the vagrant file you can pass the name of the box to use. I used the box we added earlier, \"centos-6-4\". If you leave this parameter off you can always edit the Vagrant file to change it.</p>\n\n<p>Configuration is rather minimal. Not a whole lot we need to do to get something running. Open Vagrantfile in your text editor.</p>\n\n<p>I want my VM to use the local network. You could opt to use the private network, which I believe is the default. You can also setup port forwarding here. For example, if you want to forward requests to http://localhost:8080 to port 80 on the VM. I just setup a public network for my VMs because often times I have people in the office review work on my server from their</p>\n\n<pre class=\"code\"><code>config.vm.network :public_network</code></pre>\n\n<p>Let's set up the shared folders. The first path is the local directory you want to share, and is relative from the Vagrantfile. The second is the mount point in the VM. Since the Vagrant file is the root of my project, I will set the share directory to the current directory. Set the mount point to someplace you will want to serve your project from. I tend to do things the Enterprise Linux way, and will put my web projects under /var/</p>\n\n<pre class=\"code\"><code>config.vm.synced_folder \"./\", \"/var/www/mywebproject.com\"\n</code></pre>\n\n<p>Now, to tell Vagrant how to provision the VM using</p>\n\n<pre class=\"code\"><code>config.omnibus.chef_version = :latest\n\tconfig.vm.provision :chef_solo do |chef|\n\t\tchef.cookbooks_path = \"/home/user/Development/my-chef/cookbooks\"\n\t\tchef.roles_path = \"/home/user/Development/my-chef/roles\"\n\t\tchef.data_bags_path = \"/home/user/Development/my-chef/data_bags\"\n\t\tchef.add_recipe \"my-cookbook::role-apache-server\"\n\t\tchef.add_recipe \"my-cookbook::role-mysql-server\"\n\t\tchef.add_recipe \"my-cookbook::role-php-server\"\n\t\tchef.add_recipe \"my-cookbook::site-my-kickass-site.com\"\n\n\t\tchef.json = {\n\t\t  \"mysql\" => {\n\t\t    \"server_root_password\" => \"MyMysqlRootPassword\",\n\t\t  }\n\t\t}\n\n\tend\n</code></pre>\n\n<p>This is pretty straight forward. We tell Vagrant where our chef data is stored, which recipes to run, and pass along any attributes we want Chef to use. I would like to explain how I organize my Chef cookbooks very briefly. I use a fat-recipe/sinny-role approach. I have a cloud server cookbook to manage AWS and Rackspace instances. I have role recipes, and site recipes. A role recipe defines how the node should act: is it an Nginx server? Or a MySQL server? Will it run PHP-FPM? And so on. Then I have a site recipe which is defines how the website will be configured. It creates an apache vhost file, sets up a PHP-FPM pool, creates an Nginx proxy to a NodeJS app. I have data bags that correspond to different environments to configure the site as well, so production uses a different hostname than staging, has a different MySQL configuration, and so on. Now when Chef runs, it detects the environment, loads the corresponding data bag, and configures the site and node.</p>\n\n<p>There is one more step before we can start up our VM. We need to install the omnibus vagrant plugin.</p>\n\n<pre class=\"code\"><code>vagrant plugin install vagrant-omnibus</code></pre>\n\n<p>The Omnibus Vagrant plugin automatically hooks into the Vagrant provisioning middleware. It will bootstrap the VM for us. It is required if you are going to provision the VM with chef.</p>\n\n<p>Ok, when that is installed you can fire up vagrate:</p>\n\n<pre class=\"code\"><code>vagrant up\n</code></pre>\n\n<p>And there we go. You can continue working on your project locally, but serve it using a VM configured identically to your production servers. Have fun with your kick ass new dev environment!</p>\n\n<p>The resulting Vagrant file should look something like:</p>\n\n<pre class=\"code\"><code>    VAGRANTFILE_API_VERSION = \"2\"\n\n    Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n\n        config.vm.box \"centos-6.4\"\n\n        config.vm.network :public_network\n\n        config.vm.synced_folder \"./\", \"/var/www/mywebproject.com\"\n             \n        config.omnibus.chef_version = :latest\n        config.vm.provision :chef_solo do |chef|\n            chef.cookbooks_path = \"/home/user/Development/my-chef/cookbooks\"\n            chef.roles_path = \"/home/user/Development/my-chef/roles\"\n            chef.data_bags_path = \"/home/user/Development/my-chef/data_bags\"\n            chef.add_recipe \"my-cookbook::role-apache-server\"\n            chef.add_recipe \"my-cookbook::role-mysql-server\"\n            chef.add_recipe \"my-cookbook::role-php-server\"\n            chef.add_recipe \"my-cookbook::site-my-kickass-site.com\"\n\n            chef.json = {\n              \"mysql\" => {\n                \"server_root_password\" => \"MyMysqlRootPassword\",\n              }\n            }\n\n        end\n    end\n</code></pre>\n",
				"date_published": "2014-07-12T16:00:00-08:00",
				"url": "https://roylindauer.dev/2014/07/12/setup-development-environment.html",
				"tags": ["devops","chef","vagrant"]
			},
			{
				"id": "http://royldev.micro.blog/2014/07/12/trying-to-troubleshoot.html",
				"title": "Trying to Troubleshoot extremely high MySQL CPU Usage",
				"content_html": "<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n",
				"content_text": "\n<p>MySQL CPU usage was spiking upwards of 1000%. Load average was around 50-60. I could not SSH into the machine though, not immediately.</p>\n\n<p>Since I could not actually get into the machine I had it restarted. Just as soon as the machine came back up MySQL CPU usage jumped right back up to 1000%.</p>\n\n<p>Once I was able to finally shut MySQL down I had to discover _why_ the load was so ridiculously high.</p>\n\n<p>MySQL slow query log showed that it was a WordPress plugin that was making a SHIT TON of queries, and MySQL could not keep up. The plugin in question was the WP Security & Audit Log. The plugin logs activity on a WordPress sites, failed logins, successful logins, new user accounts, etc. The queries in question were all related to failed logins. Checking the Apache access logs showed me that there was a massive spike in users attempting to brute force passwords. Thankfully none of them got through.</p>\n\n<p>Gathered a list of offending IPs and blocked them. Also changed all user passwords, just in case. Also disabled the plugin until a resolution can be found and an alternative put in place.</p>\n\n<p>A few things here..</p>\n\n<ol>\n    <li>MySQL is probably not tuned well, and should be tuned for this particular site.</li>\n    <li>The audit plugin makes too many unnecessary queries and I do not think it is meant to handle the amount of traffic the site was getting. I really should look into that further.</li>\n    <li>Our Nagios configuration does not monitor traffic spikes on websites. If it did I could have caught this _before_ it become so bad.</li>\n</ol>\n",
				"date_published": "2014-07-11T16:00:00-08:00",
				"url": "https://roylindauer.dev/2014/07/11/trying-to-troubleshoot.html",
				"tags": ["sysadmin","troubleshooting","mysql"]
			},
			{
				"id": "http://royldev.micro.blog/2014/07/10/securing-git-repository.html",
				"title": "Securing Git repository from accidental exposure using Chef",
				"content_html": "<!-- raw HTML omitted -->\n",
				"content_text": "<p>It was brought to my attention at the office that a few of our recently launched websites had publicly exposed .git repository information. Unscrupulous users could use the exposed data to pull down the entire commit history, giving them unfiltered access to what is basically the blueprint for the website.</p>\n<p>What if someone accidentally uploaded a config file to the repository with sensitive information in it? Or what if the user was able to discover a major security vulnerability in the code that would have otherwise remained \"safe\"? Scary.</p>\n<p>There is definitely too much risk in allowing public access to your .git directory.</p>\n<p>I created a Chef recipe called<br>\n<strong>gitsec.rb</strong> to deploy a very easy and quick fix to 20 web servers.\n</p>\n<pre class=\"code\"><code>\n# Apache\nif File.exist? \"/etc/init.d/httpd\"\n        directory \"/etc/httpd/conf.d/\" do\n                owner \"root\"\n                group \"root\"\n                action :create\n                recursive true\n        end\n        template \"/etc/httpd/conf.d/gitsec.conf\" do\n               source \"gitsec-apache.erb\"\n               owner  \"root\"\n               group  \"root\"\n               mode   \"0644\"\n        end\n        service \"httpd\" do \n                action :restart\n        end\nend\n# Nginx\nif File.exist? \"/etc/init.d/nginx\"\n        directory \"/etc/nginx/conf.d/\" do\n                owner \"root\"\n                group \"root\"\n                action :create\n                recursive true\n        end\n        template \"/etc/nginx/conf.d/gitsec.conf\" do\n               source \"gitsec-nginx.erb\"\n               owner  \"root\"\n               group  \"root\"\n               mode   \"0644\"\n        end\n        service \"nginx\" do \n                action :restart\n        end\nend\n</code>\n</pre>\n<p>I was not sure the best way to determine if a service exists using Chef, so I just check for the appropriate init script. All of our servers are RHEL or CentOS, so I do not do any detection of platform here. It would be trivial to do so, so I will leave that to you!</p>\n<p>You will also want to create a couple of templates:</p>\n<p><strong>gitsec-apache.conf</strong>\n</p>\n<pre class=\"code\"><code>\n## \n# Deny access to git/svn files by adding the following to httpd.conf\n#\n<Directorymatch \"^/(.*/)*.git/\">\n    RewriteEngine on\n    RewriteRule .* - [L,R=404]\n</Directorymatch>\n</code>\n</pre>\n<p><strong>gitsec-nginx.conf</strong>\n</p>\n<pre class=\"code\"><code class=\"language-ruby\">\nserver {\n    location ~ /.git {\n        return 404;\n    }\n}\n</code>\n</pre>\n<p>At first I was returning a 403 status code, but realized that it was still announcing that a file did exist at that location. 404 is better, it does not expose the existence of a file.</p>\n<p>This recipe is part of my initial web server setup now.</p>\n<p><strong>Update</strong></p>\n<p>Really though, the web server should be configured to not allow access to any dot files...</p>\n",
				"date_published": "2014-07-09T16:00:00-08:00",
				"url": "https://roylindauer.dev/2014/07/09/securing-git-repository.html",
				"tags": ["devops","sysadmin","git","chef"]
			}
	]
}
